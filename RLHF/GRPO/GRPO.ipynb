{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A0fRHEMkWjN"
   },
   "source": [
    "### GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4myyln8ZGoS"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "bf16 = False\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    bf16 = True\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "def load_model(model_name):\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, torch_dtype=torch.bfloat16 if bf16 else \"auto\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token # will use the same eos token as pad token\n",
    "\n",
    "  return model, tokenizer\n",
    "\n",
    "model_name = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "model, tokenizer = load_model(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C66aBrD1wIU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/open-thought/tiny-grpo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCZYbrJ91zqe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-IivS0O1zCr"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "  batch_size: int = 8\n",
    "  group_size: int = 8\n",
    "  epsilon : float = 1e-6\n",
    "  exp_epoch : int = 1\n",
    "  top_p : float = 1.0\n",
    "  temperature :float = 0.7\n",
    "  max_length : int = 512\n",
    "  do_sample : bool = True\n",
    "  mini_batch : int = 4\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZTc2XWR101I"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Optional, Any\n",
    "from collections.abc import Callable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def reward_fn(output: list, ground_truth_ans: Any) -> torch.tensor:\n",
    "  # give reward based on its structure and the final answer\n",
    "  returns = torch.zeros(config.group_size, 1, dtype=torch.float)\n",
    "\n",
    "  for i, completion in enumerate(output):\n",
    "      # search answer tag\n",
    "      answer_match = re.search(\n",
    "          r\"<answer>(.*?)</answer>\", #maybe strip the leading and tralining zeros\n",
    "          completion,\n",
    "          flags=re.DOTALL\n",
    "      )\n",
    "\n",
    "      answer = answer_match.group(1) if answer_match else None\n",
    "      reward = 0\n",
    "      if answer is not None:\n",
    "          if answer.strip(' ') == ground_truth_ans:\n",
    "              reward = 1.0\n",
    "          elif ground_truth_ans in answer:\n",
    "              reward = 0.5\n",
    "          else:\n",
    "              reward = 0.01\n",
    "\n",
    "      returns[i] = reward\n",
    "\n",
    "  return returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D60r6db914iT"
   },
   "outputs": [],
   "source": [
    "def get_logprobs(logits, target):\n",
    "  logprobs = torch.gather(logits[:,:-1,:], dim=-1, index=target[:,1:].unsqueeze(-1)).squeeze() #B,T\n",
    "  return logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgbGtVNT17Jf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Optional, Any\n",
    "from collections.abc import Callable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def read_jsonl(path : str | Path) -> Iterator:\n",
    "  with open(path, 'r') as f:\n",
    "    # data = [json.loads(line) for line in f]\n",
    "    for line in f:\n",
    "      yield(json.loads(line))\n",
    "\n",
    "def load_prompt(path: str, check_fn : Optional[Callable[ [Any],bool ]]) -> str:\n",
    "  rows = []\n",
    "  for x in read_jsonl(path):\n",
    "    if check_fn(x):\n",
    "      rows.append(x)\n",
    "\n",
    "  return rows\n",
    "\n",
    "loaded_prompt = load_prompt(\n",
    "    '/home/ubuntu/tiny-grpo/data/math_tasks.jsonl',\n",
    "    lambda x: len(x['question']) < 128\n",
    "    and x['num_terms'] <=3\n",
    "    and x['num_digits'] <=3)\n",
    "\n",
    "print('total prompt size', len(loaded_prompt))\n",
    "\n",
    "system_prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "<answer> answer here </answer>.\"\"\"\n",
    "\n",
    "\n",
    "dataloader = DataLoader(loaded_prompt, batch_size=config.batch_size, pin_memory=False, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLjUKAke19KI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def rollout(model, q, a):\n",
    "  model.eval()\n",
    "  system_prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "<answer> answer here </answer>.\"\"\"\n",
    "\n",
    "  template = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": q},\n",
    "]\n",
    "\n",
    "  prompt_template = tokenizer.apply_chat_template(template, add_generation_token=True, tokenize=False)\n",
    "  tokens = tokenizer(prompt_template, padding=True, padding_side='left', return_tensors='pt').to(device)\n",
    "\n",
    "  #duplicate the tokens to groups\n",
    "  tokens_input_ids = tokens['input_ids'].repeat(config.group_size,1)\n",
    "  tokens_attention_mask = tokens['attention_mask'].repeat(config.group_size,1)\n",
    "\n",
    "  tokens = {'input_ids' : tokens_input_ids, 'attention_mask': tokens_attention_mask}\n",
    "  generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        top_p=config.top_p,\n",
    "        temperature=config.temperature,\n",
    "        max_length=config.max_length,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "  output = model.generate(**tokens, generation_config=generation_config)\n",
    "  #get the output tokens only\n",
    "  response_tokens = output[:, tokens_input_ids.shape[1]:] # remove all the input tokens\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(response_tokens, skip_special_tokens=True) # should now skip all pad tokens added at the end of response_tokens\n",
    "  rewards = reward_fn(decoded_output, ground_truth_ans=a)\n",
    "\n",
    "  action_mask = output != tokenizer.pad_token_id # generates a mask i.e false in place of tokens with pad_token_id else true\n",
    "  action_mask[:, :tokens_input_ids.shape[1]] = False\n",
    "\n",
    "\n",
    "  # perform the generations, find rewards, action_mask\n",
    "  return output, rewards, action_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVslM12e1_Ps"
   },
   "outputs": [],
   "source": [
    "reference_model, _ = load_model(model_name)\n",
    "print(f\"Memory reserved before clearing: {torch.cuda.memory_reserved()/1e9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYKwjYRx1_3H"
   },
   "outputs": [],
   "source": [
    "# make experience.\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "  logprobs: torch.tensor\n",
    "  logprobs_ref : torch.tensor\n",
    "  advantages: torch.tensor\n",
    "  action_mask : torch.tensor\n",
    "  output : torch.tensor # the is the generated tokens\n",
    "  rewards : torch.tensor\n",
    "\n",
    "  def to(self, device: str) -> None:\n",
    "    # method to change all the tensor's device\n",
    "    for key, val in self.__dict__.items():\n",
    "      if isinstance(val,torch.Tensor):\n",
    "        setattr(self, key, val.to(device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZmQP7Gi2CGY"
   },
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if bf16 else torch.float32\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syrBzHgI2EXj"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "ctx = (nullcontext() if device=='cpu' else torch.amp.autocast(device_type=device, dtype=dtype))\n",
    "\n",
    "print(f\"Memory reserved before clearing: {torch.cuda.memory_reserved()/1e9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1lKBMfU2F_i"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def zero_pad_sequences(\n",
    "    sequences: list[torch.Tensor], side: str = \"left\"\n",
    ") -> torch.Tensor:\n",
    "    assert side in (\"left\", \"right\")\n",
    "    max_len = max(seq.size(0) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        pad_len = max_len - seq.size(0)\n",
    "        padding = (pad_len, 0) if side == \"left\" else (0, pad_len)\n",
    "        padded_sequences.append(F.pad(seq, padding))\n",
    "    return torch.stack(padded_sequences, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOhD_L2N2IV0"
   },
   "outputs": [],
   "source": [
    "def join_experience_batch(items: list[Experience]) -> Experience:\n",
    "    batch_data = {}\n",
    "    keys = (\n",
    "        \"logprobs\",\n",
    "        \"logprobs_ref\",\n",
    "        \"output\",\n",
    "        \"advantages\",\n",
    "        \"action_mask\",\n",
    "    )\n",
    "    for key in keys:\n",
    "        vals = [getattr(item, key) for item in items]\n",
    "        if all(v is not None for v in vals):\n",
    "            data = zero_pad_sequences(vals, \"left\")\n",
    "        else:\n",
    "            data = None\n",
    "        batch_data[key] = data\n",
    "    return Experience(**batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZQ4qgeQ2LO0"
   },
   "outputs": [],
   "source": [
    "def get_data(data, mini_bsz):\n",
    "    exp_list = []\n",
    "    ls = torch.randint(len(data), (mini_bsz,))\n",
    "    for i in ls:\n",
    "        exp_list.append(data[i])\n",
    "\n",
    "    return exp_list\n",
    "\n",
    "def approx_kl_divergence(\n",
    "    log_probs: torch.Tensor,\n",
    "    log_probs_ref: torch.Tensor,\n",
    "    action_mask: Optional[torch.Tensor],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Monte-Carlo approximation of KL divergence, k3 estimator, see: http://joschu.net/blog/kl-approx.html\n",
    "    \"\"\"\n",
    "\n",
    "    log_ratio = log_probs_ref.float() - log_probs.float()\n",
    "    if action_mask is not None:\n",
    "        log_ratio = log_ratio * action_mask\n",
    "\n",
    "    return log_ratio.exp() - log_ratio - 1\n",
    "\n",
    "def grpo_loss(logprobs, exp, clip_eps=0.2, kl_coeff=0.01):\n",
    "  ratio = (logprobs - exp.logprobs).exp()\n",
    "  surr1 = ratio * exp.advantages\n",
    "  surr2 = ratio.clamp(1 - clip_eps, 1 + clip_eps) * exp.advantages\n",
    "\n",
    "  kl = approx_kl_divergence(logprobs, exp.logprobs_ref, exp.action_mask)\n",
    "  loss = -torch.min(surr1, surr2) + kl_coeff * kl\n",
    "\n",
    "  loss = (loss * exp.action_mask).sum(dim=-1) / exp.action_mask.sum(dim=-1) # mean across the tokens\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5ssMBgK2L5W"
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "loss_list = []\n",
    "rewards_list = []\n",
    "\n",
    "for batch in dataloader:\n",
    "  exp_list = []\n",
    "  print(f\" {cnt} Before starting one batch logits: {torch.cuda.memory_reserved()/1e9}\")\n",
    "  for q,a in zip(batch['question'], batch['answer']):\n",
    "    print(f\" {cnt} start questions {torch.cuda.memory_reserved()/1e9}\")\n",
    "    print(q,a)\n",
    "    output, rewards, action_mask = rollout(model, q,a)\n",
    "    advantages = (rewards - rewards.mean(dim=0))/ (rewards.std(dim=0) + config.epsilon)\n",
    "\n",
    "    attention_mask = output != tokenizer.pad_token_id\n",
    "    with torch.no_grad():\n",
    "      logits = model.forward(output,attention_mask=attention_mask).logits\n",
    "\n",
    "    logprobs = get_logprobs(logits, output)\n",
    "\n",
    "    # print(f\"Before clearing logits: {torch.cuda.memory_reserved()/1e9}\")\n",
    "    del logits\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(f\"After clearing logits: {torch.cuda.memory_reserved()/1e9}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits_ref = reference_model.forward(output, attention_mask=attention_mask).logits\n",
    "    logprobs_ref = get_logprobs(logits_ref, output)\n",
    "\n",
    "    # print(f\"Before clearing logits_ref: {torch.cuda.memory_reserved()/1e9}\")\n",
    "    # delete unwanted tensors from the GPU\n",
    "\n",
    "    del logits_ref\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    experience = Experience(\n",
    "        logprobs=logprobs,\n",
    "        logprobs_ref=logprobs_ref,\n",
    "        advantages=advantages,\n",
    "        action_mask=action_mask[:, 1:],\n",
    "        output=output,\n",
    "        rewards=rewards\n",
    "    )\n",
    "    experience.to('cpu')\n",
    "    exp_list.append(experience)\n",
    "\n",
    "  print(f\" {cnt} After moving experience to cpu: {torch.cuda.memory_reserved()/1e9}\")\n",
    "\n",
    "\n",
    "  for e in range(config.exp_epoch):\n",
    "    # exp_dataloader = DataLoader(exp_list, shuffle=True, batch_size=config.mini_batch, drop_last=True, collate_fn=join_experience_batch)\n",
    "    exp_dataloader = get_data(exp_list, config.mini_batch)\n",
    "    for exp in exp_dataloader:\n",
    "      # find the grpo loss\n",
    "      exp.to(device)\n",
    "      # first sample the new model's logits\n",
    "      #create attention mask from the output tokens\n",
    "      attention_mask = exp.output != pad_token_id\n",
    "      with ctx:\n",
    "\n",
    "        logits = model.forward(exp.output, attention_mask=attention_mask).logits\n",
    "        logprobs = get_logprobs(logits, exp.output)\n",
    "\n",
    "        # delete logits and clear gpu cache\n",
    "        # del logits\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "        loss = grpo_loss(logprobs, exp).mean() # mean across same examples\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "      optimizer.step()\n",
    "\n",
    "      del logits, logprobs\n",
    "      gc.collect()\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "      print(f\" {cnt} After doing step: {torch.cuda.memory_reserved()/1e9}\")\n",
    "\n",
    "      ## extras\n",
    "\n",
    "      print(f'LOSS {cnt}', loss)\n",
    "      print(f'AVG REWARDS: {exp.rewards.mean()}')\n",
    "      loss_list.append(loss)\n",
    "      rewards_list.append(exp.rewards.mean())\n",
    "      cnt+=1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
