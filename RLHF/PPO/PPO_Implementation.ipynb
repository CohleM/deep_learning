{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c04509",
   "metadata": {
    "id": "66c04509"
   },
   "source": [
    "```\n",
    "Author: Ehsan Kamalinejad (EK)\n",
    "Created: 2023-02-25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468731d6",
   "metadata": {
    "id": "468731d6"
   },
   "source": [
    "# PPO Training\n",
    "\n",
    "This notebook is a basic implementation of reinforcement learning (RL) training through proximal policy optimization (PPO). PPO is the default RL training for many problems at OpenAI and has great performance while very flexible. Here, the focus is education and simplicity.\n",
    "\n",
    "Here, we will learn how to train a RL agent from scratch through PPO. This will be useful when we do (reinforcement learning with human feedback) RLHF to fine-tune language models in future lectures.\n",
    "\n",
    "Pre-requisites:\n",
    "- Intermediate level familiarity with Python and PyTorch.\n",
    "- Intermediate level familiarity with general concepts in machine learning (ML) and gradient based optimization.\n",
    "- Basic familiarity with concepts in RL such as environments, rewards, agents, etc.\n",
    "\n",
    "Dependencies:\n",
    "```\n",
    "pip install torch --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "pip install moviepy omegaconf matplotlib\n",
    "pip install gym==0.26.2\n",
    "pip install git+https://github.com/carlosluis/stable-baselines3@fix_tests\n",
    "pip install gym[classic_control] gym[atari] gym[accept-rom-license] gym[other]\n",
    "```\n",
    "\n",
    "References:\n",
    "- [EK's Video Lecture](https://www.youtube.com/watch?v=3uvnoVjM8nY) This is the lecture where we did a deep dive into the theory of PPO.\n",
    "- [OpenAI PPO Repo](https://github.com/openai/baselines/blob/master/baselines/ppo2/runner.py) This is helpful as a reference for further implementations.\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347) This is the original paper that introduced PPO.\n",
    "- [Sergey Levine UC Berkley CS285](http://rail.eecs.berkeley.edu/deeprlcourse/) This is a complete course in RL.\n",
    "- [Pieter Abbeel mini-course](https://www.youtube.com/watch?v=2GwBez0D20A&list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0) This is a mini-course focusing on TRPO, PPO, DDPG and model free RL.\n",
    "- [OpenAI Documentation on RL](https://spinningup.openai.com/en/latest/index.html) THis is OpenAI documentation on RL and parts of our code was borrowed from here.\n",
    "- [labml.ai](https://nn.labml.ai/) This repo contains popular papers with their annotated PyTorch implementations.\n",
    "- [cleanrl](https://github.com/vwxyzjn/cleanrl) This repo has clean implementations of RL algorithms and parts of our code was borrowed from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "xJ_EnYYfIyWv",
   "metadata": {
    "id": "xJ_EnYYfIyWv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /Users/cohlem/anaconda3/lib/python3.11/site-packages (2.0.0.dev2)\n",
      "Collecting omegaconf\n",
      "  Obtaining dependency information for omegaconf from https://files.pythonhosted.org/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/cohlem/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy) (2.25.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy) (0.1.10)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from omegaconf) (6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (68.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from proglog<=1.0.0->moviepy) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.7.22)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=dee7a7ea43993b6873a76982ff4e38d940859e768a2f5dc739d10be4b80323a4\n",
      "  Stored in directory: /Users/cohlem/Library/Caches/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n",
      "Collecting gym==0.26.2\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym==0.26.2) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym==0.26.2) (2.2.1)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/gym-notices/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gym_notices>=0.0.4 (from gym==0.26.2)\n",
      "  Obtaining dependency information for gym_notices>=0.0.4 from https://files.pythonhosted.org/packages/25/26/d786c6bec30fe6110fd3d22c9a273a2a0e56c0b73b93e25ea1af5a53243b/gym_notices-0.0.8-py3-none-any.whl.metadata\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/25/26/d786c6bec30fe6110fd3d22c9a273a2a0e56c0b73b93e25ea1af5a53243b/gym_notices-0.0.8-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827727 sha256=b220ec890a6cc90fd8b7940961570da9773ac3f4bd710665b4e04f3d0446f2e9\n",
      "  Stored in directory: /Users/cohlem/Library/Caches/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n",
      "Collecting git+https://github.com/carlosluis/stable-baselines3@fix_tests\n",
      "  Cloning https://github.com/carlosluis/stable-baselines3 (to revision fix_tests) to /private/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/pip-req-build-tgvom1mt\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/carlosluis/stable-baselines3 /private/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/pip-req-build-tgvom1mt\n",
      "  Running command git checkout -b fix_tests --track origin/fix_tests\n",
      "  Switched to a new branch 'fix_tests'\n",
      "  branch 'fix_tests' set up to track 'origin/fix_tests'.\n",
      "  Resolved https://github.com/carlosluis/stable-baselines3 to commit 6617e6e73cb3a70f3e88cea780ea12bed95c099e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym==0.26.2 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from stable_baselines3==2.0.0a0) (0.26.2)\n",
      "Requirement already satisfied: numpy in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from stable_baselines3==2.0.0a0) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.11 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from stable_baselines3==2.0.0a0) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from stable_baselines3==2.0.0a0) (2.2.1)\n",
      "Requirement already satisfied: pandas in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from stable_baselines3==2.0.0a0) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from stable_baselines3==2.0.0a0) (3.7.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting importlib-metadata~=4.13 (from stable_baselines3==2.0.0a0)\n",
      "  Obtaining dependency information for importlib-metadata~=4.13 from https://files.pythonhosted.org/packages/d0/98/c277899f5aa21f6e6946e1c83f2af650cbfee982763ffb91db07ff7d3a13/importlib_metadata-4.13.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym==0.26.2->stable_baselines3==2.0.0a0) (0.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from importlib-metadata~=4.13->stable_baselines3==2.0.0a0) (3.11.0)\n",
      "Requirement already satisfied: filelock in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from torch>=1.11->stable_baselines3==2.0.0a0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from torch>=1.11->stable_baselines3==2.0.0a0) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from torch>=1.11->stable_baselines3==2.0.0a0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from torch>=1.11->stable_baselines3==2.0.0a0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from torch>=1.11->stable_baselines3==2.0.0a0) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib->stable_baselines3==2.0.0a0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from pandas->stable_baselines3==2.0.0a0) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3==2.0.0a0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11->stable_baselines3==2.0.0a0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11->stable_baselines3==2.0.0a0) (1.3.0)\n",
      "Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Building wheels for collected packages: stable_baselines3\n",
      "  Building wheel for stable_baselines3 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stable_baselines3: filename=stable_baselines3-2.0.0a0-py3-none-any.whl size=174675 sha256=67d7e9f421066cf3a81cbb1e571345da13ae11bfb581cf1a862984484c478aa2\n",
      "  Stored in directory: /private/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/pip-ephem-wheel-cache-8icna2yc/wheels/cc/3f/36/c2107a7756801b30e445eb0eea4eccce687ca61229763e2a3f\n",
      "Successfully built stable_baselines3\n",
      "Installing collected packages: importlib-metadata, stable_baselines3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed importlib-metadata-4.13.0 stable_baselines3-2.0.0a0\n",
      "Requirement already satisfied: gym[classic_control] in /Users/cohlem/anaconda3/lib/python3.11/site-packages (0.26.2)\n",
      "\u001b[33mWARNING: gym 0.26.2 does not provide the extra 'classic_control'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym[classic_control]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym[classic_control]) (0.0.8)\n",
      "Collecting ale-py~=0.8.0 (from gym[classic_control])\n",
      "  Obtaining dependency information for ale-py~=0.8.0 from https://files.pythonhosted.org/packages/67/3f/3f53173019d153c07025f0387cce61e2d6bccd3d82cdbffb98402388c4bc/ale_py-0.8.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading ale_py-0.8.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2 (from gym[classic_control])\n",
      "  Obtaining dependency information for autorom[accept-rom-license]~=0.4.2 from https://files.pythonhosted.org/packages/83/aa/e2695fa0c93b39cfe2065fcd189e7eb2db88f6e0922e932fc615827070da/AutoROM-0.4.2-py3-none-any.whl.metadata\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: lz4>=3.1.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym[classic_control]) (4.3.2)\n",
      "Collecting opencv-python>=3.0 (from gym[classic_control])\n",
      "  Obtaining dependency information for opencv-python>=3.0 from https://files.pythonhosted.org/packages/05/4d/53b30a2a3ac1f75f65a59eb29cf2ee7207ce64867db47036ad61743d5a23/opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym[classic_control]) (3.7.1)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from gym[classic_control]) (2.0.0.dev2)\n",
      "Collecting importlib-resources (from ale-py~=0.8.0->gym[classic_control])\n",
      "  Obtaining dependency information for importlib-resources from https://files.pythonhosted.org/packages/a4/ed/1f1afb2e9e7f38a545d628f864d562a5ae64fe6f7a10e28ffb9b185b4e89/importlib_resources-6.5.2-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: click in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (8.1.7)\n",
      "Requirement already satisfied: requests in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (4.65.0)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[classic_control])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (1.0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cycler>=0.10 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0->gym[classic_control]) (2.8.2)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy>=1.0.0->gym[classic_control]) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy>=1.0.0->gym[classic_control]) (2.25.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy>=1.0.0->gym[classic_control]) (0.5.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from moviepy>=1.0.0->gym[classic_control]) (0.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gym[classic_control]) (68.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[classic_control]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cohlem/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2023.7.22)\n",
      "Downloading ale_py-0.8.1-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=81134d21c8b4ff11e2975b101c4da4e52086564918ba89b3bda3ffa635dd1d42\n",
      "  Stored in directory: /Users/cohlem/Library/Caches/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: opencv-python, importlib-resources, AutoROM.accept-rom-license, autorom, ale-py\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 importlib-resources-6.5.2 opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install moviepy omegaconf matplotlib\n",
    "!pip install gym==0.26.2\n",
    "!pip install git+https://github.com/carlosluis/stable-baselines3@fix_tests\n",
    "!pip install gym[classic_control] gym[atari] gym[accept-rom-license] gym[other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab63e82",
   "metadata": {
    "id": "fab63e82"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('dark_background')\n",
    "from tqdm.notebook import tqdm\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78297f",
   "metadata": {
    "id": "ce78297f"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d950c464",
   "metadata": {
    "id": "d950c464"
   },
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5dfd22",
   "metadata": {
    "id": "ae5dfd22"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bc11fa",
   "metadata": {
    "id": "a0bc11fa"
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "    # experiment arguments\n",
    "    \"exp_name\": \"cartpole\",\n",
    "    \"gym_id\": \"CartPole-v1\", # the id of from OpenAI gym\n",
    "    # training arguments\n",
    "    \"learning_rate\": 1e-3, # the learning rate of the optimizer\n",
    "    \"total_timesteps\": 1000000, # total timesteps of the training\n",
    "    \"max_grad_norm\": 0.5, # the maximum norm allowed for the gradient\n",
    "    # PPO parameters\n",
    "    \"num_trajcts\": 32, # N\n",
    "    \"max_trajects_length\": 64, # T\n",
    "    \"gamma\": 0.99, # gamma\n",
    "    \"gae_lambda\":0.95, # lambda for the generalized advantage estimation\n",
    "    \"num_minibatches\": 2, # number of mibibatches used in each gradient\n",
    "    \"update_epochs\": 2, # number of full rollout storage creations\n",
    "    \"clip_epsilon\": 0.2, # the surrogate clipping coefficient\n",
    "    \"ent_coef\": 0.01, # entroy coefficient controlling the exploration factor C2\n",
    "    \"vf_coef\": 0.5, # value function controlling value estimation importance C1\n",
    "    # visualization and print parameters\n",
    "    \"num_returns_to_average\": 3, # how many episodes to use for printing average return\n",
    "    \"num_episodes_to_average\": 23, # how many episodes to use for smoothing of the return diagram\n",
    "    }\n",
    "\n",
    "# batch_size is the size of the flatten sequences when trajcts are flatten\n",
    "configs['batch_size'] = int(configs['num_trajcts'] * configs['max_trajects_length'])\n",
    "# number of samples used in each gradient\n",
    "configs['minibatch_size'] = int(configs['batch_size'] // configs['num_minibatches'])\n",
    "\n",
    "configs = DictConfig(configs)\n",
    "\n",
    "run_name = f\"{configs.gym_id}__{configs.exp_name}__{seed}__{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2426d7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "362ac2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.zeros((2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d164d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0, 1:4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91853586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcb802",
   "metadata": {
    "id": "f3dcb802"
   },
   "source": [
    "## Env\n",
    "\n",
    "`envs` is a set of parallel environments each holding a random initiali `state` and accepts an `action` to change and return its new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aff6a3b",
   "metadata": {
    "id": "1aff6a3b"
   },
   "outputs": [],
   "source": [
    "# create an env with random state\n",
    "def make_env_func(gym_id, seed, idx, run_name, capture_video=False):\n",
    "    def env_fun():\n",
    "        env = gym.make(gym_id, render_mode=\"rgb_array\")\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            # initiate the video capture if not already initiated\n",
    "            if idx == 0:\n",
    "                # wrapper to create the video of the performance\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return env_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a95904",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64a95904",
    "outputId": "267f2c6c-05a6-4811-97f0-4e5cb2d8a7e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncVectorEnv(num_envs=32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create N envs\n",
    "envs = []\n",
    "for i in range(configs.num_trajcts):\n",
    "    envs.append( make_env_func(configs.gym_id, seed + i, i, run_name) )\n",
    "envs = gym.vector.SyncVectorEnv(envs)\n",
    "envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb0ce2",
   "metadata": {
    "id": "0cbb0ce2"
   },
   "source": [
    "## Model\n",
    "\n",
    "A simple fully connected model that gets a state and has two methods:\n",
    "- `agent.value_func(state)` gets a state and returns the estimated expected total future rewards from that state $V_{\\theta}(s)$.\n",
    "- `agent.policy(state)` gets a state and returns next `action`, `log_prob` of actions, the `entropy` and `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe4d0b2",
   "metadata": {
    "id": "fbe4d0b2"
   },
   "outputs": [],
   "source": [
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A generic fully connected residual block with good setup\"\"\"\n",
    "    def __init__(self, embd_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LayerNorm(embd_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embd_dim, 4*embd_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embd_dim, embd_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \"\"\"an agent that creates actions and estimates values\"\"\"\n",
    "    def __init__(self, env_observation_dim, action_space_dim, embd_dim=64, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Linear(env_observation_dim, embd_dim)\n",
    "        self.shared_layers = nn.Sequential(*[FCBlock(embd_dim=embd_dim) for _ in range(num_blocks)])\n",
    "        self.value_head = nn.Linear(embd_dim, 1)\n",
    "        self.policy_head = nn.Linear(embd_dim, action_space_dim)\n",
    "        # orthogonal initialization with a hi entropy for more exploration at the start\n",
    "        torch.nn.init.orthogonal_(self.policy_head.weight, 0.01)\n",
    "\n",
    "    def value_func(self, state):\n",
    "        hidden = self.shared_layers(self.embedding_layer(state))\n",
    "        value = self.value_head(hidden)\n",
    "        return value\n",
    "\n",
    "    def policy(self, state, action=None):\n",
    "        hidden = self.shared_layers(self.embedding_layer(state))\n",
    "        logits = self.policy_head(hidden)\n",
    "        # PyTorch categorical class helpful for sampling and probability calculations\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.value_head(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cc4411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled outcome: 3\n",
      "Log probability of outcome 2: -1.2039728164672852\n",
      "Entropy of the distribution: 1.2798542976379395\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Define a categorical distribution with specified probabilities\n",
    "probs = torch.tensor([0.1, 0.2, 0.3, 0.4])  # Probabilities for 4 outcomes\n",
    "dist = Categorical(probs)\n",
    "\n",
    "# Sample from the distribution\n",
    "sample = dist.sample()\n",
    "print(\"Sampled outcome:\", sample.item())\n",
    "\n",
    "# Calculate the log probability of an outcome\n",
    "log_prob = dist.log_prob(torch.tensor(2))  # Log probability of outcome 2\n",
    "print(\"Log probability of outcome 2:\", log_prob.item())\n",
    "\n",
    "# Calculate the entropy of the distribution\n",
    "entropy = dist.entropy()\n",
    "print(\"Entropy of the distribution:\", entropy.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c31f4",
   "metadata": {
    "id": "678c31f4"
   },
   "source": [
    "### Generalized Advantage Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d335ca0",
   "metadata": {
    "id": "1d335ca0"
   },
   "outputs": [],
   "source": [
    "def gae(\n",
    "    cur_observation,  # the current state when advantages will be calculated\n",
    "    rewards,          # rewards collected from trajectories of shape [num_trajcts, max_trajects_length]\n",
    "    dones,            # binary marker of end of trajectories of shape [num_trajcts, max_trajects_length]\n",
    "    values            # value estimates collected over trajectories of shape [num_trajcts, max_trajects_length]\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimation (gae) estimating advantage of a particular trajecotry\n",
    "    vs the expected return starting from a state\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros((configs.num_trajcts, configs.max_trajects_length))\n",
    "    last_advantage = 0\n",
    "\n",
    "    # the value after the last step\n",
    "    with torch.no_grad():\n",
    "        last_value = agent.value_func(cur_observation).reshape(1, -1)\n",
    "#         last_value = agent.value_func(cur_observation)\n",
    "\n",
    "\n",
    "    # reverse recursive to calculate advantages based on the delta formula\n",
    "    for t in reversed(range(configs.max_trajects_length)):\n",
    "        # mask if episode completed after step t\n",
    "        \n",
    "\n",
    "        mask = 1.0 - dones[:, t]\n",
    "        last_value = last_value * mask\n",
    "        \n",
    "        last_advantage = last_advantage * mask\n",
    "        delta = rewards[:, t] + configs.gamma * last_value - values[:, t]\n",
    "        last_advantage = delta + configs.gamma * configs.gae_lambda * last_advantage\n",
    "\n",
    "        \n",
    "        advantages[:, t] = last_advantage\n",
    "        last_value = values[:, t]\n",
    "\n",
    "    advantages = advantages.to(device)\n",
    "    returns = advantages + values\n",
    "\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377b719",
   "metadata": {
    "id": "7377b719"
   },
   "source": [
    "### Creating Rollout Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264ef812",
   "metadata": {
    "id": "264ef812"
   },
   "outputs": [],
   "source": [
    "def create_rollout(\n",
    "    envs,            # parallel envs creating trajectories\n",
    "    cur_observation, # starting observation of shape [num_trajcts, observation_dim]\n",
    "    cur_done,        # current termination status of shape [num_trajcts,]\n",
    "    all_returns      # a list to track returns\n",
    "):\n",
    "    \"\"\"\n",
    "    rollout phase: create parallel trajectories and store them in the rollout storage\n",
    "    \"\"\"\n",
    "\n",
    "    # cache empty tensors to store the rollouts\n",
    "    observations = torch.zeros((configs.num_trajcts, configs.max_trajects_length) +\n",
    "                               envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((configs.num_trajcts, configs.max_trajects_length) +\n",
    "                          envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
    "    rewards = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
    "    dones = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
    "    values = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
    "\n",
    "    for t in range(configs.max_trajects_length):\n",
    "        observations[:,t] = cur_observation\n",
    "        dones[:,t] = cur_done\n",
    "\n",
    "        # give observation to the model and collect action, logprobs of actions, entropy and value\n",
    "        with torch.no_grad():\n",
    "            action, logprob, entropy, value = agent.policy(cur_observation)\n",
    "        values[:,t] = value.flatten()\n",
    "        actions[:,t] = action\n",
    "        logprobs[:,t] = logprob\n",
    "\n",
    "        # apply the action to the env and collect observation and reward\n",
    "        cur_observation, reward, cur_done, _, info = envs.step(action.cpu().numpy())\n",
    "        rewards[:,t] = torch.tensor(reward).to(device).view(-1)\n",
    "        cur_observation = torch.Tensor(cur_observation).to(device)\n",
    "        cur_done = torch.Tensor(cur_done).to(device)\n",
    "\n",
    "        # if an episode ended store its total reward for progress report\n",
    "#         if info:\n",
    "#             for item in info['final_info']:\n",
    "#                 if item and \"episode\" in item.keys():\n",
    "#                     all_returns.append(item['episode']['r'])\n",
    "#                     break\n",
    "\n",
    "    # create the rollout storage\n",
    "    rollout = {\n",
    "        'cur_observation': cur_observation,\n",
    "        'cur_done': cur_done,\n",
    "        'observations': observations,\n",
    "        'actions': actions,\n",
    "        'logprobs': logprobs,\n",
    "        'values': values,\n",
    "        'dones': dones,\n",
    "        'rewards': rewards\n",
    "    }\n",
    "\n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f951ea58",
   "metadata": {
    "id": "f951ea58"
   },
   "outputs": [],
   "source": [
    "class Storage(Dataset):\n",
    "    def __init__(self, rollout, advantages, returns, envs):\n",
    "        # fill in the storage and flatten the parallel trajectories\n",
    "        self.observations = rollout['observations'].reshape((-1,) + envs.single_observation_space.shape)\n",
    "        self.logprobs = rollout['logprobs'].reshape(-1)\n",
    "        self.actions = rollout['actions'].reshape((-1,) + envs.single_action_space.shape).long()\n",
    "        self.advantages = advantages.reshape(-1)\n",
    "        self.returns = returns.reshape(-1)\n",
    "\n",
    "    def __getitem__(self, ix: int):\n",
    "        item = [\n",
    "            self.observations[ix],\n",
    "            self.logprobs[ix],\n",
    "            self.actions[ix],\n",
    "            self.advantages[ix],\n",
    "            self.returns[ix]\n",
    "        ]\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be00145",
   "metadata": {
    "id": "3be00145"
   },
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f952663c",
   "metadata": {
    "id": "f952663c"
   },
   "outputs": [],
   "source": [
    "def loss_clip(\n",
    "    mb_oldlogporb,     # old logprob of mini batch actions collected during the rollout\n",
    "    mb_newlogprob,     # new logprob of mini batch actions created by the new policy\n",
    "    mb_advantages      # mini batch of advantages collected during the the rollout\n",
    "):\n",
    "    \"\"\"\n",
    "    policy loss with clipping to control gradients\n",
    "    \"\"\"\n",
    "    ratio = torch.exp(mb_newlogprob - mb_oldlogporb)\n",
    "    policy_loss = -mb_advantages * ratio\n",
    "    # clipped policy gradient loss enforces closeness\n",
    "    clipped_loss = -mb_advantages * torch.clamp(ratio, 1 - configs.clip_epsilon, 1 + configs.clip_epsilon)\n",
    "    pessimistic_loss = torch.max(policy_loss, clipped_loss).mean()\n",
    "    return pessimistic_loss\n",
    "\n",
    "\n",
    "def loss_vf(\n",
    "    mb_oldreturns,  # mini batch of old returns collected during the rollout\n",
    "    mb_newvalues    # minibach of values calculated by the new value function\n",
    "):\n",
    "    \"\"\"\n",
    "    enforcing the value function to give more accurate estimates of returns\n",
    "    \"\"\"\n",
    "    mb_newvalues = mb_newvalues.view(-1)\n",
    "    loss = 0.5 * ((mb_newvalues - mb_oldreturns) ** 2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8a6ca",
   "metadata": {
    "id": "ddd8a6ca"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14bf543",
   "metadata": {
    "id": "e14bf543"
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env_observation_dim=envs.single_observation_space.shape[0],\n",
    "    action_space_dim=envs.single_action_space.n\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=configs.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09baa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_returns = []\n",
    "\n",
    "# initialize the game\n",
    "cur_observation = torch.Tensor(envs.reset()[0]).to(device)\n",
    "cur_done = torch.zeros(configs.num_trajcts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de7beed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56b3d5ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e99fe17e9dea4e29b02de15e530a2cb0",
      "4dec0bfe69c549ee8592c0370a70b7e3",
      "9f417783428643318a4e87b9d9b0ede0",
      "abc5d38f35e14acfaf289f8fce7f1b25",
      "b1946f1f49d54305aaef3b79c1b06ed6",
      "bc0fc96b3b0e45899411ba50c374d071",
      "0316ca17377046859d1193ec8823753f",
      "7ee7e62c4e684d8d90d43b480c79e59e",
      "0966961b01ad4834b1d3b4315f4ea4f5",
      "863c99deb9e5423087fac355abb483e2",
      "14aeb2262b9341328c66de55c09b9a50"
     ]
    },
    "id": "56b3d5ca",
    "outputId": "c731770c-590f-4c5a-c523-7d1b90ccd691"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f53767125643c298474e488df59dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "tensor(12.5200, grad_fn=<AddBackward0>)\n",
      "tensor(11.2932, grad_fn=<AddBackward0>)\n",
      "tensor(9.5838, grad_fn=<AddBackward0>)\n",
      "tensor(8.5303, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.4960, grad_fn=<AddBackward0>)\n",
      "tensor(8.8961, grad_fn=<AddBackward0>)\n",
      "tensor(7.8095, grad_fn=<AddBackward0>)\n",
      "tensor(5.8538, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.6547, grad_fn=<AddBackward0>)\n",
      "tensor(6.2848, grad_fn=<AddBackward0>)\n",
      "tensor(4.8548, grad_fn=<AddBackward0>)\n",
      "tensor(3.2112, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.4795, grad_fn=<AddBackward0>)\n",
      "tensor(6.2684, grad_fn=<AddBackward0>)\n",
      "tensor(4.6960, grad_fn=<AddBackward0>)\n",
      "tensor(3.9718, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.1310, grad_fn=<AddBackward0>)\n",
      "tensor(8.5069, grad_fn=<AddBackward0>)\n",
      "tensor(7.3593, grad_fn=<AddBackward0>)\n",
      "tensor(6.1121, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.7728, grad_fn=<AddBackward0>)\n",
      "tensor(12.6545, grad_fn=<AddBackward0>)\n",
      "tensor(13.1728, grad_fn=<AddBackward0>)\n",
      "tensor(13.1289, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.9630, grad_fn=<AddBackward0>)\n",
      "tensor(16.5288, grad_fn=<AddBackward0>)\n",
      "tensor(17.2532, grad_fn=<AddBackward0>)\n",
      "tensor(16.7159, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.0041, grad_fn=<AddBackward0>)\n",
      "tensor(18.6569, grad_fn=<AddBackward0>)\n",
      "tensor(17.6482, grad_fn=<AddBackward0>)\n",
      "tensor(19.8761, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.0269, grad_fn=<AddBackward0>)\n",
      "tensor(16.4240, grad_fn=<AddBackward0>)\n",
      "tensor(17.2296, grad_fn=<AddBackward0>)\n",
      "tensor(17.5769, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.8449, grad_fn=<AddBackward0>)\n",
      "tensor(14.9365, grad_fn=<AddBackward0>)\n",
      "tensor(16.2072, grad_fn=<AddBackward0>)\n",
      "tensor(15.2561, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.0762, grad_fn=<AddBackward0>)\n",
      "tensor(22.0392, grad_fn=<AddBackward0>)\n",
      "tensor(22.4525, grad_fn=<AddBackward0>)\n",
      "tensor(22.0012, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.5133, grad_fn=<AddBackward0>)\n",
      "tensor(19.5539, grad_fn=<AddBackward0>)\n",
      "tensor(19.4838, grad_fn=<AddBackward0>)\n",
      "tensor(20.2291, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.8014, grad_fn=<AddBackward0>)\n",
      "tensor(16.7458, grad_fn=<AddBackward0>)\n",
      "tensor(16.8582, grad_fn=<AddBackward0>)\n",
      "tensor(14.2185, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.5787, grad_fn=<AddBackward0>)\n",
      "tensor(18.0011, grad_fn=<AddBackward0>)\n",
      "tensor(19.5714, grad_fn=<AddBackward0>)\n",
      "tensor(19.7499, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(24.4045, grad_fn=<AddBackward0>)\n",
      "tensor(21.3417, grad_fn=<AddBackward0>)\n",
      "tensor(22.8038, grad_fn=<AddBackward0>)\n",
      "tensor(21.3566, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(29.9440, grad_fn=<AddBackward0>)\n",
      "tensor(30.1652, grad_fn=<AddBackward0>)\n",
      "tensor(29.2832, grad_fn=<AddBackward0>)\n",
      "tensor(29.2353, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(22.6749, grad_fn=<AddBackward0>)\n",
      "tensor(22.5383, grad_fn=<AddBackward0>)\n",
      "tensor(23.0521, grad_fn=<AddBackward0>)\n",
      "tensor(19.6254, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(18.8605, grad_fn=<AddBackward0>)\n",
      "tensor(16.9656, grad_fn=<AddBackward0>)\n",
      "tensor(16.8837, grad_fn=<AddBackward0>)\n",
      "tensor(13.7639, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(22.5004, grad_fn=<AddBackward0>)\n",
      "tensor(22.9437, grad_fn=<AddBackward0>)\n",
      "tensor(20.8740, grad_fn=<AddBackward0>)\n",
      "tensor(23.8953, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.3370, grad_fn=<AddBackward0>)\n",
      "tensor(27.3528, grad_fn=<AddBackward0>)\n",
      "tensor(27.0781, grad_fn=<AddBackward0>)\n",
      "tensor(22.8793, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.9128, grad_fn=<AddBackward0>)\n",
      "tensor(25.8024, grad_fn=<AddBackward0>)\n",
      "tensor(26.9365, grad_fn=<AddBackward0>)\n",
      "tensor(25.0850, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(30.2452, grad_fn=<AddBackward0>)\n",
      "tensor(34.6945, grad_fn=<AddBackward0>)\n",
      "tensor(33.1152, grad_fn=<AddBackward0>)\n",
      "tensor(30.5334, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.2933, grad_fn=<AddBackward0>)\n",
      "tensor(20.7039, grad_fn=<AddBackward0>)\n",
      "tensor(23.8022, grad_fn=<AddBackward0>)\n",
      "tensor(19.6128, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.7199, grad_fn=<AddBackward0>)\n",
      "tensor(22.1134, grad_fn=<AddBackward0>)\n",
      "tensor(24.4364, grad_fn=<AddBackward0>)\n",
      "tensor(17.3433, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.7287, grad_fn=<AddBackward0>)\n",
      "tensor(36.6803, grad_fn=<AddBackward0>)\n",
      "tensor(34.1212, grad_fn=<AddBackward0>)\n",
      "tensor(29.7690, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(28.1401, grad_fn=<AddBackward0>)\n",
      "tensor(31.3940, grad_fn=<AddBackward0>)\n",
      "tensor(32.2099, grad_fn=<AddBackward0>)\n",
      "tensor(27.0716, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(35.8359, grad_fn=<AddBackward0>)\n",
      "tensor(31.1809, grad_fn=<AddBackward0>)\n",
      "tensor(36.8135, grad_fn=<AddBackward0>)\n",
      "tensor(31.5825, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.2126, grad_fn=<AddBackward0>)\n",
      "tensor(14.0516, grad_fn=<AddBackward0>)\n",
      "tensor(12.4457, grad_fn=<AddBackward0>)\n",
      "tensor(11.9044, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.3186, grad_fn=<AddBackward0>)\n",
      "tensor(20.4455, grad_fn=<AddBackward0>)\n",
      "tensor(19.8132, grad_fn=<AddBackward0>)\n",
      "tensor(20.7432, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(22.3947, grad_fn=<AddBackward0>)\n",
      "tensor(18.2676, grad_fn=<AddBackward0>)\n",
      "tensor(20.9239, grad_fn=<AddBackward0>)\n",
      "tensor(19.0707, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.9338, grad_fn=<AddBackward0>)\n",
      "tensor(24.2433, grad_fn=<AddBackward0>)\n",
      "tensor(26.7094, grad_fn=<AddBackward0>)\n",
      "tensor(25.5082, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.9304, grad_fn=<AddBackward0>)\n",
      "tensor(15.6799, grad_fn=<AddBackward0>)\n",
      "tensor(14.9496, grad_fn=<AddBackward0>)\n",
      "tensor(15.4911, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.8933, grad_fn=<AddBackward0>)\n",
      "tensor(18.8848, grad_fn=<AddBackward0>)\n",
      "tensor(22.1855, grad_fn=<AddBackward0>)\n",
      "tensor(18.3519, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(34.8126, grad_fn=<AddBackward0>)\n",
      "tensor(29.6171, grad_fn=<AddBackward0>)\n",
      "tensor(33.0613, grad_fn=<AddBackward0>)\n",
      "tensor(31.5009, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.0954, grad_fn=<AddBackward0>)\n",
      "tensor(20.6587, grad_fn=<AddBackward0>)\n",
      "tensor(17.1818, grad_fn=<AddBackward0>)\n",
      "tensor(23.4226, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.3129, grad_fn=<AddBackward0>)\n",
      "tensor(13.3083, grad_fn=<AddBackward0>)\n",
      "tensor(13.5774, grad_fn=<AddBackward0>)\n",
      "tensor(14.3771, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.5220, grad_fn=<AddBackward0>)\n",
      "tensor(13.8289, grad_fn=<AddBackward0>)\n",
      "tensor(16.6000, grad_fn=<AddBackward0>)\n",
      "tensor(15.9081, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.1417, grad_fn=<AddBackward0>)\n",
      "tensor(10.1584, grad_fn=<AddBackward0>)\n",
      "tensor(7.6393, grad_fn=<AddBackward0>)\n",
      "tensor(9.0571, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(28.9053, grad_fn=<AddBackward0>)\n",
      "tensor(21.3429, grad_fn=<AddBackward0>)\n",
      "tensor(22.9969, grad_fn=<AddBackward0>)\n",
      "tensor(27.5540, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(37.9695, grad_fn=<AddBackward0>)\n",
      "tensor(43.0026, grad_fn=<AddBackward0>)\n",
      "tensor(41.4561, grad_fn=<AddBackward0>)\n",
      "tensor(38.4861, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.9330, grad_fn=<AddBackward0>)\n",
      "tensor(27.3665, grad_fn=<AddBackward0>)\n",
      "tensor(25.4024, grad_fn=<AddBackward0>)\n",
      "tensor(25.5358, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(61.6673, grad_fn=<AddBackward0>)\n",
      "tensor(69.6685, grad_fn=<AddBackward0>)\n",
      "tensor(70.5764, grad_fn=<AddBackward0>)\n",
      "tensor(59.5357, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.7239, grad_fn=<AddBackward0>)\n",
      "tensor(19.2484, grad_fn=<AddBackward0>)\n",
      "tensor(19.3750, grad_fn=<AddBackward0>)\n",
      "tensor(17.5097, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(32.5253, grad_fn=<AddBackward0>)\n",
      "tensor(39.3218, grad_fn=<AddBackward0>)\n",
      "tensor(33.3242, grad_fn=<AddBackward0>)\n",
      "tensor(35.9152, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.7003, grad_fn=<AddBackward0>)\n",
      "tensor(23.6000, grad_fn=<AddBackward0>)\n",
      "tensor(24.8910, grad_fn=<AddBackward0>)\n",
      "tensor(25.7686, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.5646, grad_fn=<AddBackward0>)\n",
      "tensor(13.1733, grad_fn=<AddBackward0>)\n",
      "tensor(12.3413, grad_fn=<AddBackward0>)\n",
      "tensor(14.8110, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.2722, grad_fn=<AddBackward0>)\n",
      "tensor(20.9373, grad_fn=<AddBackward0>)\n",
      "tensor(20.2586, grad_fn=<AddBackward0>)\n",
      "tensor(17.3786, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(26.4632, grad_fn=<AddBackward0>)\n",
      "tensor(27.3987, grad_fn=<AddBackward0>)\n",
      "tensor(26.1749, grad_fn=<AddBackward0>)\n",
      "tensor(25.4750, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.0839, grad_fn=<AddBackward0>)\n",
      "tensor(13.6926, grad_fn=<AddBackward0>)\n",
      "tensor(16.4574, grad_fn=<AddBackward0>)\n",
      "tensor(9.0463, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.2139, grad_fn=<AddBackward0>)\n",
      "tensor(18.7250, grad_fn=<AddBackward0>)\n",
      "tensor(21.1512, grad_fn=<AddBackward0>)\n",
      "tensor(18.9612, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.9290, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.3628, grad_fn=<AddBackward0>)\n",
      "tensor(22.6316, grad_fn=<AddBackward0>)\n",
      "tensor(19.8418, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(22.0911, grad_fn=<AddBackward0>)\n",
      "tensor(24.0254, grad_fn=<AddBackward0>)\n",
      "tensor(22.8915, grad_fn=<AddBackward0>)\n",
      "tensor(23.1287, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(58.3377, grad_fn=<AddBackward0>)\n",
      "tensor(71.5483, grad_fn=<AddBackward0>)\n",
      "tensor(66.2170, grad_fn=<AddBackward0>)\n",
      "tensor(60.9500, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.4455, grad_fn=<AddBackward0>)\n",
      "tensor(24.4350, grad_fn=<AddBackward0>)\n",
      "tensor(21.1959, grad_fn=<AddBackward0>)\n",
      "tensor(25.1333, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(26.7439, grad_fn=<AddBackward0>)\n",
      "tensor(26.3652, grad_fn=<AddBackward0>)\n",
      "tensor(25.0380, grad_fn=<AddBackward0>)\n",
      "tensor(27.3200, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.2687, grad_fn=<AddBackward0>)\n",
      "tensor(19.8037, grad_fn=<AddBackward0>)\n",
      "tensor(18.2750, grad_fn=<AddBackward0>)\n",
      "tensor(22.1706, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.5483, grad_fn=<AddBackward0>)\n",
      "tensor(16.7293, grad_fn=<AddBackward0>)\n",
      "tensor(22.2117, grad_fn=<AddBackward0>)\n",
      "tensor(11.5038, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(22.5586, grad_fn=<AddBackward0>)\n",
      "tensor(21.4107, grad_fn=<AddBackward0>)\n",
      "tensor(23.3973, grad_fn=<AddBackward0>)\n",
      "tensor(19.6816, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.8489, grad_fn=<AddBackward0>)\n",
      "tensor(24.4209, grad_fn=<AddBackward0>)\n",
      "tensor(19.7029, grad_fn=<AddBackward0>)\n",
      "tensor(24.1737, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(32.7066, grad_fn=<AddBackward0>)\n",
      "tensor(34.4465, grad_fn=<AddBackward0>)\n",
      "tensor(36.9552, grad_fn=<AddBackward0>)\n",
      "tensor(27.4696, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(104.8247, grad_fn=<AddBackward0>)\n",
      "tensor(82.8777, grad_fn=<AddBackward0>)\n",
      "tensor(93.1464, grad_fn=<AddBackward0>)\n",
      "tensor(84.4386, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(32.2477, grad_fn=<AddBackward0>)\n",
      "tensor(32.9052, grad_fn=<AddBackward0>)\n",
      "tensor(27.3041, grad_fn=<AddBackward0>)\n",
      "tensor(28.1001, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(37.4877, grad_fn=<AddBackward0>)\n",
      "tensor(39.3607, grad_fn=<AddBackward0>)\n",
      "tensor(37.5112, grad_fn=<AddBackward0>)\n",
      "tensor(35.7233, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.1334, grad_fn=<AddBackward0>)\n",
      "tensor(14.3657, grad_fn=<AddBackward0>)\n",
      "tensor(14.3669, grad_fn=<AddBackward0>)\n",
      "tensor(13.8540, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.0707, grad_fn=<AddBackward0>)\n",
      "tensor(16.3364, grad_fn=<AddBackward0>)\n",
      "tensor(18.4146, grad_fn=<AddBackward0>)\n",
      "tensor(17.1566, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(18.7067, grad_fn=<AddBackward0>)\n",
      "tensor(15.8938, grad_fn=<AddBackward0>)\n",
      "tensor(17.2323, grad_fn=<AddBackward0>)\n",
      "tensor(17.4213, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.0242, grad_fn=<AddBackward0>)\n",
      "tensor(24.2861, grad_fn=<AddBackward0>)\n",
      "tensor(30.9098, grad_fn=<AddBackward0>)\n",
      "tensor(20.9103, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.3412, grad_fn=<AddBackward0>)\n",
      "tensor(18.4979, grad_fn=<AddBackward0>)\n",
      "tensor(20.2508, grad_fn=<AddBackward0>)\n",
      "tensor(9.8630, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.6327, grad_fn=<AddBackward0>)\n",
      "tensor(24.2514, grad_fn=<AddBackward0>)\n",
      "tensor(26.5337, grad_fn=<AddBackward0>)\n",
      "tensor(24.5621, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(43.8230, grad_fn=<AddBackward0>)\n",
      "tensor(50.8092, grad_fn=<AddBackward0>)\n",
      "tensor(42.6345, grad_fn=<AddBackward0>)\n",
      "tensor(42.7538, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.3090, grad_fn=<AddBackward0>)\n",
      "tensor(11.6910, grad_fn=<AddBackward0>)\n",
      "tensor(13.2708, grad_fn=<AddBackward0>)\n",
      "tensor(10.6598, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.8848, grad_fn=<AddBackward0>)\n",
      "tensor(14.4008, grad_fn=<AddBackward0>)\n",
      "tensor(9.9870, grad_fn=<AddBackward0>)\n",
      "tensor(13.5394, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.9447, grad_fn=<AddBackward0>)\n",
      "tensor(5.4656, grad_fn=<AddBackward0>)\n",
      "tensor(5.3789, grad_fn=<AddBackward0>)\n",
      "tensor(5.1854, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.2854, grad_fn=<AddBackward0>)\n",
      "tensor(8.8445, grad_fn=<AddBackward0>)\n",
      "tensor(7.8563, grad_fn=<AddBackward0>)\n",
      "tensor(8.2666, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.0612, grad_fn=<AddBackward0>)\n",
      "tensor(8.9188, grad_fn=<AddBackward0>)\n",
      "tensor(8.6020, grad_fn=<AddBackward0>)\n",
      "tensor(11.2074, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.7888, grad_fn=<AddBackward0>)\n",
      "tensor(19.9825, grad_fn=<AddBackward0>)\n",
      "tensor(16.6273, grad_fn=<AddBackward0>)\n",
      "tensor(19.0360, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(55.3330, grad_fn=<AddBackward0>)\n",
      "tensor(55.6555, grad_fn=<AddBackward0>)\n",
      "tensor(48.1996, grad_fn=<AddBackward0>)\n",
      "tensor(55.7875, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(18.6616, grad_fn=<AddBackward0>)\n",
      "tensor(22.2038, grad_fn=<AddBackward0>)\n",
      "tensor(22.3770, grad_fn=<AddBackward0>)\n",
      "tensor(20.6018, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.7906, grad_fn=<AddBackward0>)\n",
      "tensor(17.9974, grad_fn=<AddBackward0>)\n",
      "tensor(19.6769, grad_fn=<AddBackward0>)\n",
      "tensor(21.5965, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.8279, grad_fn=<AddBackward0>)\n",
      "tensor(24.0956, grad_fn=<AddBackward0>)\n",
      "tensor(19.6354, grad_fn=<AddBackward0>)\n",
      "tensor(26.1140, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.0129, grad_fn=<AddBackward0>)\n",
      "tensor(22.6691, grad_fn=<AddBackward0>)\n",
      "tensor(20.3349, grad_fn=<AddBackward0>)\n",
      "tensor(19.7434, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(28.2497, grad_fn=<AddBackward0>)\n",
      "tensor(18.0160, grad_fn=<AddBackward0>)\n",
      "tensor(23.2287, grad_fn=<AddBackward0>)\n",
      "tensor(23.1925, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.1679, grad_fn=<AddBackward0>)\n",
      "tensor(12.3996, grad_fn=<AddBackward0>)\n",
      "tensor(12.8228, grad_fn=<AddBackward0>)\n",
      "tensor(12.5185, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(24.2161, grad_fn=<AddBackward0>)\n",
      "tensor(21.6541, grad_fn=<AddBackward0>)\n",
      "tensor(22.8088, grad_fn=<AddBackward0>)\n",
      "tensor(24.0792, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.4091, grad_fn=<AddBackward0>)\n",
      "tensor(29.0709, grad_fn=<AddBackward0>)\n",
      "tensor(22.0048, grad_fn=<AddBackward0>)\n",
      "tensor(26.5038, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(51.6393, grad_fn=<AddBackward0>)\n",
      "tensor(45.4658, grad_fn=<AddBackward0>)\n",
      "tensor(44.1111, grad_fn=<AddBackward0>)\n",
      "tensor(48.9178, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.0555, grad_fn=<AddBackward0>)\n",
      "tensor(8.3934, grad_fn=<AddBackward0>)\n",
      "tensor(7.6428, grad_fn=<AddBackward0>)\n",
      "tensor(6.6557, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.5980, grad_fn=<AddBackward0>)\n",
      "tensor(14.2221, grad_fn=<AddBackward0>)\n",
      "tensor(12.9895, grad_fn=<AddBackward0>)\n",
      "tensor(12.6032, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.9755, grad_fn=<AddBackward0>)\n",
      "tensor(11.6963, grad_fn=<AddBackward0>)\n",
      "tensor(12.6659, grad_fn=<AddBackward0>)\n",
      "tensor(10.3481, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.7661, grad_fn=<AddBackward0>)\n",
      "tensor(10.4120, grad_fn=<AddBackward0>)\n",
      "tensor(9.4961, grad_fn=<AddBackward0>)\n",
      "tensor(8.6166, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.4899, grad_fn=<AddBackward0>)\n",
      "tensor(23.1407, grad_fn=<AddBackward0>)\n",
      "tensor(18.0008, grad_fn=<AddBackward0>)\n",
      "tensor(22.9363, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.3245, grad_fn=<AddBackward0>)\n",
      "tensor(13.7989, grad_fn=<AddBackward0>)\n",
      "tensor(12.1386, grad_fn=<AddBackward0>)\n",
      "tensor(13.1943, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.8384, grad_fn=<AddBackward0>)\n",
      "tensor(12.9830, grad_fn=<AddBackward0>)\n",
      "tensor(12.3110, grad_fn=<AddBackward0>)\n",
      "tensor(20.4391, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.3314, grad_fn=<AddBackward0>)\n",
      "tensor(29.6344, grad_fn=<AddBackward0>)\n",
      "tensor(26.7302, grad_fn=<AddBackward0>)\n",
      "tensor(27.7659, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.6574, grad_fn=<AddBackward0>)\n",
      "tensor(16.4042, grad_fn=<AddBackward0>)\n",
      "tensor(16.2511, grad_fn=<AddBackward0>)\n",
      "tensor(16.6716, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.0941, grad_fn=<AddBackward0>)\n",
      "tensor(21.7136, grad_fn=<AddBackward0>)\n",
      "tensor(16.5039, grad_fn=<AddBackward0>)\n",
      "tensor(21.4745, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(33.1801, grad_fn=<AddBackward0>)\n",
      "tensor(30.7798, grad_fn=<AddBackward0>)\n",
      "tensor(30.3589, grad_fn=<AddBackward0>)\n",
      "tensor(27.8358, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.8040, grad_fn=<AddBackward0>)\n",
      "tensor(17.8412, grad_fn=<AddBackward0>)\n",
      "tensor(14.4258, grad_fn=<AddBackward0>)\n",
      "tensor(19.1098, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(52.8631, grad_fn=<AddBackward0>)\n",
      "tensor(50.1842, grad_fn=<AddBackward0>)\n",
      "tensor(55.7759, grad_fn=<AddBackward0>)\n",
      "tensor(44.7032, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.9714, grad_fn=<AddBackward0>)\n",
      "tensor(23.5457, grad_fn=<AddBackward0>)\n",
      "tensor(21.5045, grad_fn=<AddBackward0>)\n",
      "tensor(20.0609, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.2603, grad_fn=<AddBackward0>)\n",
      "tensor(18.3135, grad_fn=<AddBackward0>)\n",
      "tensor(15.4762, grad_fn=<AddBackward0>)\n",
      "tensor(15.6166, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.6266, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.2355, grad_fn=<AddBackward0>)\n",
      "tensor(24.4227, grad_fn=<AddBackward0>)\n",
      "tensor(20.6883, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(22.6260, grad_fn=<AddBackward0>)\n",
      "tensor(19.9253, grad_fn=<AddBackward0>)\n",
      "tensor(22.2208, grad_fn=<AddBackward0>)\n",
      "tensor(19.7763, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.2520, grad_fn=<AddBackward0>)\n",
      "tensor(7.2097, grad_fn=<AddBackward0>)\n",
      "tensor(7.1564, grad_fn=<AddBackward0>)\n",
      "tensor(9.2074, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.7018, grad_fn=<AddBackward0>)\n",
      "tensor(22.4772, grad_fn=<AddBackward0>)\n",
      "tensor(19.5793, grad_fn=<AddBackward0>)\n",
      "tensor(25.5296, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.5828, grad_fn=<AddBackward0>)\n",
      "tensor(21.0041, grad_fn=<AddBackward0>)\n",
      "tensor(19.0576, grad_fn=<AddBackward0>)\n",
      "tensor(23.6924, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.5945, grad_fn=<AddBackward0>)\n",
      "tensor(9.6387, grad_fn=<AddBackward0>)\n",
      "tensor(9.3784, grad_fn=<AddBackward0>)\n",
      "tensor(8.7270, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(24.3616, grad_fn=<AddBackward0>)\n",
      "tensor(21.1731, grad_fn=<AddBackward0>)\n",
      "tensor(25.0997, grad_fn=<AddBackward0>)\n",
      "tensor(19.7689, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.0148, grad_fn=<AddBackward0>)\n",
      "tensor(21.5733, grad_fn=<AddBackward0>)\n",
      "tensor(19.8733, grad_fn=<AddBackward0>)\n",
      "tensor(17.0366, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(29.3557, grad_fn=<AddBackward0>)\n",
      "tensor(24.8633, grad_fn=<AddBackward0>)\n",
      "tensor(33.4503, grad_fn=<AddBackward0>)\n",
      "tensor(19.4873, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.3304, grad_fn=<AddBackward0>)\n",
      "tensor(17.8377, grad_fn=<AddBackward0>)\n",
      "tensor(16.5796, grad_fn=<AddBackward0>)\n",
      "tensor(16.7723, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(11.8098, grad_fn=<AddBackward0>)\n",
      "tensor(10.2501, grad_fn=<AddBackward0>)\n",
      "tensor(9.1143, grad_fn=<AddBackward0>)\n",
      "tensor(10.4693, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.6761, grad_fn=<AddBackward0>)\n",
      "tensor(20.2374, grad_fn=<AddBackward0>)\n",
      "tensor(16.9029, grad_fn=<AddBackward0>)\n",
      "tensor(20.3919, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(30.9564, grad_fn=<AddBackward0>)\n",
      "tensor(33.7842, grad_fn=<AddBackward0>)\n",
      "tensor(33.8329, grad_fn=<AddBackward0>)\n",
      "tensor(30.5696, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(41.3296, grad_fn=<AddBackward0>)\n",
      "tensor(37.3681, grad_fn=<AddBackward0>)\n",
      "tensor(34.9749, grad_fn=<AddBackward0>)\n",
      "tensor(42.7496, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.7598, grad_fn=<AddBackward0>)\n",
      "tensor(14.9158, grad_fn=<AddBackward0>)\n",
      "tensor(13.6531, grad_fn=<AddBackward0>)\n",
      "tensor(15.2229, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.6688, grad_fn=<AddBackward0>)\n",
      "tensor(13.1549, grad_fn=<AddBackward0>)\n",
      "tensor(11.4160, grad_fn=<AddBackward0>)\n",
      "tensor(12.1037, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.3957, grad_fn=<AddBackward0>)\n",
      "tensor(8.6971, grad_fn=<AddBackward0>)\n",
      "tensor(7.2225, grad_fn=<AddBackward0>)\n",
      "tensor(9.5960, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.5022, grad_fn=<AddBackward0>)\n",
      "tensor(18.0712, grad_fn=<AddBackward0>)\n",
      "tensor(21.7098, grad_fn=<AddBackward0>)\n",
      "tensor(17.2521, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(26.7435, grad_fn=<AddBackward0>)\n",
      "tensor(25.1407, grad_fn=<AddBackward0>)\n",
      "tensor(24.8136, grad_fn=<AddBackward0>)\n",
      "tensor(23.7998, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.6880, grad_fn=<AddBackward0>)\n",
      "tensor(14.5052, grad_fn=<AddBackward0>)\n",
      "tensor(13.8346, grad_fn=<AddBackward0>)\n",
      "tensor(14.6496, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.5422, grad_fn=<AddBackward0>)\n",
      "tensor(10.0455, grad_fn=<AddBackward0>)\n",
      "tensor(11.1891, grad_fn=<AddBackward0>)\n",
      "tensor(6.5826, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.3819, grad_fn=<AddBackward0>)\n",
      "tensor(5.1773, grad_fn=<AddBackward0>)\n",
      "tensor(5.0536, grad_fn=<AddBackward0>)\n",
      "tensor(5.6729, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(18.5299, grad_fn=<AddBackward0>)\n",
      "tensor(18.7827, grad_fn=<AddBackward0>)\n",
      "tensor(17.3951, grad_fn=<AddBackward0>)\n",
      "tensor(18.4062, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.0404, grad_fn=<AddBackward0>)\n",
      "tensor(29.0632, grad_fn=<AddBackward0>)\n",
      "tensor(22.9725, grad_fn=<AddBackward0>)\n",
      "tensor(28.9201, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.6300, grad_fn=<AddBackward0>)\n",
      "tensor(6.1549, grad_fn=<AddBackward0>)\n",
      "tensor(7.1629, grad_fn=<AddBackward0>)\n",
      "tensor(5.9462, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.3498, grad_fn=<AddBackward0>)\n",
      "tensor(6.9436, grad_fn=<AddBackward0>)\n",
      "tensor(9.1485, grad_fn=<AddBackward0>)\n",
      "tensor(7.8474, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.3152, grad_fn=<AddBackward0>)\n",
      "tensor(16.5353, grad_fn=<AddBackward0>)\n",
      "tensor(13.5239, grad_fn=<AddBackward0>)\n",
      "tensor(20.0796, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.0447, grad_fn=<AddBackward0>)\n",
      "tensor(9.3303, grad_fn=<AddBackward0>)\n",
      "tensor(8.6047, grad_fn=<AddBackward0>)\n",
      "tensor(10.9563, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.0098, grad_fn=<AddBackward0>)\n",
      "tensor(29.5255, grad_fn=<AddBackward0>)\n",
      "tensor(27.1181, grad_fn=<AddBackward0>)\n",
      "tensor(25.1848, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.7202, grad_fn=<AddBackward0>)\n",
      "tensor(15.2546, grad_fn=<AddBackward0>)\n",
      "tensor(15.1513, grad_fn=<AddBackward0>)\n",
      "tensor(20.1650, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.7128, grad_fn=<AddBackward0>)\n",
      "tensor(30.7674, grad_fn=<AddBackward0>)\n",
      "tensor(23.2680, grad_fn=<AddBackward0>)\n",
      "tensor(25.6447, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(30.2156, grad_fn=<AddBackward0>)\n",
      "tensor(32.1943, grad_fn=<AddBackward0>)\n",
      "tensor(32.1256, grad_fn=<AddBackward0>)\n",
      "tensor(28.3827, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.6743, grad_fn=<AddBackward0>)\n",
      "tensor(13.9732, grad_fn=<AddBackward0>)\n",
      "tensor(15.8732, grad_fn=<AddBackward0>)\n",
      "tensor(16.4386, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.7085, grad_fn=<AddBackward0>)\n",
      "tensor(14.4575, grad_fn=<AddBackward0>)\n",
      "tensor(12.9050, grad_fn=<AddBackward0>)\n",
      "tensor(18.9338, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.7981, grad_fn=<AddBackward0>)\n",
      "tensor(22.2516, grad_fn=<AddBackward0>)\n",
      "tensor(17.1242, grad_fn=<AddBackward0>)\n",
      "tensor(23.8832, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.6722, grad_fn=<AddBackward0>)\n",
      "tensor(22.0744, grad_fn=<AddBackward0>)\n",
      "tensor(23.7629, grad_fn=<AddBackward0>)\n",
      "tensor(13.7985, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.7229, grad_fn=<AddBackward0>)\n",
      "tensor(7.7120, grad_fn=<AddBackward0>)\n",
      "tensor(8.0015, grad_fn=<AddBackward0>)\n",
      "tensor(6.7852, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(21.3994, grad_fn=<AddBackward0>)\n",
      "tensor(12.9115, grad_fn=<AddBackward0>)\n",
      "tensor(14.5868, grad_fn=<AddBackward0>)\n",
      "tensor(18.1026, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.4147, grad_fn=<AddBackward0>)\n",
      "tensor(6.0188, grad_fn=<AddBackward0>)\n",
      "tensor(5.4421, grad_fn=<AddBackward0>)\n",
      "tensor(6.9142, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.7493, grad_fn=<AddBackward0>)\n",
      "tensor(20.7073, grad_fn=<AddBackward0>)\n",
      "tensor(17.5295, grad_fn=<AddBackward0>)\n",
      "tensor(14.9625, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(26.6391, grad_fn=<AddBackward0>)\n",
      "tensor(19.3678, grad_fn=<AddBackward0>)\n",
      "tensor(27.2585, grad_fn=<AddBackward0>)\n",
      "tensor(15.8087, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.5804, grad_fn=<AddBackward0>)\n",
      "tensor(8.4783, grad_fn=<AddBackward0>)\n",
      "tensor(7.9477, grad_fn=<AddBackward0>)\n",
      "tensor(8.1868, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.7305, grad_fn=<AddBackward0>)\n",
      "tensor(20.2151, grad_fn=<AddBackward0>)\n",
      "tensor(19.1526, grad_fn=<AddBackward0>)\n",
      "tensor(20.2130, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.1442, grad_fn=<AddBackward0>)\n",
      "tensor(15.5891, grad_fn=<AddBackward0>)\n",
      "tensor(14.2468, grad_fn=<AddBackward0>)\n",
      "tensor(13.9715, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.0153, grad_fn=<AddBackward0>)\n",
      "tensor(11.4807, grad_fn=<AddBackward0>)\n",
      "tensor(10.1445, grad_fn=<AddBackward0>)\n",
      "tensor(15.7561, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(20.0456, grad_fn=<AddBackward0>)\n",
      "tensor(18.1743, grad_fn=<AddBackward0>)\n",
      "tensor(14.1580, grad_fn=<AddBackward0>)\n",
      "tensor(22.3312, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.6222, grad_fn=<AddBackward0>)\n",
      "tensor(8.2466, grad_fn=<AddBackward0>)\n",
      "tensor(9.9443, grad_fn=<AddBackward0>)\n",
      "tensor(8.4558, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(18.5928, grad_fn=<AddBackward0>)\n",
      "tensor(14.0655, grad_fn=<AddBackward0>)\n",
      "tensor(16.1693, grad_fn=<AddBackward0>)\n",
      "tensor(16.8676, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.7370, grad_fn=<AddBackward0>)\n",
      "tensor(22.6031, grad_fn=<AddBackward0>)\n",
      "tensor(22.7085, grad_fn=<AddBackward0>)\n",
      "tensor(21.3950, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(11.3059, grad_fn=<AddBackward0>)\n",
      "tensor(10.0518, grad_fn=<AddBackward0>)\n",
      "tensor(10.6856, grad_fn=<AddBackward0>)\n",
      "tensor(12.1347, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.1030, grad_fn=<AddBackward0>)\n",
      "tensor(13.2688, grad_fn=<AddBackward0>)\n",
      "tensor(14.5549, grad_fn=<AddBackward0>)\n",
      "tensor(13.9350, grad_fn=<AddBackward0>)\n",
      "2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.0100, grad_fn=<AddBackward0>)\n",
      "tensor(15.7087, grad_fn=<AddBackward0>)\n",
      "tensor(15.9030, grad_fn=<AddBackward0>)\n",
      "tensor(14.0642, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.7521, grad_fn=<AddBackward0>)\n",
      "tensor(13.8309, grad_fn=<AddBackward0>)\n",
      "tensor(15.8963, grad_fn=<AddBackward0>)\n",
      "tensor(10.9333, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.7706, grad_fn=<AddBackward0>)\n",
      "tensor(10.6997, grad_fn=<AddBackward0>)\n",
      "tensor(12.5791, grad_fn=<AddBackward0>)\n",
      "tensor(12.1680, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.8878, grad_fn=<AddBackward0>)\n",
      "tensor(26.6002, grad_fn=<AddBackward0>)\n",
      "tensor(17.0177, grad_fn=<AddBackward0>)\n",
      "tensor(20.6987, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.7322, grad_fn=<AddBackward0>)\n",
      "tensor(25.9837, grad_fn=<AddBackward0>)\n",
      "tensor(27.2851, grad_fn=<AddBackward0>)\n",
      "tensor(23.1378, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.1275, grad_fn=<AddBackward0>)\n",
      "tensor(15.5818, grad_fn=<AddBackward0>)\n",
      "tensor(15.1690, grad_fn=<AddBackward0>)\n",
      "tensor(13.5136, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(4.5763, grad_fn=<AddBackward0>)\n",
      "tensor(3.7887, grad_fn=<AddBackward0>)\n",
      "tensor(3.3704, grad_fn=<AddBackward0>)\n",
      "tensor(3.3208, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(4.3090, grad_fn=<AddBackward0>)\n",
      "tensor(4.0259, grad_fn=<AddBackward0>)\n",
      "tensor(4.0094, grad_fn=<AddBackward0>)\n",
      "tensor(3.1724, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.4184, grad_fn=<AddBackward0>)\n",
      "tensor(13.6555, grad_fn=<AddBackward0>)\n",
      "tensor(12.1310, grad_fn=<AddBackward0>)\n",
      "tensor(10.5426, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.8334, grad_fn=<AddBackward0>)\n",
      "tensor(11.3664, grad_fn=<AddBackward0>)\n",
      "tensor(9.3394, grad_fn=<AddBackward0>)\n",
      "tensor(11.2137, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(11.5627, grad_fn=<AddBackward0>)\n",
      "tensor(7.5267, grad_fn=<AddBackward0>)\n",
      "tensor(8.7966, grad_fn=<AddBackward0>)\n",
      "tensor(8.7349, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(3.3600, grad_fn=<AddBackward0>)\n",
      "tensor(2.6789, grad_fn=<AddBackward0>)\n",
      "tensor(2.5453, grad_fn=<AddBackward0>)\n",
      "tensor(2.6413, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.6221, grad_fn=<AddBackward0>)\n",
      "tensor(8.4425, grad_fn=<AddBackward0>)\n",
      "tensor(8.0185, grad_fn=<AddBackward0>)\n",
      "tensor(10.4621, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.5656, grad_fn=<AddBackward0>)\n",
      "tensor(12.3508, grad_fn=<AddBackward0>)\n",
      "tensor(17.7099, grad_fn=<AddBackward0>)\n",
      "tensor(8.1926, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.3972, grad_fn=<AddBackward0>)\n",
      "tensor(41.5408, grad_fn=<AddBackward0>)\n",
      "tensor(27.0526, grad_fn=<AddBackward0>)\n",
      "tensor(40.7736, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(3.1116, grad_fn=<AddBackward0>)\n",
      "tensor(3.2654, grad_fn=<AddBackward0>)\n",
      "tensor(2.9317, grad_fn=<AddBackward0>)\n",
      "tensor(2.9128, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.1141, grad_fn=<AddBackward0>)\n",
      "tensor(12.6130, grad_fn=<AddBackward0>)\n",
      "tensor(10.3440, grad_fn=<AddBackward0>)\n",
      "tensor(9.8121, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.8152, grad_fn=<AddBackward0>)\n",
      "tensor(8.3801, grad_fn=<AddBackward0>)\n",
      "tensor(5.9170, grad_fn=<AddBackward0>)\n",
      "tensor(10.5212, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.7510, grad_fn=<AddBackward0>)\n",
      "tensor(16.2382, grad_fn=<AddBackward0>)\n",
      "tensor(16.2226, grad_fn=<AddBackward0>)\n",
      "tensor(14.6678, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.6188, grad_fn=<AddBackward0>)\n",
      "tensor(8.6871, grad_fn=<AddBackward0>)\n",
      "tensor(9.7353, grad_fn=<AddBackward0>)\n",
      "tensor(11.4685, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(3.7123, grad_fn=<AddBackward0>)\n",
      "tensor(4.0837, grad_fn=<AddBackward0>)\n",
      "tensor(3.5773, grad_fn=<AddBackward0>)\n",
      "tensor(4.3418, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.9907, grad_fn=<AddBackward0>)\n",
      "tensor(8.5928, grad_fn=<AddBackward0>)\n",
      "tensor(9.3363, grad_fn=<AddBackward0>)\n",
      "tensor(11.8927, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(4.0134, grad_fn=<AddBackward0>)\n",
      "tensor(3.3449, grad_fn=<AddBackward0>)\n",
      "tensor(3.5297, grad_fn=<AddBackward0>)\n",
      "tensor(3.6828, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(2.1729, grad_fn=<AddBackward0>)\n",
      "tensor(2.3553, grad_fn=<AddBackward0>)\n",
      "tensor(2.0944, grad_fn=<AddBackward0>)\n",
      "tensor(2.0744, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(2.4446, grad_fn=<AddBackward0>)\n",
      "tensor(2.3370, grad_fn=<AddBackward0>)\n",
      "tensor(2.4517, grad_fn=<AddBackward0>)\n",
      "tensor(2.1072, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(3.1555, grad_fn=<AddBackward0>)\n",
      "tensor(3.0602, grad_fn=<AddBackward0>)\n",
      "tensor(2.9825, grad_fn=<AddBackward0>)\n",
      "tensor(2.7064, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.9031, grad_fn=<AddBackward0>)\n",
      "tensor(4.9349, grad_fn=<AddBackward0>)\n",
      "tensor(7.4506, grad_fn=<AddBackward0>)\n",
      "tensor(9.4166, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.0434, grad_fn=<AddBackward0>)\n",
      "tensor(11.9516, grad_fn=<AddBackward0>)\n",
      "tensor(9.2176, grad_fn=<AddBackward0>)\n",
      "tensor(14.7636, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.9126, grad_fn=<AddBackward0>)\n",
      "tensor(1.6872, grad_fn=<AddBackward0>)\n",
      "tensor(1.8756, grad_fn=<AddBackward0>)\n",
      "tensor(1.9651, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.8159, grad_fn=<AddBackward0>)\n",
      "tensor(1.6843, grad_fn=<AddBackward0>)\n",
      "tensor(1.5138, grad_fn=<AddBackward0>)\n",
      "tensor(1.6945, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.1535, grad_fn=<AddBackward0>)\n",
      "tensor(7.9872, grad_fn=<AddBackward0>)\n",
      "tensor(5.9217, grad_fn=<AddBackward0>)\n",
      "tensor(8.2770, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(2.2129, grad_fn=<AddBackward0>)\n",
      "tensor(2.3595, grad_fn=<AddBackward0>)\n",
      "tensor(2.1084, grad_fn=<AddBackward0>)\n",
      "tensor(2.2788, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.6432, grad_fn=<AddBackward0>)\n",
      "tensor(1.7827, grad_fn=<AddBackward0>)\n",
      "tensor(1.6327, grad_fn=<AddBackward0>)\n",
      "tensor(1.5924, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(2.1904, grad_fn=<AddBackward0>)\n",
      "tensor(2.1956, grad_fn=<AddBackward0>)\n",
      "tensor(2.2614, grad_fn=<AddBackward0>)\n",
      "tensor(2.2756, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.4731, grad_fn=<AddBackward0>)\n",
      "tensor(8.1772, grad_fn=<AddBackward0>)\n",
      "tensor(7.9726, grad_fn=<AddBackward0>)\n",
      "tensor(10.7033, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.1072, grad_fn=<AddBackward0>)\n",
      "tensor(8.8603, grad_fn=<AddBackward0>)\n",
      "tensor(7.7156, grad_fn=<AddBackward0>)\n",
      "tensor(7.6334, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.1723, grad_fn=<AddBackward0>)\n",
      "tensor(9.9133, grad_fn=<AddBackward0>)\n",
      "tensor(8.3984, grad_fn=<AddBackward0>)\n",
      "tensor(12.7728, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.5918, grad_fn=<AddBackward0>)\n",
      "tensor(1.5451, grad_fn=<AddBackward0>)\n",
      "tensor(1.7133, grad_fn=<AddBackward0>)\n",
      "tensor(1.4658, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(3.5302, grad_fn=<AddBackward0>)\n",
      "tensor(3.4498, grad_fn=<AddBackward0>)\n",
      "tensor(3.3657, grad_fn=<AddBackward0>)\n",
      "tensor(3.3573, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.6512, grad_fn=<AddBackward0>)\n",
      "tensor(19.0930, grad_fn=<AddBackward0>)\n",
      "tensor(21.4212, grad_fn=<AddBackward0>)\n",
      "tensor(16.1863, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.5998, grad_fn=<AddBackward0>)\n",
      "tensor(18.7230, grad_fn=<AddBackward0>)\n",
      "tensor(21.7630, grad_fn=<AddBackward0>)\n",
      "tensor(15.7725, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.6309, grad_fn=<AddBackward0>)\n",
      "tensor(1.3935, grad_fn=<AddBackward0>)\n",
      "tensor(1.6721, grad_fn=<AddBackward0>)\n",
      "tensor(1.6514, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.8792, grad_fn=<AddBackward0>)\n",
      "tensor(0.7522, grad_fn=<AddBackward0>)\n",
      "tensor(0.7686, grad_fn=<AddBackward0>)\n",
      "tensor(0.8155, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.1264, grad_fn=<AddBackward0>)\n",
      "tensor(1.0642, grad_fn=<AddBackward0>)\n",
      "tensor(1.0704, grad_fn=<AddBackward0>)\n",
      "tensor(1.0922, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.8132, grad_fn=<AddBackward0>)\n",
      "tensor(1.0171, grad_fn=<AddBackward0>)\n",
      "tensor(0.8333, grad_fn=<AddBackward0>)\n",
      "tensor(0.9797, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.1862, grad_fn=<AddBackward0>)\n",
      "tensor(1.1849, grad_fn=<AddBackward0>)\n",
      "tensor(0.8752, grad_fn=<AddBackward0>)\n",
      "tensor(1.0777, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.1703, grad_fn=<AddBackward0>)\n",
      "tensor(1.2488, grad_fn=<AddBackward0>)\n",
      "tensor(1.0930, grad_fn=<AddBackward0>)\n",
      "tensor(1.1951, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.8961, grad_fn=<AddBackward0>)\n",
      "tensor(1.0088, grad_fn=<AddBackward0>)\n",
      "tensor(0.8861, grad_fn=<AddBackward0>)\n",
      "tensor(0.9579, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.9430, grad_fn=<AddBackward0>)\n",
      "tensor(1.0230, grad_fn=<AddBackward0>)\n",
      "tensor(1.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.9643, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.9828, grad_fn=<AddBackward0>)\n",
      "tensor(0.8205, grad_fn=<AddBackward0>)\n",
      "tensor(0.8270, grad_fn=<AddBackward0>)\n",
      "tensor(0.7735, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.0901, grad_fn=<AddBackward0>)\n",
      "tensor(1.0806, grad_fn=<AddBackward0>)\n",
      "tensor(1.0543, grad_fn=<AddBackward0>)\n",
      "tensor(1.2312, grad_fn=<AddBackward0>)\n",
      "2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2496, grad_fn=<AddBackward0>)\n",
      "tensor(1.2400, grad_fn=<AddBackward0>)\n",
      "tensor(1.2760, grad_fn=<AddBackward0>)\n",
      "tensor(1.0972, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.2377, grad_fn=<AddBackward0>)\n",
      "tensor(15.3091, grad_fn=<AddBackward0>)\n",
      "tensor(16.0453, grad_fn=<AddBackward0>)\n",
      "tensor(14.0682, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.8019, grad_fn=<AddBackward0>)\n",
      "tensor(1.0387, grad_fn=<AddBackward0>)\n",
      "tensor(0.7754, grad_fn=<AddBackward0>)\n",
      "tensor(0.9002, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.6668, grad_fn=<AddBackward0>)\n",
      "tensor(0.5950, grad_fn=<AddBackward0>)\n",
      "tensor(0.6236, grad_fn=<AddBackward0>)\n",
      "tensor(0.6637, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.7542, grad_fn=<AddBackward0>)\n",
      "tensor(0.7417, grad_fn=<AddBackward0>)\n",
      "tensor(0.6082, grad_fn=<AddBackward0>)\n",
      "tensor(0.8413, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.0393, grad_fn=<AddBackward0>)\n",
      "tensor(1.2188, grad_fn=<AddBackward0>)\n",
      "tensor(0.8979, grad_fn=<AddBackward0>)\n",
      "tensor(0.8894, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.7934, grad_fn=<AddBackward0>)\n",
      "tensor(0.9163, grad_fn=<AddBackward0>)\n",
      "tensor(0.8264, grad_fn=<AddBackward0>)\n",
      "tensor(0.8335, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.9655, grad_fn=<AddBackward0>)\n",
      "tensor(1.0090, grad_fn=<AddBackward0>)\n",
      "tensor(0.8583, grad_fn=<AddBackward0>)\n",
      "tensor(0.7875, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.8082, grad_fn=<AddBackward0>)\n",
      "tensor(0.6020, grad_fn=<AddBackward0>)\n",
      "tensor(0.7019, grad_fn=<AddBackward0>)\n",
      "tensor(0.8063, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.6506, grad_fn=<AddBackward0>)\n",
      "tensor(0.8097, grad_fn=<AddBackward0>)\n",
      "tensor(0.8561, grad_fn=<AddBackward0>)\n",
      "tensor(0.6650, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.7940, grad_fn=<AddBackward0>)\n",
      "tensor(11.5421, grad_fn=<AddBackward0>)\n",
      "tensor(10.3662, grad_fn=<AddBackward0>)\n",
      "tensor(11.7599, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.5633, grad_fn=<AddBackward0>)\n",
      "tensor(0.6532, grad_fn=<AddBackward0>)\n",
      "tensor(0.6305, grad_fn=<AddBackward0>)\n",
      "tensor(0.6585, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.7653, grad_fn=<AddBackward0>)\n",
      "tensor(0.7007, grad_fn=<AddBackward0>)\n",
      "tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "tensor(0.8056, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.7268, grad_fn=<AddBackward0>)\n",
      "tensor(0.5252, grad_fn=<AddBackward0>)\n",
      "tensor(0.6152, grad_fn=<AddBackward0>)\n",
      "tensor(0.5870, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.9070, grad_fn=<AddBackward0>)\n",
      "tensor(6.1620, grad_fn=<AddBackward0>)\n",
      "tensor(4.4893, grad_fn=<AddBackward0>)\n",
      "tensor(11.0095, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.5723, grad_fn=<AddBackward0>)\n",
      "tensor(0.6350, grad_fn=<AddBackward0>)\n",
      "tensor(0.6860, grad_fn=<AddBackward0>)\n",
      "tensor(0.5332, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.7079, grad_fn=<AddBackward0>)\n",
      "tensor(18.1791, grad_fn=<AddBackward0>)\n",
      "tensor(14.9190, grad_fn=<AddBackward0>)\n",
      "tensor(16.8765, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(18.5756, grad_fn=<AddBackward0>)\n",
      "tensor(16.3138, grad_fn=<AddBackward0>)\n",
      "tensor(21.0734, grad_fn=<AddBackward0>)\n",
      "tensor(13.2277, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.3050, grad_fn=<AddBackward0>)\n",
      "tensor(7.6520, grad_fn=<AddBackward0>)\n",
      "tensor(10.1796, grad_fn=<AddBackward0>)\n",
      "tensor(4.4659, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2405, grad_fn=<AddBackward0>)\n",
      "tensor(0.1979, grad_fn=<AddBackward0>)\n",
      "tensor(0.1723, grad_fn=<AddBackward0>)\n",
      "tensor(0.2958, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.5426, grad_fn=<AddBackward0>)\n",
      "tensor(9.8609, grad_fn=<AddBackward0>)\n",
      "tensor(6.2115, grad_fn=<AddBackward0>)\n",
      "tensor(9.8968, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(5.7424, grad_fn=<AddBackward0>)\n",
      "tensor(13.7971, grad_fn=<AddBackward0>)\n",
      "tensor(7.3491, grad_fn=<AddBackward0>)\n",
      "tensor(12.6539, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3748, grad_fn=<AddBackward0>)\n",
      "tensor(0.2845, grad_fn=<AddBackward0>)\n",
      "tensor(0.4059, grad_fn=<AddBackward0>)\n",
      "tensor(0.3408, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.4125, grad_fn=<AddBackward0>)\n",
      "tensor(0.4049, grad_fn=<AddBackward0>)\n",
      "tensor(0.3585, grad_fn=<AddBackward0>)\n",
      "tensor(0.4702, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.5022, grad_fn=<AddBackward0>)\n",
      "tensor(0.2851, grad_fn=<AddBackward0>)\n",
      "tensor(0.2703, grad_fn=<AddBackward0>)\n",
      "tensor(0.4759, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.4283, grad_fn=<AddBackward0>)\n",
      "tensor(0.3558, grad_fn=<AddBackward0>)\n",
      "tensor(0.3534, grad_fn=<AddBackward0>)\n",
      "tensor(0.3747, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3249, grad_fn=<AddBackward0>)\n",
      "tensor(0.3398, grad_fn=<AddBackward0>)\n",
      "tensor(0.3592, grad_fn=<AddBackward0>)\n",
      "tensor(0.2051, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3620, grad_fn=<AddBackward0>)\n",
      "tensor(0.3341, grad_fn=<AddBackward0>)\n",
      "tensor(0.3382, grad_fn=<AddBackward0>)\n",
      "tensor(0.3739, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3563, grad_fn=<AddBackward0>)\n",
      "tensor(0.4915, grad_fn=<AddBackward0>)\n",
      "tensor(0.4204, grad_fn=<AddBackward0>)\n",
      "tensor(0.3851, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1734, grad_fn=<AddBackward0>)\n",
      "tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "tensor(0.2050, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2667, grad_fn=<AddBackward0>)\n",
      "tensor(0.3351, grad_fn=<AddBackward0>)\n",
      "tensor(0.3082, grad_fn=<AddBackward0>)\n",
      "tensor(0.2607, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3394, grad_fn=<AddBackward0>)\n",
      "tensor(0.3480, grad_fn=<AddBackward0>)\n",
      "tensor(0.3565, grad_fn=<AddBackward0>)\n",
      "tensor(0.2534, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.4455, grad_fn=<AddBackward0>)\n",
      "tensor(0.3293, grad_fn=<AddBackward0>)\n",
      "tensor(0.4259, grad_fn=<AddBackward0>)\n",
      "tensor(0.3625, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3552, grad_fn=<AddBackward0>)\n",
      "tensor(0.3880, grad_fn=<AddBackward0>)\n",
      "tensor(0.4080, grad_fn=<AddBackward0>)\n",
      "tensor(0.3190, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3798, grad_fn=<AddBackward0>)\n",
      "tensor(0.3091, grad_fn=<AddBackward0>)\n",
      "tensor(0.3668, grad_fn=<AddBackward0>)\n",
      "tensor(0.2921, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1438, grad_fn=<AddBackward0>)\n",
      "tensor(0.1848, grad_fn=<AddBackward0>)\n",
      "tensor(0.1255, grad_fn=<AddBackward0>)\n",
      "tensor(0.1934, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1994, grad_fn=<AddBackward0>)\n",
      "tensor(0.2678, grad_fn=<AddBackward0>)\n",
      "tensor(0.2373, grad_fn=<AddBackward0>)\n",
      "tensor(0.2643, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3443, grad_fn=<AddBackward0>)\n",
      "tensor(0.3684, grad_fn=<AddBackward0>)\n",
      "tensor(0.3347, grad_fn=<AddBackward0>)\n",
      "tensor(0.3657, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2554, grad_fn=<AddBackward0>)\n",
      "tensor(0.3644, grad_fn=<AddBackward0>)\n",
      "tensor(0.3359, grad_fn=<AddBackward0>)\n",
      "tensor(0.2900, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(25.2882, grad_fn=<AddBackward0>)\n",
      "tensor(9.7402, grad_fn=<AddBackward0>)\n",
      "tensor(16.3313, grad_fn=<AddBackward0>)\n",
      "tensor(18.5812, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.8262, grad_fn=<AddBackward0>)\n",
      "tensor(12.5366, grad_fn=<AddBackward0>)\n",
      "tensor(8.5560, grad_fn=<AddBackward0>)\n",
      "tensor(12.5506, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.1409, grad_fn=<AddBackward0>)\n",
      "tensor(13.8507, grad_fn=<AddBackward0>)\n",
      "tensor(12.4770, grad_fn=<AddBackward0>)\n",
      "tensor(7.5497, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(4.4313, grad_fn=<AddBackward0>)\n",
      "tensor(2.3346, grad_fn=<AddBackward0>)\n",
      "tensor(3.7688, grad_fn=<AddBackward0>)\n",
      "tensor(3.2030, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2015, grad_fn=<AddBackward0>)\n",
      "tensor(0.1675, grad_fn=<AddBackward0>)\n",
      "tensor(0.1583, grad_fn=<AddBackward0>)\n",
      "tensor(0.2102, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "tensor(0.1846, grad_fn=<AddBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "tensor(0.2373, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1653, grad_fn=<AddBackward0>)\n",
      "tensor(0.2052, grad_fn=<AddBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "tensor(0.1788, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.2009, grad_fn=<AddBackward0>)\n",
      "tensor(7.8967, grad_fn=<AddBackward0>)\n",
      "tensor(9.3465, grad_fn=<AddBackward0>)\n",
      "tensor(8.4741, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0426, grad_fn=<AddBackward0>)\n",
      "tensor(0.0204, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0413, grad_fn=<AddBackward0>)\n",
      "tensor(0.0125, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2312, grad_fn=<AddBackward0>)\n",
      "tensor(0.2166, grad_fn=<AddBackward0>)\n",
      "tensor(0.1943, grad_fn=<AddBackward0>)\n",
      "tensor(0.2257, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0726, grad_fn=<AddBackward0>)\n",
      "tensor(0.0706, grad_fn=<AddBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1046, grad_fn=<AddBackward0>)\n",
      "tensor(0.2515, grad_fn=<AddBackward0>)\n",
      "tensor(0.1328, grad_fn=<AddBackward0>)\n",
      "tensor(0.1372, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0509, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0386, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0094, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2359, grad_fn=<AddBackward0>)\n",
      "tensor(0.3033, grad_fn=<AddBackward0>)\n",
      "tensor(0.2817, grad_fn=<AddBackward0>)\n",
      "tensor(0.2603, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0829, grad_fn=<AddBackward0>)\n",
      "tensor(0.1515, grad_fn=<AddBackward0>)\n",
      "tensor(0.2263, grad_fn=<AddBackward0>)\n",
      "tensor(0.0727, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.8275, grad_fn=<AddBackward0>)\n",
      "tensor(12.3001, grad_fn=<AddBackward0>)\n",
      "tensor(10.9493, grad_fn=<AddBackward0>)\n",
      "tensor(9.0408, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3022, grad_fn=<AddBackward0>)\n",
      "tensor(0.3086, grad_fn=<AddBackward0>)\n",
      "tensor(0.3618, grad_fn=<AddBackward0>)\n",
      "tensor(0.2499, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2615, grad_fn=<AddBackward0>)\n",
      "tensor(0.3377, grad_fn=<AddBackward0>)\n",
      "tensor(0.2497, grad_fn=<AddBackward0>)\n",
      "tensor(0.3298, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1834, grad_fn=<AddBackward0>)\n",
      "tensor(0.2196, grad_fn=<AddBackward0>)\n",
      "tensor(0.1794, grad_fn=<AddBackward0>)\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1657, grad_fn=<AddBackward0>)\n",
      "tensor(0.3092, grad_fn=<AddBackward0>)\n",
      "tensor(0.2510, grad_fn=<AddBackward0>)\n",
      "tensor(0.2532, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1650, grad_fn=<AddBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2084, grad_fn=<AddBackward0>)\n",
      "tensor(0.1046, grad_fn=<AddBackward0>)\n",
      "tensor(0.2246, grad_fn=<AddBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1443, grad_fn=<AddBackward0>)\n",
      "tensor(0.2067, grad_fn=<AddBackward0>)\n",
      "tensor(0.2748, grad_fn=<AddBackward0>)\n",
      "tensor(0.2065, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2551, grad_fn=<AddBackward0>)\n",
      "tensor(0.2365, grad_fn=<AddBackward0>)\n",
      "tensor(0.2050, grad_fn=<AddBackward0>)\n",
      "tensor(0.2646, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2653, grad_fn=<AddBackward0>)\n",
      "tensor(0.2090, grad_fn=<AddBackward0>)\n",
      "tensor(0.2151, grad_fn=<AddBackward0>)\n",
      "tensor(0.2706, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2322, grad_fn=<AddBackward0>)\n",
      "tensor(0.2117, grad_fn=<AddBackward0>)\n",
      "tensor(0.1750, grad_fn=<AddBackward0>)\n",
      "tensor(0.2473, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1672, grad_fn=<AddBackward0>)\n",
      "tensor(0.1906, grad_fn=<AddBackward0>)\n",
      "tensor(0.2084, grad_fn=<AddBackward0>)\n",
      "tensor(0.1682, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1783, grad_fn=<AddBackward0>)\n",
      "tensor(0.2773, grad_fn=<AddBackward0>)\n",
      "tensor(0.1992, grad_fn=<AddBackward0>)\n",
      "tensor(0.2091, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.8479, grad_fn=<AddBackward0>)\n",
      "tensor(11.7435, grad_fn=<AddBackward0>)\n",
      "tensor(8.7488, grad_fn=<AddBackward0>)\n",
      "tensor(12.7878, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "tensor(0.0765, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(3.6453, grad_fn=<AddBackward0>)\n",
      "tensor(7.5848, grad_fn=<AddBackward0>)\n",
      "tensor(7.6232, grad_fn=<AddBackward0>)\n",
      "tensor(3.2292, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0147, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0191, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0446, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0043, grad_fn=<AddBackward0>)\n",
      "tensor(0.0391, grad_fn=<AddBackward0>)\n",
      "tensor(0.0555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0242, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2141, grad_fn=<AddBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "tensor(0.2108, grad_fn=<AddBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1555, grad_fn=<AddBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0471, grad_fn=<AddBackward0>)\n",
      "tensor(0.1684, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2200, grad_fn=<AddBackward0>)\n",
      "tensor(0.2885, grad_fn=<AddBackward0>)\n",
      "tensor(0.2784, grad_fn=<AddBackward0>)\n",
      "tensor(0.2494, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.1373, grad_fn=<AddBackward0>)\n",
      "tensor(9.8019, grad_fn=<AddBackward0>)\n",
      "tensor(5.7447, grad_fn=<AddBackward0>)\n",
      "tensor(14.4440, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "tensor(0.1501, grad_fn=<AddBackward0>)\n",
      "tensor(0.2023, grad_fn=<AddBackward0>)\n",
      "tensor(0.1086, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0375, grad_fn=<AddBackward0>)\n",
      "tensor(0.0152, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0586, grad_fn=<AddBackward0>)\n",
      "tensor(0.0104, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(8.1084, grad_fn=<AddBackward0>)\n",
      "tensor(12.1540, grad_fn=<AddBackward0>)\n",
      "tensor(11.0870, grad_fn=<AddBackward0>)\n",
      "tensor(9.0609, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.4813, grad_fn=<AddBackward0>)\n",
      "tensor(6.1368, grad_fn=<AddBackward0>)\n",
      "tensor(12.0552, grad_fn=<AddBackward0>)\n",
      "tensor(8.6184, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "tensor(0.0195, grad_fn=<AddBackward0>)\n",
      "tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0665, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0962, grad_fn=<AddBackward0>)\n",
      "tensor(-0.1302, grad_fn=<AddBackward0>)\n",
      "tensor(-0.1594, grad_fn=<AddBackward0>)\n",
      "tensor(-0.1076, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1993, grad_fn=<AddBackward0>)\n",
      "tensor(0.2174, grad_fn=<AddBackward0>)\n",
      "tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "tensor(0.2110, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0165, grad_fn=<AddBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0005, grad_fn=<AddBackward0>)\n",
      "tensor(0.0158, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2441, grad_fn=<AddBackward0>)\n",
      "tensor(0.2213, grad_fn=<AddBackward0>)\n",
      "tensor(0.2660, grad_fn=<AddBackward0>)\n",
      "tensor(0.2156, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1396, grad_fn=<AddBackward0>)\n",
      "tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0461, grad_fn=<AddBackward0>)\n",
      "tensor(0.0750, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0712, grad_fn=<AddBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "tensor(0.0639, grad_fn=<AddBackward0>)\n",
      "tensor(0.0683, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1320, grad_fn=<AddBackward0>)\n",
      "tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.1578, grad_fn=<AddBackward0>)\n",
      "tensor(0.0567, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1846, grad_fn=<AddBackward0>)\n",
      "tensor(0.1530, grad_fn=<AddBackward0>)\n",
      "tensor(0.1913, grad_fn=<AddBackward0>)\n",
      "tensor(0.1224, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "tensor(0.1245, grad_fn=<AddBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "tensor(0.1256, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0284, grad_fn=<AddBackward0>)\n",
      "tensor(0.0192, grad_fn=<AddBackward0>)\n",
      "tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "tensor(0.0702, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1970, grad_fn=<AddBackward0>)\n",
      "tensor(0.3244, grad_fn=<AddBackward0>)\n",
      "tensor(0.2728, grad_fn=<AddBackward0>)\n",
      "tensor(0.1781, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(4.8194, grad_fn=<AddBackward0>)\n",
      "tensor(9.2281, grad_fn=<AddBackward0>)\n",
      "tensor(5.5414, grad_fn=<AddBackward0>)\n",
      "tensor(8.7411, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.7984, grad_fn=<AddBackward0>)\n",
      "tensor(11.4219, grad_fn=<AddBackward0>)\n",
      "tensor(8.2632, grad_fn=<AddBackward0>)\n",
      "tensor(12.6661, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(2.3490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0836, grad_fn=<AddBackward0>)\n",
      "tensor(2.3854, grad_fn=<AddBackward0>)\n",
      "tensor(0.0559, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "tensor(0.1351, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "tensor(0.1252, grad_fn=<AddBackward0>)\n",
      "tensor(0.0748, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "tensor(0.2561, grad_fn=<AddBackward0>)\n",
      "tensor(0.2312, grad_fn=<AddBackward0>)\n",
      "tensor(0.2012, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.1354, grad_fn=<AddBackward0>)\n",
      "tensor(0.0784, grad_fn=<AddBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0567, grad_fn=<AddBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "tensor(0.0830, grad_fn=<AddBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2680, grad_fn=<AddBackward0>)\n",
      "tensor(0.1974, grad_fn=<AddBackward0>)\n",
      "tensor(0.2058, grad_fn=<AddBackward0>)\n",
      "tensor(0.2614, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.6397, grad_fn=<AddBackward0>)\n",
      "tensor(18.0277, grad_fn=<AddBackward0>)\n",
      "tensor(15.8880, grad_fn=<AddBackward0>)\n",
      "tensor(20.9671, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0777, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0678, grad_fn=<AddBackward0>)\n",
      "tensor(-0.1168, grad_fn=<AddBackward0>)\n",
      "tensor(-0.1084, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "tensor(10.2622, grad_fn=<AddBackward0>)\n",
      "tensor(10.7355, grad_fn=<AddBackward0>)\n",
      "tensor(10.8788, grad_fn=<AddBackward0>)\n",
      "tensor(9.8857, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0706, grad_fn=<AddBackward0>)\n",
      "tensor(0.0869, grad_fn=<AddBackward0>)\n",
      "tensor(0.0718, grad_fn=<AddBackward0>)\n",
      "tensor(0.0376, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0730, grad_fn=<AddBackward0>)\n",
      "tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0647, grad_fn=<AddBackward0>)\n",
      "tensor(0.0620, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(2.0295, grad_fn=<AddBackward0>)\n",
      "tensor(4.0413, grad_fn=<AddBackward0>)\n",
      "tensor(1.7708, grad_fn=<AddBackward0>)\n",
      "tensor(4.4228, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0758, grad_fn=<AddBackward0>)\n",
      "tensor(0.0166, grad_fn=<AddBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1735, grad_fn=<AddBackward0>)\n",
      "tensor(0.1007, grad_fn=<AddBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0298, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0342, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0230, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0620, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "tensor(0.0568, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(9.7782, grad_fn=<AddBackward0>)\n",
      "tensor(8.1835, grad_fn=<AddBackward0>)\n",
      "tensor(8.0219, grad_fn=<AddBackward0>)\n",
      "tensor(9.7896, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0326, grad_fn=<AddBackward0>)\n",
      "tensor(0.0634, grad_fn=<AddBackward0>)\n",
      "tensor(0.0369, grad_fn=<AddBackward0>)\n",
      "tensor(0.0400, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0660, grad_fn=<AddBackward0>)\n",
      "tensor(0.0510, grad_fn=<AddBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "tensor(0.0262, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.4107, grad_fn=<AddBackward0>)\n",
      "tensor(3.3658, grad_fn=<AddBackward0>)\n",
      "tensor(5.5904, grad_fn=<AddBackward0>)\n",
      "tensor(5.2198, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0261, grad_fn=<AddBackward0>)\n",
      "tensor(0.0633, grad_fn=<AddBackward0>)\n",
      "tensor(0.0700, grad_fn=<AddBackward0>)\n",
      "tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0766, grad_fn=<AddBackward0>)\n",
      "tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0801, grad_fn=<AddBackward0>)\n",
      "tensor(0.0349, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "tensor(0.1311, grad_fn=<AddBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "tensor(0.1121, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0920, grad_fn=<AddBackward0>)\n",
      "tensor(0.1692, grad_fn=<AddBackward0>)\n",
      "tensor(0.1252, grad_fn=<AddBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0426, grad_fn=<AddBackward0>)\n",
      "tensor(0.0319, grad_fn=<AddBackward0>)\n",
      "tensor(0.0146, grad_fn=<AddBackward0>)\n",
      "tensor(0.0582, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "tensor(0.1374, grad_fn=<AddBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "tensor(0.1193, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1985, grad_fn=<AddBackward0>)\n",
      "tensor(0.2491, grad_fn=<AddBackward0>)\n",
      "tensor(0.2378, grad_fn=<AddBackward0>)\n",
      "tensor(0.2539, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1927, grad_fn=<AddBackward0>)\n",
      "tensor(0.1361, grad_fn=<AddBackward0>)\n",
      "tensor(0.2751, grad_fn=<AddBackward0>)\n",
      "tensor(0.1321, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1484, grad_fn=<AddBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "tensor(0.1490, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1682, grad_fn=<AddBackward0>)\n",
      "tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "tensor(0.1554, grad_fn=<AddBackward0>)\n",
      "tensor(0.1610, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2042, grad_fn=<AddBackward0>)\n",
      "tensor(0.2505, grad_fn=<AddBackward0>)\n",
      "tensor(0.1504, grad_fn=<AddBackward0>)\n",
      "tensor(0.2873, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1504, grad_fn=<AddBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "tensor(0.1627, grad_fn=<AddBackward0>)\n",
      "tensor(0.1429, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "tensor(0.1657, grad_fn=<AddBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "tensor(0.1529, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0583, grad_fn=<AddBackward0>)\n",
      "tensor(0.1320, grad_fn=<AddBackward0>)\n",
      "tensor(0.1174, grad_fn=<AddBackward0>)\n",
      "tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1678, grad_fn=<AddBackward0>)\n",
      "tensor(0.0295, grad_fn=<AddBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "tensor(0.0681, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1552, grad_fn=<AddBackward0>)\n",
      "tensor(0.0809, grad_fn=<AddBackward0>)\n",
      "tensor(0.0702, grad_fn=<AddBackward0>)\n",
      "tensor(0.1595, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "tensor(0.1551, grad_fn=<AddBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "tensor(0.1391, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0938, grad_fn=<AddBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1292, grad_fn=<AddBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "tensor(0.1328, grad_fn=<AddBackward0>)\n",
      "tensor(0.0847, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0833, grad_fn=<AddBackward0>)\n",
      "tensor(0.0708, grad_fn=<AddBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0241, grad_fn=<AddBackward0>)\n",
      "tensor(0.1217, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0784, grad_fn=<AddBackward0>)\n",
      "tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(0.0684, grad_fn=<AddBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "tensor(0.0470, grad_fn=<AddBackward0>)\n",
      "tensor(0.0396, grad_fn=<AddBackward0>)\n",
      "tensor(0.1566, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1246, grad_fn=<AddBackward0>)\n",
      "tensor(0.1446, grad_fn=<AddBackward0>)\n",
      "tensor(0.1312, grad_fn=<AddBackward0>)\n",
      "tensor(0.1409, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1982, grad_fn=<AddBackward0>)\n",
      "tensor(0.1819, grad_fn=<AddBackward0>)\n",
      "tensor(0.1543, grad_fn=<AddBackward0>)\n",
      "tensor(0.2284, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0377, grad_fn=<AddBackward0>)\n",
      "tensor(0.0618, grad_fn=<AddBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>)\n",
      "tensor(0.0749, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1668, grad_fn=<AddBackward0>)\n",
      "tensor(0.1668, grad_fn=<AddBackward0>)\n",
      "tensor(0.1542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1068, grad_fn=<AddBackward0>)\n",
      "tensor(0.0794, grad_fn=<AddBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "tensor(0.0605, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1663, grad_fn=<AddBackward0>)\n",
      "tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "tensor(0.1487, grad_fn=<AddBackward0>)\n",
      "tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "tensor(0.1282, grad_fn=<AddBackward0>)\n",
      "tensor(0.1209, grad_fn=<AddBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "tensor(0.1174, grad_fn=<AddBackward0>)\n",
      "tensor(0.1415, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1578, grad_fn=<AddBackward0>)\n",
      "tensor(0.1535, grad_fn=<AddBackward0>)\n",
      "tensor(0.1656, grad_fn=<AddBackward0>)\n",
      "tensor(0.1477, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0788, grad_fn=<AddBackward0>)\n",
      "tensor(0.0677, grad_fn=<AddBackward0>)\n",
      "tensor(0.0763, grad_fn=<AddBackward0>)\n",
      "tensor(0.0755, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(10.0827, grad_fn=<AddBackward0>)\n",
      "tensor(12.1270, grad_fn=<AddBackward0>)\n",
      "tensor(12.5413, grad_fn=<AddBackward0>)\n",
      "tensor(9.6274, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0082, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0105, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0114, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0051, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1706, grad_fn=<AddBackward0>)\n",
      "tensor(0.1589, grad_fn=<AddBackward0>)\n",
      "tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "tensor(0.1614, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(15.1476, grad_fn=<AddBackward0>)\n",
      "tensor(3.8311, grad_fn=<AddBackward0>)\n",
      "tensor(9.7444, grad_fn=<AddBackward0>)\n",
      "tensor(9.2692, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.8573, grad_fn=<AddBackward0>)\n",
      "tensor(11.8672, grad_fn=<AddBackward0>)\n",
      "tensor(8.1846, grad_fn=<AddBackward0>)\n",
      "tensor(11.2940, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.5090, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9302, grad_fn=<AddBackward0>)\n",
      "tensor(7.7533, grad_fn=<AddBackward0>)\n",
      "tensor(11.3252, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(32.6305, grad_fn=<AddBackward0>)\n",
      "tensor(37.4447, grad_fn=<AddBackward0>)\n",
      "tensor(32.6611, grad_fn=<AddBackward0>)\n",
      "tensor(35.5773, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(33.2096, grad_fn=<AddBackward0>)\n",
      "tensor(32.7352, grad_fn=<AddBackward0>)\n",
      "tensor(27.5702, grad_fn=<AddBackward0>)\n",
      "tensor(36.5906, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(27.3151, grad_fn=<AddBackward0>)\n",
      "tensor(37.0020, grad_fn=<AddBackward0>)\n",
      "tensor(34.4508, grad_fn=<AddBackward0>)\n",
      "tensor(27.0527, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(23.8567, grad_fn=<AddBackward0>)\n",
      "tensor(22.7242, grad_fn=<AddBackward0>)\n",
      "tensor(23.5324, grad_fn=<AddBackward0>)\n",
      "tensor(21.8429, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(36.1844, grad_fn=<AddBackward0>)\n",
      "tensor(29.9729, grad_fn=<AddBackward0>)\n",
      "tensor(31.4542, grad_fn=<AddBackward0>)\n",
      "tensor(31.7128, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(16.8462, grad_fn=<AddBackward0>)\n",
      "tensor(19.8342, grad_fn=<AddBackward0>)\n",
      "tensor(13.3918, grad_fn=<AddBackward0>)\n",
      "tensor(22.6361, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(45.0614, grad_fn=<AddBackward0>)\n",
      "tensor(29.4503, grad_fn=<AddBackward0>)\n",
      "tensor(34.5938, grad_fn=<AddBackward0>)\n",
      "tensor(37.6292, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0774, grad_fn=<AddBackward0>)\n",
      "tensor(0.2182, grad_fn=<AddBackward0>)\n",
      "tensor(0.5434, grad_fn=<AddBackward0>)\n",
      "tensor(0.6313, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(12.9007, grad_fn=<AddBackward0>)\n",
      "tensor(11.2285, grad_fn=<AddBackward0>)\n",
      "tensor(14.2451, grad_fn=<AddBackward0>)\n",
      "tensor(10.1273, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(37.1931, grad_fn=<AddBackward0>)\n",
      "tensor(39.9929, grad_fn=<AddBackward0>)\n",
      "tensor(37.4476, grad_fn=<AddBackward0>)\n",
      "tensor(36.3995, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(13.8400, grad_fn=<AddBackward0>)\n",
      "tensor(13.8795, grad_fn=<AddBackward0>)\n",
      "tensor(11.0911, grad_fn=<AddBackward0>)\n",
      "tensor(15.7779, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(17.9491, grad_fn=<AddBackward0>)\n",
      "tensor(14.7062, grad_fn=<AddBackward0>)\n",
      "tensor(18.5655, grad_fn=<AddBackward0>)\n",
      "tensor(13.2960, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(7.3675, grad_fn=<AddBackward0>)\n",
      "tensor(11.7585, grad_fn=<AddBackward0>)\n",
      "tensor(10.6622, grad_fn=<AddBackward0>)\n",
      "tensor(8.3810, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(5.3016, grad_fn=<AddBackward0>)\n",
      "tensor(6.4195, grad_fn=<AddBackward0>)\n",
      "tensor(5.9420, grad_fn=<AddBackward0>)\n",
      "tensor(5.5620, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(19.2388, grad_fn=<AddBackward0>)\n",
      "tensor(19.2633, grad_fn=<AddBackward0>)\n",
      "tensor(20.3170, grad_fn=<AddBackward0>)\n",
      "tensor(17.4591, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.8545, grad_fn=<AddBackward0>)\n",
      "tensor(0.7770, grad_fn=<AddBackward0>)\n",
      "tensor(0.7432, grad_fn=<AddBackward0>)\n",
      "tensor(0.5253, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.5975, grad_fn=<AddBackward0>)\n",
      "tensor(4.2017, grad_fn=<AddBackward0>)\n",
      "tensor(4.4297, grad_fn=<AddBackward0>)\n",
      "tensor(4.7570, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.4612, grad_fn=<AddBackward0>)\n",
      "tensor(1.1405, grad_fn=<AddBackward0>)\n",
      "tensor(0.8733, grad_fn=<AddBackward0>)\n",
      "tensor(1.0836, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(1.3963, grad_fn=<AddBackward0>)\n",
      "tensor(1.0169, grad_fn=<AddBackward0>)\n",
      "tensor(1.1229, grad_fn=<AddBackward0>)\n",
      "tensor(0.8920, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.7214, grad_fn=<AddBackward0>)\n",
      "tensor(0.5805, grad_fn=<AddBackward0>)\n",
      "tensor(0.4682, grad_fn=<AddBackward0>)\n",
      "tensor(0.4572, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.4258, grad_fn=<AddBackward0>)\n",
      "tensor(0.3714, grad_fn=<AddBackward0>)\n",
      "tensor(0.3337, grad_fn=<AddBackward0>)\n",
      "tensor(0.3643, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.6714, grad_fn=<AddBackward0>)\n",
      "tensor(0.6019, grad_fn=<AddBackward0>)\n",
      "tensor(0.6124, grad_fn=<AddBackward0>)\n",
      "tensor(0.6006, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.4522, grad_fn=<AddBackward0>)\n",
      "tensor(0.4571, grad_fn=<AddBackward0>)\n",
      "tensor(0.5030, grad_fn=<AddBackward0>)\n",
      "tensor(0.4452, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3460, grad_fn=<AddBackward0>)\n",
      "tensor(0.4000, grad_fn=<AddBackward0>)\n",
      "tensor(0.3538, grad_fn=<AddBackward0>)\n",
      "tensor(0.3858, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.6278, grad_fn=<AddBackward0>)\n",
      "tensor(0.6337, grad_fn=<AddBackward0>)\n",
      "tensor(0.6220, grad_fn=<AddBackward0>)\n",
      "tensor(0.5844, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.4642, grad_fn=<AddBackward0>)\n",
      "tensor(0.4822, grad_fn=<AddBackward0>)\n",
      "tensor(0.5083, grad_fn=<AddBackward0>)\n",
      "tensor(0.4371, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3854, grad_fn=<AddBackward0>)\n",
      "tensor(0.3432, grad_fn=<AddBackward0>)\n",
      "tensor(0.3940, grad_fn=<AddBackward0>)\n",
      "tensor(0.3247, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3468, grad_fn=<AddBackward0>)\n",
      "tensor(0.3862, grad_fn=<AddBackward0>)\n",
      "tensor(0.3819, grad_fn=<AddBackward0>)\n",
      "tensor(0.3695, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.3307, grad_fn=<AddBackward0>)\n",
      "tensor(0.1902, grad_fn=<AddBackward0>)\n",
      "tensor(0.2082, grad_fn=<AddBackward0>)\n",
      "tensor(0.2536, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1485, grad_fn=<AddBackward0>)\n",
      "tensor(0.1328, grad_fn=<AddBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "tensor(0.0743, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "tensor(0.0705, grad_fn=<AddBackward0>)\n",
      "tensor(0.0399, grad_fn=<AddBackward0>)\n",
      "tensor(0.0258, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2211, grad_fn=<AddBackward0>)\n",
      "tensor(0.1820, grad_fn=<AddBackward0>)\n",
      "tensor(0.1617, grad_fn=<AddBackward0>)\n",
      "tensor(0.1973, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1533, grad_fn=<AddBackward0>)\n",
      "tensor(0.2456, grad_fn=<AddBackward0>)\n",
      "tensor(0.0678, grad_fn=<AddBackward0>)\n",
      "tensor(0.2101, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2844, grad_fn=<AddBackward0>)\n",
      "tensor(0.2788, grad_fn=<AddBackward0>)\n",
      "tensor(0.2398, grad_fn=<AddBackward0>)\n",
      "tensor(0.2975, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2533, grad_fn=<AddBackward0>)\n",
      "tensor(0.1774, grad_fn=<AddBackward0>)\n",
      "tensor(0.3009, grad_fn=<AddBackward0>)\n",
      "tensor(0.1805, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2646, grad_fn=<AddBackward0>)\n",
      "tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "tensor(0.1960, grad_fn=<AddBackward0>)\n",
      "tensor(0.2428, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1242, grad_fn=<AddBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1270, grad_fn=<AddBackward0>)\n",
      "tensor(0.1413, grad_fn=<AddBackward0>)\n",
      "tensor(0.1771, grad_fn=<AddBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1613, grad_fn=<AddBackward0>)\n",
      "tensor(0.1510, grad_fn=<AddBackward0>)\n",
      "tensor(0.1513, grad_fn=<AddBackward0>)\n",
      "tensor(0.1470, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1729, grad_fn=<AddBackward0>)\n",
      "tensor(0.1857, grad_fn=<AddBackward0>)\n",
      "tensor(0.2137, grad_fn=<AddBackward0>)\n",
      "tensor(0.1930, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2397, grad_fn=<AddBackward0>)\n",
      "tensor(0.2926, grad_fn=<AddBackward0>)\n",
      "tensor(0.3050, grad_fn=<AddBackward0>)\n",
      "tensor(0.2734, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0836, grad_fn=<AddBackward0>)\n",
      "tensor(0.0593, grad_fn=<AddBackward0>)\n",
      "tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.1267, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0727, grad_fn=<AddBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2055, grad_fn=<AddBackward0>)\n",
      "tensor(0.1802, grad_fn=<AddBackward0>)\n",
      "tensor(0.1662, grad_fn=<AddBackward0>)\n",
      "tensor(0.1938, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1414, grad_fn=<AddBackward0>)\n",
      "tensor(0.1462, grad_fn=<AddBackward0>)\n",
      "tensor(0.1336, grad_fn=<AddBackward0>)\n",
      "tensor(0.1684, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1952, grad_fn=<AddBackward0>)\n",
      "tensor(0.1984, grad_fn=<AddBackward0>)\n",
      "tensor(0.1963, grad_fn=<AddBackward0>)\n",
      "tensor(0.1908, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1938, grad_fn=<AddBackward0>)\n",
      "tensor(0.2368, grad_fn=<AddBackward0>)\n",
      "tensor(0.2075, grad_fn=<AddBackward0>)\n",
      "tensor(0.2333, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1402, grad_fn=<AddBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "tensor(0.1280, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "tensor(0.1535, grad_fn=<AddBackward0>)\n",
      "tensor(0.1883, grad_fn=<AddBackward0>)\n",
      "tensor(0.1368, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1513, grad_fn=<AddBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "tensor(0.1708, grad_fn=<AddBackward0>)\n",
      "tensor(0.1424, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(6.7691, grad_fn=<AddBackward0>)\n",
      "tensor(8.5676, grad_fn=<AddBackward0>)\n",
      "tensor(9.6110, grad_fn=<AddBackward0>)\n",
      "tensor(5.7045, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0555, grad_fn=<AddBackward0>)\n",
      "tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "tensor(0.0801, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "tensor(0.1274, grad_fn=<AddBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1853, grad_fn=<AddBackward0>)\n",
      "tensor(0.1890, grad_fn=<AddBackward0>)\n",
      "tensor(0.1968, grad_fn=<AddBackward0>)\n",
      "tensor(0.1624, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(0.0106, grad_fn=<AddBackward0>)\n",
      "tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0049, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0650, grad_fn=<AddBackward0>)\n",
      "tensor(0.0217, grad_fn=<AddBackward0>)\n",
      "tensor(0.0747, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0585, grad_fn=<AddBackward0>)\n",
      "tensor(0.0094, grad_fn=<AddBackward0>)\n",
      "tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0219, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0669, grad_fn=<AddBackward0>)\n",
      "tensor(0.0884, grad_fn=<AddBackward0>)\n",
      "tensor(0.1348, grad_fn=<AddBackward0>)\n",
      "tensor(0.0230, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1108, grad_fn=<AddBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0677, grad_fn=<AddBackward0>)\n",
      "tensor(0.0792, grad_fn=<AddBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "tensor(0.0053, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2145, grad_fn=<AddBackward0>)\n",
      "tensor(0.1980, grad_fn=<AddBackward0>)\n",
      "tensor(0.1834, grad_fn=<AddBackward0>)\n",
      "tensor(0.1795, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "tensor(0.1336, grad_fn=<AddBackward0>)\n",
      "tensor(0.1508, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1523, grad_fn=<AddBackward0>)\n",
      "tensor(0.1271, grad_fn=<AddBackward0>)\n",
      "tensor(0.1824, grad_fn=<AddBackward0>)\n",
      "tensor(0.1347, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "tensor(0.0963, grad_fn=<AddBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1909, grad_fn=<AddBackward0>)\n",
      "tensor(0.1520, grad_fn=<AddBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "tensor(0.1540, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2155, grad_fn=<AddBackward0>)\n",
      "tensor(0.1469, grad_fn=<AddBackward0>)\n",
      "tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(14.9816, grad_fn=<AddBackward0>)\n",
      "tensor(6.4461, grad_fn=<AddBackward0>)\n",
      "tensor(9.2839, grad_fn=<AddBackward0>)\n",
      "tensor(11.7897, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0693, grad_fn=<AddBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>)\n",
      "tensor(0.0635, grad_fn=<AddBackward0>)\n",
      "tensor(0.0687, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "tensor(0.1279, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0452, grad_fn=<AddBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "tensor(0.1740, grad_fn=<AddBackward0>)\n",
      "tensor(0.1782, grad_fn=<AddBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0040, grad_fn=<AddBackward0>)\n",
      "tensor(0.0632, grad_fn=<AddBackward0>)\n",
      "tensor(0.0182, grad_fn=<AddBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2236, grad_fn=<AddBackward0>)\n",
      "tensor(0.0920, grad_fn=<AddBackward0>)\n",
      "tensor(0.1471, grad_fn=<AddBackward0>)\n",
      "tensor(0.1407, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1478, grad_fn=<AddBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2301, grad_fn=<AddBackward0>)\n",
      "tensor(0.1818, grad_fn=<AddBackward0>)\n",
      "tensor(0.2163, grad_fn=<AddBackward0>)\n",
      "tensor(0.2236, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1631, grad_fn=<AddBackward0>)\n",
      "tensor(0.1524, grad_fn=<AddBackward0>)\n",
      "tensor(0.1863, grad_fn=<AddBackward0>)\n",
      "tensor(0.1644, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2637, grad_fn=<AddBackward0>)\n",
      "tensor(0.2675, grad_fn=<AddBackward0>)\n",
      "tensor(0.2924, grad_fn=<AddBackward0>)\n",
      "tensor(0.2732, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1759, grad_fn=<AddBackward0>)\n",
      "tensor(0.2225, grad_fn=<AddBackward0>)\n",
      "tensor(0.1846, grad_fn=<AddBackward0>)\n",
      "tensor(0.2018, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1677, grad_fn=<AddBackward0>)\n",
      "tensor(0.1694, grad_fn=<AddBackward0>)\n",
      "tensor(0.1464, grad_fn=<AddBackward0>)\n",
      "tensor(0.1929, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0595, grad_fn=<AddBackward0>)\n",
      "tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0688, grad_fn=<AddBackward0>)\n",
      "tensor(0.0805, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "tensor(0.1333, grad_fn=<AddBackward0>)\n",
      "tensor(0.1182, grad_fn=<AddBackward0>)\n",
      "tensor(0.0697, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "tensor(0.1663, grad_fn=<AddBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1426, grad_fn=<AddBackward0>)\n",
      "tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "tensor(0.1177, grad_fn=<AddBackward0>)\n",
      "tensor(0.1336, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1850, grad_fn=<AddBackward0>)\n",
      "tensor(0.1858, grad_fn=<AddBackward0>)\n",
      "tensor(0.1606, grad_fn=<AddBackward0>)\n",
      "tensor(0.2168, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0694, grad_fn=<AddBackward0>)\n",
      "tensor(0.0564, grad_fn=<AddBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "tensor(0.0728, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1377, grad_fn=<AddBackward0>)\n",
      "tensor(0.1340, grad_fn=<AddBackward0>)\n",
      "tensor(0.1673, grad_fn=<AddBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "tensor(0.1531, grad_fn=<AddBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.1443, grad_fn=<AddBackward0>)\n",
      "tensor(0.0785, grad_fn=<AddBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1562, grad_fn=<AddBackward0>)\n",
      "tensor(0.1278, grad_fn=<AddBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "tensor(0.1361, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1766, grad_fn=<AddBackward0>)\n",
      "tensor(0.1929, grad_fn=<AddBackward0>)\n",
      "tensor(0.2002, grad_fn=<AddBackward0>)\n",
      "tensor(0.1830, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1301, grad_fn=<AddBackward0>)\n",
      "tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "tensor(0.1288, grad_fn=<AddBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "tensor(0.1597, grad_fn=<AddBackward0>)\n",
      "tensor(0.1280, grad_fn=<AddBackward0>)\n",
      "tensor(0.1603, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2055, grad_fn=<AddBackward0>)\n",
      "tensor(0.2314, grad_fn=<AddBackward0>)\n",
      "tensor(0.2197, grad_fn=<AddBackward0>)\n",
      "tensor(0.2283, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0429, grad_fn=<AddBackward0>)\n",
      "tensor(0.0752, grad_fn=<AddBackward0>)\n",
      "tensor(0.0595, grad_fn=<AddBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "tensor(0.1782, grad_fn=<AddBackward0>)\n",
      "tensor(0.1777, grad_fn=<AddBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2076, grad_fn=<AddBackward0>)\n",
      "tensor(0.2536, grad_fn=<AddBackward0>)\n",
      "tensor(0.2325, grad_fn=<AddBackward0>)\n",
      "tensor(0.2208, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(-0.0020, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "tensor(-0.0011, grad_fn=<AddBackward0>)\n",
      "tensor(0.0178, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "tensor(0.0637, grad_fn=<AddBackward0>)\n",
      "tensor(0.1458, grad_fn=<AddBackward0>)\n",
      "tensor(0.0605, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1474, grad_fn=<AddBackward0>)\n",
      "tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1326, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "tensor(0.1726, grad_fn=<AddBackward0>)\n",
      "tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2092, grad_fn=<AddBackward0>)\n",
      "tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "tensor(0.1985, grad_fn=<AddBackward0>)\n",
      "tensor(0.1749, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1410, grad_fn=<AddBackward0>)\n",
      "tensor(0.2559, grad_fn=<AddBackward0>)\n",
      "tensor(0.1898, grad_fn=<AddBackward0>)\n",
      "tensor(0.2119, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0666, grad_fn=<AddBackward0>)\n",
      "tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "tensor(0.0857, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "tensor(0.1368, grad_fn=<AddBackward0>)\n",
      "tensor(0.1661, grad_fn=<AddBackward0>)\n",
      "tensor(0.1296, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "tensor(0.1350, grad_fn=<AddBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "tensor(0.1552, grad_fn=<AddBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1246, grad_fn=<AddBackward0>)\n",
      "tensor(0.1711, grad_fn=<AddBackward0>)\n",
      "tensor(0.1950, grad_fn=<AddBackward0>)\n",
      "tensor(0.1312, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1449, grad_fn=<AddBackward0>)\n",
      "tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "tensor(0.1491, grad_fn=<AddBackward0>)\n",
      "tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1773, grad_fn=<AddBackward0>)\n",
      "tensor(0.1917, grad_fn=<AddBackward0>)\n",
      "tensor(0.1669, grad_fn=<AddBackward0>)\n",
      "tensor(0.1707, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(0.0683, grad_fn=<AddBackward0>)\n",
      "tensor(0.0431, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "tensor(0.1691, grad_fn=<AddBackward0>)\n",
      "tensor(0.1485, grad_fn=<AddBackward0>)\n",
      "tensor(0.1557, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "tensor(0.1501, grad_fn=<AddBackward0>)\n",
      "tensor(0.1371, grad_fn=<AddBackward0>)\n",
      "tensor(0.1484, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2247, grad_fn=<AddBackward0>)\n",
      "tensor(0.1926, grad_fn=<AddBackward0>)\n",
      "tensor(0.1772, grad_fn=<AddBackward0>)\n",
      "tensor(0.2064, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "tensor(0.1425, grad_fn=<AddBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1531, grad_fn=<AddBackward0>)\n",
      "tensor(0.1530, grad_fn=<AddBackward0>)\n",
      "tensor(0.1448, grad_fn=<AddBackward0>)\n",
      "tensor(0.1493, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1822, grad_fn=<AddBackward0>)\n",
      "tensor(0.2176, grad_fn=<AddBackward0>)\n",
      "tensor(0.1968, grad_fn=<AddBackward0>)\n",
      "tensor(0.1888, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1809, grad_fn=<AddBackward0>)\n",
      "tensor(0.1931, grad_fn=<AddBackward0>)\n",
      "tensor(0.2204, grad_fn=<AddBackward0>)\n",
      "tensor(0.2017, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1515, grad_fn=<AddBackward0>)\n",
      "tensor(0.1616, grad_fn=<AddBackward0>)\n",
      "tensor(0.1731, grad_fn=<AddBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1449, grad_fn=<AddBackward0>)\n",
      "tensor(0.1309, grad_fn=<AddBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1371, grad_fn=<AddBackward0>)\n",
      "tensor(0.1346, grad_fn=<AddBackward0>)\n",
      "tensor(0.2012, grad_fn=<AddBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1496, grad_fn=<AddBackward0>)\n",
      "tensor(0.1803, grad_fn=<AddBackward0>)\n",
      "tensor(0.1591, grad_fn=<AddBackward0>)\n",
      "tensor(0.1682, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0545, grad_fn=<AddBackward0>)\n",
      "tensor(0.1274, grad_fn=<AddBackward0>)\n",
      "tensor(0.0729, grad_fn=<AddBackward0>)\n",
      "tensor(0.1201, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1758, grad_fn=<AddBackward0>)\n",
      "tensor(0.1434, grad_fn=<AddBackward0>)\n",
      "tensor(0.1898, grad_fn=<AddBackward0>)\n",
      "tensor(0.1651, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2219, grad_fn=<AddBackward0>)\n",
      "tensor(0.1376, grad_fn=<AddBackward0>)\n",
      "tensor(0.1973, grad_fn=<AddBackward0>)\n",
      "tensor(0.2208, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.2030, grad_fn=<AddBackward0>)\n",
      "tensor(0.1613, grad_fn=<AddBackward0>)\n",
      "tensor(0.1592, grad_fn=<AddBackward0>)\n",
      "tensor(0.2141, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "tensor(0.1760, grad_fn=<AddBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "2048\n",
      "tensor(0.1748, grad_fn=<AddBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "tensor(0.1696, grad_fn=<AddBackward0>)\n",
      "tensor(0.1189, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# track returns\n",
    "all_returns = []\n",
    "\n",
    "# initialize the game\n",
    "cur_observation = torch.Tensor(envs.reset()[0]).to(device)\n",
    "cur_done = torch.zeros(configs.num_trajcts).to(device)\n",
    "\n",
    "# progress bar\n",
    "num_updates = configs.total_timesteps // configs.batch_size\n",
    "progress_bar = tqdm(total=num_updates)\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # Phase 1: rollout creation\n",
    "\n",
    "    # parallel envs creating trajectories\n",
    "    rollout = create_rollout(envs, cur_observation, cur_done, all_returns)\n",
    "\n",
    "    cur_done = rollout['cur_done']\n",
    "    cur_observation = rollout['cur_observation']\n",
    "    rewards = rollout['rewards']\n",
    "    dones = rollout['dones']\n",
    "    values = rollout['values']\n",
    "\n",
    "    # calculating advantages\n",
    "    advantages, returns = gae(cur_observation, rewards, dones, values)\n",
    "\n",
    "    # a dataset containing the rollouts\n",
    "    dataset = Storage(rollout, advantages, returns, envs)\n",
    "\n",
    "    print(len(dataset))\n",
    "    # a standard dataloader made out of current storage\n",
    "    trainloader = DataLoader(dataset, batch_size=configs.minibatch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # Phase 2: model update\n",
    "\n",
    "    # linearly shrink the lr from the initial lr to zero\n",
    "    frac = 1.0 - (update - 1.0) / num_updates\n",
    "    optimizer.param_groups[0][\"lr\"] = frac * configs.learning_rate\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(configs.update_epochs):\n",
    "        for batch in trainloader:\n",
    "            mb_observations, mb_logprobs, mb_actions, mb_advantages, mb_returns = batch\n",
    "\n",
    "            # we calculate the distribution of actions through the updated model revisiting the old trajectories\n",
    "            _, mb_newlogprob, mb_entropy, mb_newvalues = agent.policy(mb_observations, mb_actions)\n",
    "\n",
    "            policy_loss = loss_clip(mb_logprobs, mb_newlogprob, mb_advantages)\n",
    "\n",
    "            value_loss = loss_vf(mb_returns, mb_newvalues)\n",
    "\n",
    "            # average entory of the action space\n",
    "            entropy_loss = mb_entropy.mean()\n",
    "\n",
    "            # full weighted loss\n",
    "            loss = policy_loss - configs.ent_coef * entropy_loss + configs.vf_coef * value_loss\n",
    "\n",
    "            print(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # extra clipping of the gradients to avoid overshoots\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), configs.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "    # progress bar\n",
    "    if len(all_returns) > configs.num_returns_to_average:\n",
    "        progress_bar.set_description(f\"episode return: {np.mean(all_returns[-configs.num_returns_to_average:]):.2f}\")\n",
    "        progress_bar.refresh()\n",
    "        progress_bar.update()\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aba449",
   "metadata": {
    "id": "a5aba449"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1707cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "1f1707cc",
    "outputId": "d1250392-f4d1-4450-d5bf-94d9a42390bf",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     all_returns_truncated \u001b[38;5;241m=\u001b[39m all_returns\n\u001b[0;32m----> 5\u001b[0m all_returns_smoothed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(all_returns_truncated\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, configs\u001b[38;5;241m.\u001b[39mnum_episodes_to_average), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean reward:\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(all_returns_smoothed))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd reward:\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mstd(all_returns_smoothed))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "if not len(all_returns)%configs.num_episodes_to_average==0:\n",
    "    all_returns_truncated = np.array(all_returns[:-(len(all_returns)%configs.num_episodes_to_average)])\n",
    "else:\n",
    "    all_returns_truncated = all_returns\n",
    "all_returns_smoothed = np.average(all_returns_truncated.reshape(-1, configs.num_episodes_to_average), axis=1)\n",
    "print('mean reward:', np.mean(all_returns_smoothed))\n",
    "print('std reward:', np.std(all_returns_smoothed))\n",
    "print('max reward:', np.max(all_returns_smoothed))\n",
    "print('converge mean reward:', np.mean(all_returns_smoothed[-1]))\n",
    "plt.plot(all_returns_smoothed);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00113cc",
   "metadata": {
    "id": "b00113cc"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8fd76009",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "8fd76009",
    "outputId": "e808571d-c673-458b-9773-0860ce5232db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/cohlem/Projects/Experimentation/Deep Learning/RLHF/PPO/videos/inference folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,/content/videos/inference/rl-video-episode-0.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a test env\n",
    "test_env = make_env_func(configs.gym_id, seed, 0, 'inference', True)()\n",
    "\n",
    "# use the trained agent to run through the env till it terminates this is an eposide\n",
    "observation, _ = test_env.reset()\n",
    "observation = torch.unsqueeze(torch.tensor(observation),dim=0).to(device)\n",
    "for _ in range(500):\n",
    "    action, _, _, _ = agent.policy(observation)\n",
    "    action = action.cpu().item()\n",
    "    observation, reward, done, _, info = test_env.step(action)\n",
    "    observation = torch.unsqueeze(torch.tensor(observation),dim=0).to(device)\n",
    "    if done:\n",
    "        break\n",
    "test_env.close()\n",
    "\n",
    "Video('/content/videos/inference/rl-video-episode-0.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c5920cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "6c5920cf",
    "outputId": "f194e574-4afd-4795-8a0f-c4b34191e812"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,/content/videos/inference/rl-video-episode-0.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('/content/videos/inference/rl-video-episode-0.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42e803",
   "metadata": {
    "id": "ec42e803"
   },
   "source": [
    "### BreakOut\n",
    "\n",
    "You can use similar PPO setup to solve much more complex problems. The problem `\"gym_id\": \"BreakoutNoFrameskip-v4\"` is left as an exercise for you. The `env` and `agent` definitions are provided here but the rest are left to you. With the correct setup (very similar to what we did in CartPole), you should be able to get to respectable scores above 700. Even perfect score is possible with a tuned hyper-parameters with the same setup. Note that this is a much more complex problem and you will need to increase the `total_timesteps` by at least a factor of 30 to get to good results. This will take several hours.\n",
    "\n",
    "```python\n",
    "# extra imports\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv,\n",
    ")\n",
    "\n",
    "\n",
    "# env definition\n",
    "def make_env_func(gym_id, seed, idx, run_name, capture_video=False):\n",
    "    def env_fun():\n",
    "        env = gym.make(gym_id, render_mode='rgb_array')\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            # initiate the video capture if not already initiated\n",
    "            if idx == 0:\n",
    "                # wrapper to create the video of the performance\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return env_fun\n",
    "\n",
    "\n",
    "# Model defnition\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.AdaptiveMaxPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.value_head = layer_init(nn.Linear(512, 1), std=1)\n",
    "        self.policy_head = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)\n",
    "\n",
    "    def value_func(self, x):\n",
    "        return self.value_head(self.network(x / 255.0))\n",
    "\n",
    "    def policy(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.policy_head(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.value_head(hidden)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40cc78a",
   "metadata": {},
   "source": [
    "## Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d30a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an env with random state\n",
    "def make_env_func(gym_id, seed, idx, run_name, capture_video=False):\n",
    "    def env_fun():\n",
    "        env = gym.make(gym_id, render_mode=\"rgb_array\")\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            # initiate the video capture if not already initiated\n",
    "            if idx == 0:\n",
    "                # wrapper to create the video of the performance\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return env_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e801abd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_9366/1773201375.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cur_done = torch.tensor(torch.zeros(configs.num_trajcts))\n",
      "/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_9366/1773201375.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards[:,t] = torch.tensor(cur_reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.348729133605957, CLIP loss: -7.379720687866211, value function loss: 35.470760345458984, entropy loss: 0.6930763721466064\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.243270874023438, CLIP loss: -7.169240951538086, value function loss: 30.838886260986328, entropy loss: 0.6930961012840271\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.106352806091309, CLIP loss: -7.216591835021973, value function loss: 28.659748077392578, entropy loss: 0.6929196715354919\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.165403842926025, CLIP loss: -7.365848541259766, value function loss: 27.076356887817383, entropy loss: 0.6925910711288452\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.36641788482666, CLIP loss: -7.085920810699463, value function loss: 34.91851806640625, entropy loss: 0.6920415163040161\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.304258346557617, CLIP loss: -7.162054538726807, value function loss: 32.94645690917969, entropy loss: 0.6915996074676514\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.06437873840332, CLIP loss: -7.302496910095215, value function loss: 30.747573852539062, entropy loss: 0.6911313533782959\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.937123775482178, CLIP loss: -6.98167610168457, value function loss: 25.85140609741211, entropy loss: 0.6903004050254822\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.32398796081543, CLIP loss: -5.748469829559326, value function loss: 28.158708572387695, entropy loss: 0.6896597743034363\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.271900177001953, CLIP loss: -6.0193634033203125, value function loss: 26.596317291259766, entropy loss: 0.6895148754119873\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.37089729309082, CLIP loss: -5.864838600158691, value function loss: 22.48524284362793, entropy loss: 0.6885537505149841\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.197088718414307, CLIP loss: -5.917634963989258, value function loss: 20.243202209472656, entropy loss: 0.6877212524414062\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.122284889221191, CLIP loss: -5.434487342834473, value function loss: 29.127286911010742, entropy loss: 0.6871165037155151\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.057267665863037, CLIP loss: -5.303354263305664, value function loss: 24.734954833984375, entropy loss: 0.6855429410934448\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.279369831085205, CLIP loss: -5.346031665802002, value function loss: 21.26447868347168, entropy loss: 0.6838014125823975\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.8136770725250244, CLIP loss: -5.4221906661987305, value function loss: 18.485387802124023, entropy loss: 0.6826120018959045\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.949772834777832, CLIP loss: -3.8362600803375244, value function loss: 27.585735321044922, entropy loss: 0.6834511160850525\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.363040447235107, CLIP loss: -4.225957870483398, value function loss: 23.191654205322266, entropy loss: 0.6829007267951965\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.587028503417969, CLIP loss: -3.962887763977051, value function loss: 21.11347198486328, entropy loss: 0.6819934844970703\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.576302528381348, CLIP loss: -4.054020881652832, value function loss: 19.274248123168945, entropy loss: 0.6800704598426819\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.023905754089355, CLIP loss: -2.2075276374816895, value function loss: 28.476457595825195, entropy loss: 0.6794897317886353\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.915525436401367, CLIP loss: -1.5716410875320435, value function loss: 24.987899780273438, entropy loss: 0.6783270239830017\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.371006965637207, CLIP loss: -2.012430429458618, value function loss: 24.78034782409668, entropy loss: 0.673711895942688\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.517096519470215, CLIP loss: -1.8066678047180176, value function loss: 28.661026000976562, entropy loss: 0.6748934388160706\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.312278747558594, CLIP loss: 0.3266359269618988, value function loss: 35.98475646972656, entropy loss: 0.6733914613723755\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.055885314941406, CLIP loss: 0.6580941081047058, value function loss: 36.80901336669922, entropy loss: 0.6716541647911072\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.293241500854492, CLIP loss: 0.7715930938720703, value function loss: 39.056678771972656, entropy loss: 0.6690852642059326\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.86332893371582, CLIP loss: 0.3402242660522461, value function loss: 35.059547424316406, entropy loss: 0.6669101715087891\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.729679107666016, CLIP loss: 1.0944825410842896, value function loss: 39.283790588378906, entropy loss: 0.6699063181877136\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.97361183166504, CLIP loss: 0.8487088680267334, value function loss: 36.263179779052734, entropy loss: 0.6686732769012451\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.400524139404297, CLIP loss: 1.0373198986053467, value function loss: 38.73978805541992, entropy loss: 0.6688961386680603\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.49873924255371, CLIP loss: 0.8136011958122253, value function loss: 35.38361358642578, entropy loss: 0.6668981909751892\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.947267532348633, CLIP loss: 1.2026677131652832, value function loss: 33.50251770019531, entropy loss: 0.6657640933990479\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.219131469726562, CLIP loss: 1.6702468395233154, value function loss: 33.111000061035156, entropy loss: 0.6614788770675659\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.475549697875977, CLIP loss: 1.0842045545578003, value function loss: 30.795930862426758, entropy loss: 0.6620519757270813\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.074050903320312, CLIP loss: 1.7848820686340332, value function loss: 32.59157180786133, entropy loss: 0.6617506146430969\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.37055778503418, CLIP loss: -1.228327989578247, value function loss: 33.21104431152344, entropy loss: 0.6636313199996948\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.561498641967773, CLIP loss: -1.706480860710144, value function loss: 32.549110412597656, entropy loss: 0.6575247049331665\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.612483978271484, CLIP loss: -1.2817206382751465, value function loss: 31.80141830444336, entropy loss: 0.6503943204879761\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.72305679321289, CLIP loss: -1.6377477645874023, value function loss: 32.7344970703125, entropy loss: 0.6443983912467957\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.734745979309082, CLIP loss: -0.4390510618686676, value function loss: 30.360801696777344, entropy loss: 0.6604577302932739\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.930773735046387, CLIP loss: -0.246482253074646, value function loss: 30.36771011352539, entropy loss: 0.6599336266517639\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.269353866577148, CLIP loss: -0.42552828788757324, value function loss: 29.402925491333008, entropy loss: 0.6580114364624023\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.451168060302734, CLIP loss: -0.32208243012428284, value function loss: 29.559619903564453, entropy loss: 0.655925989151001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.944039344787598, CLIP loss: -0.7599526047706604, value function loss: 33.42106628417969, entropy loss: 0.6541071534156799\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.573347091674805, CLIP loss: -1.3486592769622803, value function loss: 33.85710144042969, entropy loss: 0.6543787717819214\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.704325675964355, CLIP loss: -1.150803804397583, value function loss: 33.72331237792969, entropy loss: 0.6526653170585632\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.707890510559082, CLIP loss: -0.9192712306976318, value function loss: 31.267274856567383, entropy loss: 0.6475383639335632\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.696914672851562, CLIP loss: -1.1446342468261719, value function loss: 37.696044921875, entropy loss: 0.6473550796508789\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.429353713989258, CLIP loss: -1.6352241039276123, value function loss: 38.14207458496094, entropy loss: 0.646049976348877\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.933807373046875, CLIP loss: -1.5009088516235352, value function loss: 36.882293701171875, entropy loss: 0.6432417631149292\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.71942710876465, CLIP loss: -1.4552313089370728, value function loss: 36.362037658691406, entropy loss: 0.6360378265380859\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.919158935546875, CLIP loss: 0.2551138401031494, value function loss: 43.34088134765625, entropy loss: 0.6396149396896362\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.734010696411133, CLIP loss: -0.31803667545318604, value function loss: 42.116783142089844, entropy loss: 0.6343389749526978\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.713743209838867, CLIP loss: -0.04437318816781044, value function loss: 41.52881622314453, entropy loss: 0.6291764378547668\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.052824020385742, CLIP loss: 0.1279733031988144, value function loss: 41.86223602294922, entropy loss: 0.6267942190170288\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.701892852783203, CLIP loss: 1.3207443952560425, value function loss: 48.774898529052734, entropy loss: 0.6300274133682251\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.40971565246582, CLIP loss: 1.2249393463134766, value function loss: 44.381954193115234, entropy loss: 0.6201481819152832\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.48761558532715, CLIP loss: 0.9748309254646301, value function loss: 45.03791809082031, entropy loss: 0.6173715591430664\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.25229835510254, CLIP loss: 1.5708433389663696, value function loss: 45.375205993652344, entropy loss: 0.614747166633606\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.81949806213379, CLIP loss: -0.7795717716217041, value function loss: 39.21061706542969, entropy loss: 0.6239317655563354\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.580141067504883, CLIP loss: 0.13519558310508728, value function loss: 40.90232849121094, entropy loss: 0.6217234134674072\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.319440841674805, CLIP loss: -0.22934435307979584, value function loss: 39.109962463378906, entropy loss: 0.6196373105049133\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.76006507873535, CLIP loss: -0.3367229700088501, value function loss: 40.20595169067383, entropy loss: 0.6186673641204834\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.943050384521484, CLIP loss: -0.6724669337272644, value function loss: 41.243377685546875, entropy loss: 0.6171681880950928\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.069028854370117, CLIP loss: -1.3404157161712646, value function loss: 38.8311882019043, entropy loss: 0.6149734854698181\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.49652671813965, CLIP loss: -1.0173425674438477, value function loss: 41.03987121582031, entropy loss: 0.6066882610321045\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.56143569946289, CLIP loss: -1.077641248703003, value function loss: 39.29024124145508, entropy loss: 0.6043992638587952\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.243627548217773, CLIP loss: -2.2483468055725098, value function loss: 42.996192932128906, entropy loss: 0.6122102737426758\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.681467056274414, CLIP loss: -2.0669522285461426, value function loss: 45.509056091308594, entropy loss: 0.6108865141868591\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.454675674438477, CLIP loss: -1.9519340991973877, value function loss: 40.82525634765625, entropy loss: 0.6017142534255981\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.59470558166504, CLIP loss: -2.33670711517334, value function loss: 39.87468338012695, entropy loss: 0.5929588079452515\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.45623779296875, CLIP loss: -0.80577552318573, value function loss: 44.53610610961914, entropy loss: 0.6041040420532227\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.309547424316406, CLIP loss: -1.0531777143478394, value function loss: 42.73755645751953, entropy loss: 0.6054701805114746\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.116458892822266, CLIP loss: -0.5978065133094788, value function loss: 45.44023895263672, entropy loss: 0.5853692293167114\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.35748291015625, CLIP loss: -1.3061798810958862, value function loss: 39.33892059326172, entropy loss: 0.5798178911209106\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.163803100585938, CLIP loss: 1.1669433116912842, value function loss: 56.00554275512695, entropy loss: 0.5911436080932617\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.965402603149414, CLIP loss: 0.9722650051116943, value function loss: 51.99816131591797, entropy loss: 0.5944162607192993\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.150672912597656, CLIP loss: 0.819741427898407, value function loss: 52.6735725402832, entropy loss: 0.58555006980896\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.92604637145996, CLIP loss: 1.4044502973556519, value function loss: 53.054840087890625, entropy loss: 0.5823776125907898\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.23727035522461, CLIP loss: -2.6779048442840576, value function loss: 47.84220886230469, entropy loss: 0.5930741429328918\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.835641860961914, CLIP loss: -2.6648285388946533, value function loss: 41.0129508972168, entropy loss: 0.6005228757858276\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.35500144958496, CLIP loss: -2.8751015663146973, value function loss: 40.4720458984375, entropy loss: 0.5921176075935364\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.96980857849121, CLIP loss: -2.540240526199341, value function loss: 43.03190994262695, entropy loss: 0.5904726982116699\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.511171340942383, CLIP loss: -1.7363702058792114, value function loss: 42.50695037841797, entropy loss: 0.5933791399002075\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.808204650878906, CLIP loss: -1.585894227027893, value function loss: 40.79998779296875, entropy loss: 0.5896508097648621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.09221839904785, CLIP loss: -2.036982297897339, value function loss: 38.2701301574707, entropy loss: 0.586458683013916\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.072446823120117, CLIP loss: -1.3047281503677368, value function loss: 42.76608657836914, entropy loss: 0.5868306159973145\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.104907989501953, CLIP loss: -1.9703298807144165, value function loss: 38.1622314453125, entropy loss: 0.5877543091773987\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.065033912658691, CLIP loss: -2.565640449523926, value function loss: 33.27303695678711, entropy loss: 0.5844131708145142\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.577067375183105, CLIP loss: -2.1921632289886475, value function loss: 33.55002975463867, entropy loss: 0.5784040689468384\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.043964385986328, CLIP loss: -2.2585325241088867, value function loss: 34.616512298583984, entropy loss: 0.5759286880493164\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.246585845947266, CLIP loss: -1.342717170715332, value function loss: 47.19024658203125, entropy loss: 0.5822006464004517\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.04463768005371, CLIP loss: -1.5822635889053345, value function loss: 45.265350341796875, entropy loss: 0.5772998332977295\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.473127365112305, CLIP loss: -1.2455910444259644, value function loss: 43.448917388916016, entropy loss: 0.5740354657173157\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.42790412902832, CLIP loss: -1.4411332607269287, value function loss: 47.74928665161133, entropy loss: 0.5605499744415283\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.065818786621094, CLIP loss: -0.6661458015441895, value function loss: 53.475521087646484, entropy loss: 0.5797361135482788\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.160226821899414, CLIP loss: -1.1838922500610352, value function loss: 46.699676513671875, entropy loss: 0.5720524191856384\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.193737030029297, CLIP loss: -0.8419546484947205, value function loss: 52.08283615112305, entropy loss: 0.5725162029266357\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.71489143371582, CLIP loss: -0.9937336444854736, value function loss: 47.428165435791016, entropy loss: 0.5457696318626404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.242511749267578, CLIP loss: -0.6152715086936951, value function loss: 49.72694396972656, entropy loss: 0.5689465999603271\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.560527801513672, CLIP loss: -0.7988200187683105, value function loss: 40.72986602783203, entropy loss: 0.5584712624549866\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.425580978393555, CLIP loss: -0.728130042552948, value function loss: 46.318634033203125, entropy loss: 0.5604838132858276\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.790634155273438, CLIP loss: -0.9771891236305237, value function loss: 45.54661560058594, entropy loss: 0.5482860803604126\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.039730072021484, CLIP loss: -2.4190454483032227, value function loss: 38.92913055419922, entropy loss: 0.5789790749549866\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.817115783691406, CLIP loss: -2.3875279426574707, value function loss: 44.420570373535156, entropy loss: 0.5641686916351318\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.1311092376709, CLIP loss: -2.2961089611053467, value function loss: 40.865474700927734, entropy loss: 0.552081823348999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.162253379821777, CLIP loss: -2.491988182067871, value function loss: 35.3194465637207, entropy loss: 0.548187255859375\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 47.72381591796875, CLIP loss: 3.451897621154785, value function loss: 88.55467987060547, entropy loss: 0.5421142578125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 46.410640716552734, CLIP loss: 2.986018180847168, value function loss: 86.86007690429688, entropy loss: 0.5415061116218567\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 47.023494720458984, CLIP loss: 3.213346481323242, value function loss: 87.63090515136719, entropy loss: 0.5304337739944458\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 46.558982849121094, CLIP loss: 3.223299741744995, value function loss: 86.68099975585938, entropy loss: 0.4818887412548065\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 44.28974914550781, CLIP loss: 3.446895122528076, value function loss: 81.69642639160156, entropy loss: 0.5359534025192261\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 51.63825225830078, CLIP loss: 4.427103519439697, value function loss: 94.43305969238281, entropy loss: 0.5382130146026611\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 48.39141082763672, CLIP loss: 3.639307975769043, value function loss: 89.51499938964844, entropy loss: 0.5396199822425842\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 41.750850677490234, CLIP loss: 3.7005512714385986, value function loss: 76.11134338378906, entropy loss: 0.537239134311676\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.105798721313477, CLIP loss: -1.1790549755096436, value function loss: 38.58082962036133, entropy loss: 0.5561183094978333\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.308725357055664, CLIP loss: -0.5713604688644409, value function loss: 39.77102279663086, entropy loss: 0.5425882935523987\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.276628494262695, CLIP loss: -0.9076508283615112, value function loss: 38.379207611083984, entropy loss: 0.5324384570121765\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.473896026611328, CLIP loss: -0.6420365571975708, value function loss: 42.24214172363281, entropy loss: 0.5138092041015625\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.882545471191406, CLIP loss: 2.1173391342163086, value function loss: 57.54155731201172, entropy loss: 0.5573006272315979\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.216623306274414, CLIP loss: 1.29039466381073, value function loss: 59.86354064941406, entropy loss: 0.5541741847991943\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.23647117614746, CLIP loss: 1.8884010314941406, value function loss: 58.707210540771484, entropy loss: 0.5535607933998108\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.545801162719727, CLIP loss: 1.5180329084396362, value function loss: 58.066368103027344, entropy loss: 0.5414565205574036\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.769071578979492, CLIP loss: -4.320765972137451, value function loss: 30.19055938720703, entropy loss: 0.5441400408744812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.003545761108398, CLIP loss: -4.161962985992432, value function loss: 32.341617584228516, entropy loss: 0.5300155878067017\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.900362014770508, CLIP loss: -4.244670391082764, value function loss: 28.300689697265625, entropy loss: 0.5311555862426758\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.651327133178711, CLIP loss: -4.221066474914551, value function loss: 31.755287170410156, entropy loss: 0.5250136852264404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.905291557312012, CLIP loss: -2.289132595062256, value function loss: 32.399925231933594, entropy loss: 0.5538205504417419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.473060607910156, CLIP loss: -1.9907119274139404, value function loss: 32.93883514404297, entropy loss: 0.5644832849502563\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.008715629577637, CLIP loss: -2.196770668029785, value function loss: 30.422138214111328, entropy loss: 0.5583270788192749\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.38146686553955, CLIP loss: -2.1364383697509766, value function loss: 31.0470027923584, entropy loss: 0.5595700740814209\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.919726371765137, CLIP loss: -1.0064388513565063, value function loss: 33.86322021484375, entropy loss: 0.5444563031196594\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.620643615722656, CLIP loss: -0.7727962136268616, value function loss: 36.79756164550781, entropy loss: 0.5340909957885742\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.11302661895752, CLIP loss: -0.9800925254821777, value function loss: 30.19702911376953, entropy loss: 0.5395354628562927\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.530868530273438, CLIP loss: -0.8959511518478394, value function loss: 36.86410140991211, entropy loss: 0.5231790542602539\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.87887954711914, CLIP loss: -1.839321494102478, value function loss: 35.44709014892578, entropy loss: 0.5344635844230652\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.53242301940918, CLIP loss: -1.3499802350997925, value function loss: 37.77562713623047, entropy loss: 0.541130006313324\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.377124786376953, CLIP loss: -1.5525251626968384, value function loss: 35.86991500854492, entropy loss: 0.5308225750923157\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.085243225097656, CLIP loss: -1.3410918712615967, value function loss: 34.86320114135742, entropy loss: 0.5265715718269348\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.12912368774414, CLIP loss: -0.17445355653762817, value function loss: 42.617984771728516, entropy loss: 0.5414129495620728\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.906135559082031, CLIP loss: -0.9401224255561829, value function loss: 33.703102111816406, entropy loss: 0.5293276906013489\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.18174934387207, CLIP loss: -0.254098117351532, value function loss: 44.88239288330078, entropy loss: 0.5348863005638123\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.747333526611328, CLIP loss: -1.0715370178222656, value function loss: 33.64855194091797, entropy loss: 0.540524423122406\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.15926742553711, CLIP loss: -1.776900053024292, value function loss: 35.88301086425781, entropy loss: 0.533927857875824\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.924578666687012, CLIP loss: -1.6146667003631592, value function loss: 31.08920669555664, entropy loss: 0.5357658267021179\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.918241500854492, CLIP loss: -1.8899650573730469, value function loss: 27.62704849243164, entropy loss: 0.5317581295967102\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.527759552001953, CLIP loss: -1.3200433254241943, value function loss: 37.70630645751953, entropy loss: 0.5349780321121216\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.524030685424805, CLIP loss: 0.68485426902771, value function loss: 57.68902587890625, entropy loss: 0.5336119532585144\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.763952255249023, CLIP loss: 0.5313423871994019, value function loss: 54.47589874267578, entropy loss: 0.5339283347129822\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.119915008544922, CLIP loss: 0.8480640053749084, value function loss: 56.554405212402344, entropy loss: 0.5352373719215393\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.111967086791992, CLIP loss: 0.5009896159172058, value function loss: 57.232627868652344, entropy loss: 0.5337380766868591\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 40.26647186279297, CLIP loss: 1.9800615310668945, value function loss: 76.5833740234375, entropy loss: 0.5276966691017151\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.54050064086914, CLIP loss: 1.0856101512908936, value function loss: 54.92049026489258, entropy loss: 0.5353884696960449\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 32.51960372924805, CLIP loss: 0.8379297256469727, value function loss: 63.37385940551758, entropy loss: 0.525833010673523\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 35.73613357543945, CLIP loss: 1.4914437532424927, value function loss: 68.49978637695312, entropy loss: 0.5204101204872131\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 61.422821044921875, CLIP loss: 4.425703048706055, value function loss: 114.00473022460938, entropy loss: 0.5247711539268494\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 55.57670974731445, CLIP loss: 4.428097248077393, value function loss: 102.30758666992188, entropy loss: 0.518182098865509\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 53.6312370300293, CLIP loss: 4.248340129852295, value function loss: 98.77629089355469, entropy loss: 0.5247865915298462\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 61.66153335571289, CLIP loss: 4.498027324676514, value function loss: 114.33760070800781, entropy loss: 0.529329240322113\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.26274871826172, CLIP loss: -1.287863850593567, value function loss: 39.11207962036133, entropy loss: 0.542617917060852\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.867244720458984, CLIP loss: -1.178114652633667, value function loss: 36.101165771484375, entropy loss: 0.5223437547683716\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.43840980529785, CLIP loss: -1.1794445514678955, value function loss: 37.24620056152344, entropy loss: 0.5245049595832825\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.025651931762695, CLIP loss: -0.9505404233932495, value function loss: 35.96281051635742, entropy loss: 0.5213049054145813\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.56014633178711, CLIP loss: -1.804304599761963, value function loss: 42.73981475830078, entropy loss: 0.5457680225372314\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.17559242248535, CLIP loss: -1.3803088665008545, value function loss: 43.12261962890625, entropy loss: 0.541006863117218\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.272010803222656, CLIP loss: -1.412001132965088, value function loss: 41.37886428833008, entropy loss: 0.5421339273452759\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.949199676513672, CLIP loss: -1.6104363203048706, value function loss: 41.13019561767578, entropy loss: 0.5463021397590637\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.99443817138672, CLIP loss: -1.4174137115478516, value function loss: 36.834739685058594, entropy loss: 0.5518909096717834\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.70958137512207, CLIP loss: -1.8656013011932373, value function loss: 31.16114044189453, entropy loss: 0.5387135744094849\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.41295337677002, CLIP loss: -1.7357228994369507, value function loss: 34.308143615722656, entropy loss: 0.5395489931106567\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.752237319946289, CLIP loss: -1.4899357557296753, value function loss: 30.49513816833496, entropy loss: 0.5395990610122681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.304222106933594, CLIP loss: -2.2673513889312744, value function loss: 45.1539421081543, entropy loss: 0.5397313237190247\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.790607452392578, CLIP loss: -1.234362244606018, value function loss: 40.06085205078125, entropy loss: 0.5456689596176147\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.702848434448242, CLIP loss: -1.6493875980377197, value function loss: 42.71504592895508, entropy loss: 0.5286309719085693\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.168212890625, CLIP loss: -1.7113147974014282, value function loss: 41.769813537597656, entropy loss: 0.537956178188324\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.68878746032715, CLIP loss: 1.075148582458496, value function loss: 51.23793411254883, entropy loss: 0.5326797962188721\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 35.32471466064453, CLIP loss: 1.5082262754440308, value function loss: 67.64359283447266, entropy loss: 0.5304360389709473\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 36.610774993896484, CLIP loss: 1.4741891622543335, value function loss: 70.28341674804688, entropy loss: 0.5121437907218933\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.19248390197754, CLIP loss: 0.8608277440071106, value function loss: 50.673553466796875, entropy loss: 0.5122113823890686\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.786327362060547, CLIP loss: 1.528536319732666, value function loss: 56.52610397338867, entropy loss: 0.5260766744613647\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.567798614501953, CLIP loss: 0.9339052438735962, value function loss: 49.278106689453125, entropy loss: 0.5158550143241882\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.248918533325195, CLIP loss: 1.2520511150360107, value function loss: 52.00408935546875, entropy loss: 0.5176826119422913\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.719200134277344, CLIP loss: 1.0545437335968018, value function loss: 51.33969497680664, entropy loss: 0.5192638635635376\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.802417755126953, CLIP loss: -0.2235487699508667, value function loss: 42.06270217895508, entropy loss: 0.538420557975769\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.625381469726562, CLIP loss: 0.22469079494476318, value function loss: 38.8121223449707, entropy loss: 0.5371511578559875\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.02155303955078, CLIP loss: -0.20633919537067413, value function loss: 38.466461181640625, entropy loss: 0.5338355302810669\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.707462310791016, CLIP loss: 0.11455642431974411, value function loss: 39.19663619995117, entropy loss: 0.5413365364074707\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.824119567871094, CLIP loss: -1.162985920906067, value function loss: 37.985015869140625, entropy loss: 0.540100634098053\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.527667999267578, CLIP loss: -1.347488284111023, value function loss: 37.76081085205078, entropy loss: 0.5248376131057739\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.06104278564453, CLIP loss: -1.271972417831421, value function loss: 36.676326751708984, entropy loss: 0.5148885846138\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.829832077026367, CLIP loss: -1.3859208822250366, value function loss: 36.44178771972656, entropy loss: 0.5139887928962708\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 57.11392593383789, CLIP loss: 4.973599433898926, value function loss: 104.29127502441406, entropy loss: 0.5310702323913574\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 62.02541732788086, CLIP loss: 5.494361400604248, value function loss: 113.07257080078125, entropy loss: 0.5228796601295471\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 55.54086685180664, CLIP loss: 5.103399753570557, value function loss: 100.8851089477539, entropy loss: 0.5088374614715576\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 60.519039154052734, CLIP loss: 5.083349227905273, value function loss: 110.88143920898438, entropy loss: 0.5031368732452393\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.34151840209961, CLIP loss: 0.5349080562591553, value function loss: 53.62383270263672, entropy loss: 0.530560314655304\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.226716995239258, CLIP loss: 1.2480918169021606, value function loss: 49.967918395996094, entropy loss: 0.5335234999656677\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.266698837280273, CLIP loss: 0.5769549012184143, value function loss: 49.390235900878906, entropy loss: 0.5375304818153381\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.277076721191406, CLIP loss: 1.2267794609069824, value function loss: 54.11115264892578, entropy loss: 0.5280480980873108\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.36760711669922, CLIP loss: -0.9812448024749756, value function loss: 42.708377838134766, entropy loss: 0.5335929989814758\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.996349334716797, CLIP loss: -1.5178442001342773, value function loss: 41.038856506347656, entropy loss: 0.523391604423523\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.8541202545166, CLIP loss: -1.362133502960205, value function loss: 42.442813873291016, entropy loss: 0.515440046787262\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.045207977294922, CLIP loss: -1.1192952394485474, value function loss: 38.33940124511719, entropy loss: 0.5197701454162598\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.30854320526123, CLIP loss: -1.0471690893173218, value function loss: 32.72223663330078, entropy loss: 0.5406049489974976\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.627605438232422, CLIP loss: -0.7627476453781128, value function loss: 32.791473388671875, entropy loss: 0.5383497476577759\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.35676383972168, CLIP loss: -1.4651012420654297, value function loss: 27.654260635375977, entropy loss: 0.5265092253684998\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.606904983520508, CLIP loss: -0.6504520773887634, value function loss: 36.5253791809082, entropy loss: 0.5333755016326904\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.2939510345459, CLIP loss: 0.4222547709941864, value function loss: 41.75414276123047, entropy loss: 0.5374472141265869\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.643115997314453, CLIP loss: 0.06044384837150574, value function loss: 39.175933837890625, entropy loss: 0.5293895602226257\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.661914825439453, CLIP loss: 0.07979753613471985, value function loss: 39.174835205078125, entropy loss: 0.5301089286804199\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.471933364868164, CLIP loss: 0.19281838834285736, value function loss: 42.56877517700195, entropy loss: 0.5272018313407898\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.745139122009277, CLIP loss: -1.4988187551498413, value function loss: 34.49861526489258, entropy loss: 0.534965991973877\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.57301139831543, CLIP loss: -1.3273521661758423, value function loss: 31.81146240234375, entropy loss: 0.5367488861083984\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.07759952545166, CLIP loss: -1.14677095413208, value function loss: 32.459415435791016, entropy loss: 0.5338029861450195\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.807865142822266, CLIP loss: -1.6021009683609009, value function loss: 30.830745697021484, entropy loss: 0.5406163334846497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.547203063964844, CLIP loss: -1.7149505615234375, value function loss: 30.53507423400879, entropy loss: 0.5383638739585876\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.808194160461426, CLIP loss: -1.379618525505066, value function loss: 24.386444091796875, entropy loss: 0.5408948063850403\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.190704345703125, CLIP loss: -1.5530836582183838, value function loss: 25.49832534790039, entropy loss: 0.5375353097915649\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.545846939086914, CLIP loss: -1.7130708694458008, value function loss: 26.52831268310547, entropy loss: 0.5238770246505737\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 35.4311637878418, CLIP loss: 1.3872828483581543, value function loss: 68.09848022460938, entropy loss: 0.5360649228096008\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.4025936126709, CLIP loss: 0.3753986358642578, value function loss: 44.06513977050781, entropy loss: 0.5375381708145142\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.327177047729492, CLIP loss: 0.7506817579269409, value function loss: 51.163612365722656, entropy loss: 0.5310748815536499\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.974397659301758, CLIP loss: 1.0429317951202393, value function loss: 57.8734016418457, entropy loss: 0.5235775709152222\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.16196060180664, CLIP loss: 0.17193147540092468, value function loss: 61.99079895019531, entropy loss: 0.5370822548866272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 32.63063049316406, CLIP loss: 0.2822146713733673, value function loss: 64.70735168457031, entropy loss: 0.5260547995567322\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.00044822692871, CLIP loss: 0.24216848611831665, value function loss: 59.526893615722656, entropy loss: 0.5166900157928467\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 34.746925354003906, CLIP loss: 0.28610309958457947, value function loss: 68.93213653564453, entropy loss: 0.5245698690414429\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.35771942138672, CLIP loss: -0.8239806890487671, value function loss: 40.37383270263672, entropy loss: 0.5215754508972168\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.863786697387695, CLIP loss: -1.3203003406524658, value function loss: 42.37862014770508, entropy loss: 0.5223060846328735\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.528099060058594, CLIP loss: -0.6466583609580994, value function loss: 42.36009216308594, entropy loss: 0.5289342999458313\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.399429321289062, CLIP loss: -1.3827805519104004, value function loss: 39.574676513671875, entropy loss: 0.5129764080047607\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.153168678283691, CLIP loss: -1.120079755783081, value function loss: 32.55686950683594, entropy loss: 0.5186474323272705\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.330364227294922, CLIP loss: -1.6542890071868896, value function loss: 33.97979736328125, entropy loss: 0.5245193243026733\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.23852825164795, CLIP loss: -1.4652169942855835, value function loss: 31.417999267578125, entropy loss: 0.5254449844360352\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.774831771850586, CLIP loss: -1.3010051250457764, value function loss: 30.162124633789062, entropy loss: 0.5225438475608826\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 46.09113311767578, CLIP loss: 2.5028951168060303, value function loss: 87.18722534179688, entropy loss: 0.5374648571014404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 38.652320861816406, CLIP loss: 1.7971673011779785, value function loss: 73.72087097167969, entropy loss: 0.5281556248664856\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 43.7618408203125, CLIP loss: 2.3544206619262695, value function loss: 82.82537078857422, entropy loss: 0.5266104936599731\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 37.802894592285156, CLIP loss: 2.4464271068573, value function loss: 70.72360229492188, entropy loss: 0.5332967042922974\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.366178512573242, CLIP loss: -0.8475953936576843, value function loss: 26.437997817993164, entropy loss: 0.5225476026535034\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.28928565979004, CLIP loss: -0.5707094669342041, value function loss: 35.73029327392578, entropy loss: 0.5151328444480896\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.544435501098633, CLIP loss: -0.831056535243988, value function loss: 30.761404037475586, entropy loss: 0.5209689140319824\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.168768882751465, CLIP loss: -0.5738445520401001, value function loss: 31.495708465576172, entropy loss: 0.5240440368652344\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.202531814575195, CLIP loss: 0.5946056842803955, value function loss: 55.226558685302734, entropy loss: 0.5354825854301453\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 42.042903900146484, CLIP loss: 1.755150556564331, value function loss: 80.58596801757812, entropy loss: 0.5230448246002197\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 42.261085510253906, CLIP loss: 1.4099817276000977, value function loss: 81.71281433105469, entropy loss: 0.5303939580917358\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.7191104888916, CLIP loss: 0.7005369067192078, value function loss: 54.0475959777832, entropy loss: 0.5223871469497681\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.565509796142578, CLIP loss: 0.11804649978876114, value function loss: 32.905555725097656, entropy loss: 0.5314692258834839\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.682113647460938, CLIP loss: 0.09596014022827148, value function loss: 41.182926177978516, entropy loss: 0.5309509038925171\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.221147537231445, CLIP loss: 0.49920809268951416, value function loss: 39.45459747314453, entropy loss: 0.5359150767326355\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.94120979309082, CLIP loss: -0.3869185447692871, value function loss: 32.66674041748047, entropy loss: 0.5241502523422241\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.248950958251953, CLIP loss: -0.1353873908519745, value function loss: 50.77919006347656, entropy loss: 0.5256465673446655\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.30626678466797, CLIP loss: 0.09391820430755615, value function loss: 46.43507766723633, entropy loss: 0.5190312266349792\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.84917640686035, CLIP loss: -0.43299543857574463, value function loss: 48.57477569580078, entropy loss: 0.5217213034629822\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.14167594909668, CLIP loss: 0.4884653389453888, value function loss: 47.316646575927734, entropy loss: 0.5112905502319336\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.786205291748047, CLIP loss: 1.107875108718872, value function loss: 51.3670654296875, entropy loss: 0.5203382968902588\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.594974517822266, CLIP loss: 0.12099505960941315, value function loss: 46.95832824707031, entropy loss: 0.5184294581413269\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.87640380859375, CLIP loss: 0.6084656119346619, value function loss: 50.546119689941406, entropy loss: 0.5121707916259766\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.237518310546875, CLIP loss: 0.4059920012950897, value function loss: 43.67319107055664, entropy loss: 0.5069297552108765\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 37.61582946777344, CLIP loss: 1.5585126876831055, value function loss: 72.12500762939453, entropy loss: 0.5188136696815491\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.68168067932129, CLIP loss: 0.6371071338653564, value function loss: 52.09930419921875, entropy loss: 0.5078760385513306\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.62540054321289, CLIP loss: 0.5264773964881897, value function loss: 60.208065032958984, entropy loss: 0.5110467672348022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.97437858581543, CLIP loss: 1.8271782398223877, value function loss: 60.30443572998047, entropy loss: 0.501842200756073\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.374748229980469, CLIP loss: -0.39568787813186646, value function loss: 31.551197052001953, entropy loss: 0.5162529945373535\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.539871215820312, CLIP loss: -0.729843020439148, value function loss: 28.549800872802734, entropy loss: 0.5185727477073669\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.606609344482422, CLIP loss: -0.6577704548835754, value function loss: 32.53902053833008, entropy loss: 0.5130565762519836\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.480276107788086, CLIP loss: -0.5340601205825806, value function loss: 28.03879737854004, entropy loss: 0.5062121748924255\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 58.39544677734375, CLIP loss: 3.546060562133789, value function loss: 109.70923614501953, entropy loss: 0.5234643220901489\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 53.673301696777344, CLIP loss: 3.0990750789642334, value function loss: 101.15888977050781, entropy loss: 0.5218501687049866\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 59.06561279296875, CLIP loss: 3.387537956237793, value function loss: 111.36639404296875, entropy loss: 0.512370765209198\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 53.78053283691406, CLIP loss: 2.9999208450317383, value function loss: 101.57121276855469, entropy loss: 0.4994105398654938\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.12200355529785, CLIP loss: -2.7847023010253906, value function loss: 61.82399368286133, entropy loss: 0.5291336178779602\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.31930160522461, CLIP loss: -1.7245099544525146, value function loss: 64.09794616699219, entropy loss: 0.5162019729614258\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.644298553466797, CLIP loss: -2.4675350189208984, value function loss: 64.23406219482422, entropy loss: 0.5196580290794373\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.544458389282227, CLIP loss: -2.032386064529419, value function loss: 51.16405487060547, entropy loss: 0.5182454586029053\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.310319900512695, CLIP loss: -1.6308422088623047, value function loss: 25.89273452758789, entropy loss: 0.5205427408218384\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.502676963806152, CLIP loss: -1.7237359285354614, value function loss: 20.463064193725586, entropy loss: 0.51189124584198\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.947136878967285, CLIP loss: -1.6020936965942383, value function loss: 21.108612060546875, entropy loss: 0.5075691938400269\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.860671997070312, CLIP loss: -1.7212750911712646, value function loss: 25.173948287963867, entropy loss: 0.5027070045471191\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.80459976196289, CLIP loss: -1.399947166442871, value function loss: 30.41938591003418, entropy loss: 0.5146211981773376\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.561844825744629, CLIP loss: -0.768906831741333, value function loss: 28.67162322998047, entropy loss: 0.5060477256774902\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.19833755493164, CLIP loss: -0.9328155517578125, value function loss: 30.272480010986328, entropy loss: 0.5086861848831177\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.649486541748047, CLIP loss: -1.0812958478927612, value function loss: 29.47134780883789, entropy loss: 0.48911410570144653\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.029870986938477, CLIP loss: 0.16830682754516602, value function loss: 57.73395538330078, entropy loss: 0.5412777066230774\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.906221389770508, CLIP loss: -0.39474058151245117, value function loss: 60.61260223388672, entropy loss: 0.5337921380996704\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.279159545898438, CLIP loss: -0.05733969807624817, value function loss: 60.6837043762207, entropy loss: 0.535274088382721\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.312454223632812, CLIP loss: -0.2786884605884552, value function loss: 53.193092346191406, entropy loss: 0.5404465198516846\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.090818405151367, CLIP loss: -2.0047011375427246, value function loss: 34.20167922973633, entropy loss: 0.531975269317627\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.890685081481934, CLIP loss: -1.6298774480819702, value function loss: 27.05187225341797, entropy loss: 0.5374170541763306\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.248250961303711, CLIP loss: -1.9104557037353516, value function loss: 30.327919006347656, entropy loss: 0.5252643823623657\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.069269180297852, CLIP loss: -1.8112027645111084, value function loss: 25.77155113220215, entropy loss: 0.530336856842041\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.11681365966797, CLIP loss: -0.3632872700691223, value function loss: 34.970787048339844, entropy loss: 0.5292894840240479\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.164939880371094, CLIP loss: 0.012481626123189926, value function loss: 42.31544494628906, entropy loss: 0.5265127420425415\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.220115661621094, CLIP loss: -0.2613285779953003, value function loss: 34.97325134277344, entropy loss: 0.5182002782821655\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.0433292388916, CLIP loss: -0.2967783212661743, value function loss: 38.690582275390625, entropy loss: 0.5184256434440613\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.350431442260742, CLIP loss: -0.15740755200386047, value function loss: 35.02621078491211, entropy loss: 0.5265452265739441\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.798648834228516, CLIP loss: -0.04464305564761162, value function loss: 35.697052001953125, entropy loss: 0.523432195186615\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.109596252441406, CLIP loss: 0.007394164800643921, value function loss: 36.21500015258789, entropy loss: 0.5297941565513611\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.1310977935791, CLIP loss: -0.37208330631256104, value function loss: 33.016788482666016, entropy loss: 0.5212544202804565\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.407920837402344, CLIP loss: -1.3862330913543701, value function loss: 53.59891891479492, entropy loss: 0.5306527018547058\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.145179748535156, CLIP loss: -0.39777493476867676, value function loss: 57.09650421142578, entropy loss: 0.5296181440353394\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.16619873046875, CLIP loss: -0.9613193273544312, value function loss: 58.26554870605469, entropy loss: 0.5256071090698242\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.742652893066406, CLIP loss: -0.7281166315078735, value function loss: 48.95214080810547, entropy loss: 0.5301443338394165\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.156408309936523, CLIP loss: -1.3089497089385986, value function loss: 38.94132614135742, entropy loss: 0.5305197238922119\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.351045608520508, CLIP loss: -1.7803564071655273, value function loss: 26.273584365844727, entropy loss: 0.5390640497207642\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.994215965270996, CLIP loss: -1.5126267671585083, value function loss: 33.02424240112305, entropy loss: 0.527887225151062\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.028854370117188, CLIP loss: -1.3844515085220337, value function loss: 30.837209701538086, entropy loss: 0.5298349261283875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.271333694458008, CLIP loss: 1.9538993835449219, value function loss: 42.645408630371094, entropy loss: 0.5269914865493774\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.85462188720703, CLIP loss: 1.4169095754623413, value function loss: 44.88593673706055, entropy loss: 0.5257580876350403\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.834487915039062, CLIP loss: 1.752379298210144, value function loss: 46.17464828491211, entropy loss: 0.5214694142341614\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.080320358276367, CLIP loss: 1.7696585655212402, value function loss: 40.63182830810547, entropy loss: 0.5253265500068665\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.931297302246094, CLIP loss: -2.3502345085144043, value function loss: 48.573875427246094, entropy loss: 0.5404579639434814\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.94302749633789, CLIP loss: -2.594820022583008, value function loss: 49.08641052246094, entropy loss: 0.5357396006584167\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.47418975830078, CLIP loss: -2.436537027359009, value function loss: 43.83221435546875, entropy loss: 0.5381364226341248\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.938617706298828, CLIP loss: -2.0430917739868164, value function loss: 47.97405242919922, entropy loss: 0.531724750995636\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.04573631286621, CLIP loss: -1.0474317073822021, value function loss: 38.196712493896484, entropy loss: 0.5187891721725464\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.129413604736328, CLIP loss: -0.6357706785202026, value function loss: 43.54051971435547, entropy loss: 0.5075995922088623\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.08793830871582, CLIP loss: -0.7263779640197754, value function loss: 41.638671875, entropy loss: 0.501936137676239\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.5965576171875, CLIP loss: -0.9639407992362976, value function loss: 37.1309700012207, entropy loss: 0.4986688494682312\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 36.43667221069336, CLIP loss: 2.4153101444244385, value function loss: 68.05309295654297, entropy loss: 0.5184739828109741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.12251853942871, CLIP loss: 1.4165416955947876, value function loss: 49.42219161987305, entropy loss: 0.5118712186813354\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 34.11909866333008, CLIP loss: 2.397099018096924, value function loss: 63.45433044433594, entropy loss: 0.5164936780929565\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.628032684326172, CLIP loss: 1.4953150749206543, value function loss: 50.275718688964844, entropy loss: 0.514159619808197\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.233860969543457, CLIP loss: -1.848688006401062, value function loss: 20.175830841064453, entropy loss: 0.5365860462188721\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.77744197845459, CLIP loss: -1.863203525543213, value function loss: 21.29207992553711, entropy loss: 0.5394555330276489\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.288591384887695, CLIP loss: -1.8035409450531006, value function loss: 20.194950103759766, entropy loss: 0.5342769026756287\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.644325256347656, CLIP loss: -1.777340292930603, value function loss: 18.85406494140625, entropy loss: 0.5366961359977722\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.903357028961182, CLIP loss: -1.335997462272644, value function loss: 16.48935317993164, entropy loss: 0.5321969389915466\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.235174179077148, CLIP loss: -1.2277255058288574, value function loss: 16.936235427856445, entropy loss: 0.521791398525238\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.996285915374756, CLIP loss: -1.3374100923538208, value function loss: 14.677786827087402, entropy loss: 0.5197571516036987\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.169135093688965, CLIP loss: -1.099942684173584, value function loss: 16.54861831665039, entropy loss: 0.5231243968009949\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.40627098083496, CLIP loss: -0.42821258306503296, value function loss: 39.679656982421875, entropy loss: 0.5343932509422302\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.406400680541992, CLIP loss: -0.027870234102010727, value function loss: 52.87896728515625, entropy loss: 0.5212506651878357\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.249629974365234, CLIP loss: -0.28194043040275574, value function loss: 43.07353210449219, entropy loss: 0.5194892883300781\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.653308868408203, CLIP loss: -0.06633590161800385, value function loss: 47.44972229003906, entropy loss: 0.5216357707977295\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.290184020996094, CLIP loss: -0.24381105601787567, value function loss: 17.07843589782715, entropy loss: 0.522345244884491\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.37803840637207, CLIP loss: -0.42473548650741577, value function loss: 17.615846633911133, entropy loss: 0.5150260925292969\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.239378929138184, CLIP loss: -0.48241984844207764, value function loss: 17.454105377197266, entropy loss: 0.5253758430480957\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.371181011199951, CLIP loss: -0.1589578539133072, value function loss: 15.070588111877441, entropy loss: 0.5155149102210999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.63223648071289, CLIP loss: -0.7472041845321655, value function loss: 18.769338607788086, entropy loss: 0.5228722095489502\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.269366264343262, CLIP loss: -1.0906143188476562, value function loss: 18.730491638183594, entropy loss: 0.5265337228775024\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.158488273620605, CLIP loss: -0.9566760063171387, value function loss: 18.240907669067383, entropy loss: 0.5289790034294128\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.25405216217041, CLIP loss: -1.0291788578033447, value function loss: 16.576969146728516, entropy loss: 0.5253836512565613\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.994054794311523, CLIP loss: 1.824225664138794, value function loss: 60.35016632080078, entropy loss: 0.525436282157898\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.36333656311035, CLIP loss: 1.6873071193695068, value function loss: 59.362876892089844, entropy loss: 0.5409612655639648\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.352680206298828, CLIP loss: 1.3091801404953003, value function loss: 52.097808837890625, entropy loss: 0.540349543094635\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 32.09056091308594, CLIP loss: 1.9632076025009155, value function loss: 60.26523208618164, entropy loss: 0.5263013243675232\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.437969207763672, CLIP loss: 0.04288342222571373, value function loss: 60.800819396972656, entropy loss: 0.5322638750076294\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.249324798583984, CLIP loss: 0.007720522582530975, value function loss: 62.49385452270508, entropy loss: 0.532260537147522\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 30.01171875, CLIP loss: 0.32953646779060364, value function loss: 59.375099182128906, entropy loss: 0.5367016196250916\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 32.67117691040039, CLIP loss: -0.18543654680252075, value function loss: 65.72382354736328, entropy loss: 0.5297700762748718\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.89657211303711, CLIP loss: 0.45054998993873596, value function loss: 40.902801513671875, entropy loss: 0.537804126739502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 25.260541915893555, CLIP loss: 0.8755590319633484, value function loss: 48.78072738647461, entropy loss: 0.5379893779754639\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.70567512512207, CLIP loss: 0.22237159311771393, value function loss: 36.97721481323242, entropy loss: 0.5303789973258972\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.319747924804688, CLIP loss: 1.2545777559280396, value function loss: 50.140594482421875, entropy loss: 0.5126360058784485\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 39.015480041503906, CLIP loss: 1.7176216840744019, value function loss: 74.60643005371094, entropy loss: 0.5357197523117065\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 28.932193756103516, CLIP loss: 1.8005945682525635, value function loss: 54.273963928222656, entropy loss: 0.5382097959518433\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.345561981201172, CLIP loss: 1.7586575746536255, value function loss: 59.18441390991211, entropy loss: 0.5301733016967773\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 36.92693328857422, CLIP loss: 1.8358529806137085, value function loss: 70.19244384765625, entropy loss: 0.5141406059265137\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 51.705108642578125, CLIP loss: 2.2086098194122314, value function loss: 99.00396728515625, entropy loss: 0.5484145879745483\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 46.26197814941406, CLIP loss: 1.9306350946426392, value function loss: 88.67362213134766, entropy loss: 0.5466336011886597\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 52.540287017822266, CLIP loss: 1.9284950494766235, value function loss: 101.23426055908203, entropy loss: 0.5338337421417236\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 44.2596435546875, CLIP loss: 2.018622875213623, value function loss: 84.492431640625, entropy loss: 0.5197329521179199\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.070539474487305, CLIP loss: -0.09112850576639175, value function loss: 46.334449768066406, entropy loss: 0.5556540489196777\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.1368465423584, CLIP loss: 0.0047805458307266235, value function loss: 52.27503967285156, entropy loss: 0.5453020334243774\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.600788116455078, CLIP loss: -0.11331890523433685, value function loss: 49.439273834228516, entropy loss: 0.5529671907424927\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.311723709106445, CLIP loss: -0.047702014446258545, value function loss: 46.729774475097656, entropy loss: 0.5460851192474365\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.832507133483887, CLIP loss: -0.09866337478160858, value function loss: 27.87322235107422, entropy loss: 0.5441129207611084\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.6765775680542, CLIP loss: -0.03542312979698181, value function loss: 29.434858322143555, entropy loss: 0.5428053736686707\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.919160842895508, CLIP loss: 0.004575550556182861, value function loss: 29.839921951293945, entropy loss: 0.5375683903694153\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.360230445861816, CLIP loss: -0.1500016301870346, value function loss: 25.03101348876953, entropy loss: 0.5275163650512695\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.52801513671875, CLIP loss: -1.2323659658432007, value function loss: 29.53193473815918, entropy loss: 0.5586977005004883\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.96409034729004, CLIP loss: -0.9871873259544373, value function loss: 37.913490295410156, entropy loss: 0.5468412041664124\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.278274536132812, CLIP loss: -0.9737613201141357, value function loss: 30.515138626098633, entropy loss: 0.5532920360565186\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.691642761230469, CLIP loss: -1.181732177734375, value function loss: 27.757549285888672, entropy loss: 0.5400013327598572\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.509408950805664, CLIP loss: -0.43509218096733093, value function loss: 37.89990997314453, entropy loss: 0.545376181602478\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.669814109802246, CLIP loss: -0.8303080797195435, value function loss: 29.011089324951172, entropy loss: 0.5422400236129761\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.653889656066895, CLIP loss: -0.6893940567970276, value function loss: 32.69746398925781, entropy loss: 0.544864296913147\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.034917831420898, CLIP loss: -0.5503185391426086, value function loss: 31.181224822998047, entropy loss: 0.537555992603302\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.856304168701172, CLIP loss: 0.11246460676193237, value function loss: 55.498558044433594, entropy loss: 0.5440481901168823\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.50933265686035, CLIP loss: 0.5865820050239563, value function loss: 47.85637283325195, entropy loss: 0.5435179471969604\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.92045783996582, CLIP loss: 0.2861313223838806, value function loss: 53.27937698364258, entropy loss: 0.5362043380737305\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.370948791503906, CLIP loss: 0.32168829441070557, value function loss: 44.109092712402344, entropy loss: 0.5284906625747681\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.066079139709473, CLIP loss: -1.0082098245620728, value function loss: 30.159305572509766, entropy loss: 0.5363797545433044\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.594343185424805, CLIP loss: -0.4726215600967407, value function loss: 30.144454956054688, entropy loss: 0.5262025594711304\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.886113166809082, CLIP loss: -0.8429745435714722, value function loss: 29.468753814697266, entropy loss: 0.5289146900177002\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.27441692352295, CLIP loss: -0.683807909488678, value function loss: 27.926937103271484, entropy loss: 0.524313747882843\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.552191734313965, CLIP loss: -0.34387585520744324, value function loss: 31.802621841430664, entropy loss: 0.5243757963180542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.277305603027344, CLIP loss: -0.6962920427322388, value function loss: 25.95772361755371, entropy loss: 0.5263915061950684\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.679655075073242, CLIP loss: -0.5151672959327698, value function loss: 30.400253295898438, entropy loss: 0.5304023623466492\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.630545616149902, CLIP loss: -0.5466532707214355, value function loss: 28.364810943603516, entropy loss: 0.520591139793396\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.624415397644043, CLIP loss: -0.8905212879180908, value function loss: 23.0402889251709, entropy loss: 0.5207905769348145\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.360642433166504, CLIP loss: -0.646860659122467, value function loss: 30.025310516357422, entropy loss: 0.515131413936615\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.546602249145508, CLIP loss: -0.9233928918838501, value function loss: 22.950605392456055, entropy loss: 0.5307172536849976\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.797804832458496, CLIP loss: -0.6206408739089966, value function loss: 26.847118377685547, entropy loss: 0.5113978981971741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.961658477783203, CLIP loss: -0.6072324514389038, value function loss: 37.1484260559082, entropy loss: 0.5320799350738525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.06381893157959, CLIP loss: -1.3303947448730469, value function loss: 26.79884910583496, entropy loss: 0.5210886001586914\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.845308303833008, CLIP loss: -0.7274315357208252, value function loss: 39.15598678588867, entropy loss: 0.5252132415771484\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.33300495147705, CLIP loss: -1.1372562646865845, value function loss: 24.950929641723633, entropy loss: 0.520368218421936\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.707208156585693, CLIP loss: -0.6645680665969849, value function loss: 14.754429817199707, entropy loss: 0.5438849925994873\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.23068904876709, CLIP loss: -0.7044165730476379, value function loss: 15.880660057067871, entropy loss: 0.522445023059845\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.260701179504395, CLIP loss: -0.4324534833431244, value function loss: 17.396820068359375, entropy loss: 0.5256119966506958\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.65476131439209, CLIP loss: -0.7906940579414368, value function loss: 14.901360511779785, entropy loss: 0.5224905014038086\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.779162883758545, CLIP loss: -1.8138436079025269, value function loss: 15.196922302246094, entropy loss: 0.545464813709259\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.431176662445068, CLIP loss: -1.7125988006591797, value function loss: 16.298290252685547, entropy loss: 0.5369817614555359\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.1808552742004395, CLIP loss: -1.9169305562973022, value function loss: 14.2061767578125, entropy loss: 0.5302369594573975\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.239096164703369, CLIP loss: -1.5325785875320435, value function loss: 13.5540771484375, entropy loss: 0.5363863706588745\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.42788028717041, CLIP loss: -0.33561158180236816, value function loss: 15.537982940673828, entropy loss: 0.5499739050865173\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.882184982299805, CLIP loss: 0.04035867750644684, value function loss: 15.69466495513916, entropy loss: 0.5505934953689575\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.469477653503418, CLIP loss: -0.1188550740480423, value function loss: 15.187797546386719, entropy loss: 0.5566059947013855\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.350784778594971, CLIP loss: -0.16687922179698944, value function loss: 15.046263694763184, entropy loss: 0.5467662215232849\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.106061935424805, CLIP loss: 0.916922390460968, value function loss: 42.389251708984375, entropy loss: 0.5486129522323608\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.523168563842773, CLIP loss: 0.8755197525024414, value function loss: 39.30611801147461, entropy loss: 0.5411396026611328\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.860605239868164, CLIP loss: 0.7269647717475891, value function loss: 42.27812576293945, entropy loss: 0.5422844886779785\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.62350845336914, CLIP loss: 1.319901943206787, value function loss: 38.6180534362793, entropy loss: 0.541991651058197\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.410172462463379, CLIP loss: -1.3190414905548096, value function loss: 15.469463348388672, entropy loss: 0.5517945289611816\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.950876712799072, CLIP loss: -1.2539582252502441, value function loss: 14.420578002929688, entropy loss: 0.5453968048095703\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.9037089347839355, CLIP loss: -1.2622201442718506, value function loss: 14.342741012573242, entropy loss: 0.5441182851791382\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.372990131378174, CLIP loss: -1.3179693222045898, value function loss: 15.392939567565918, entropy loss: 0.5510494709014893\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.452402114868164, CLIP loss: -0.6391280889511108, value function loss: 16.194076538085938, entropy loss: 0.5508053302764893\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.753194332122803, CLIP loss: -0.4574018716812134, value function loss: 14.432262420654297, entropy loss: 0.553499698638916\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.761444568634033, CLIP loss: -0.6033284068107605, value function loss: 14.740514755249023, entropy loss: 0.5484709739685059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.666411876678467, CLIP loss: -0.4324021339416504, value function loss: 14.208700180053711, entropy loss: 0.553614616394043\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 44.84221649169922, CLIP loss: 1.3944995403289795, value function loss: 86.90657806396484, entropy loss: 0.5574327111244202\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 42.03689193725586, CLIP loss: 1.4427968263626099, value function loss: 81.19929504394531, entropy loss: 0.5554571747779846\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 42.41256332397461, CLIP loss: 1.4412484169006348, value function loss: 81.95376586914062, entropy loss: 0.556948184967041\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 44.59128189086914, CLIP loss: 1.1450507640838623, value function loss: 86.90357971191406, entropy loss: 0.5556578636169434\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.067215919494629, CLIP loss: -0.05010329931974411, value function loss: 28.24571418762207, entropy loss: 0.553834080696106\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.813761711120605, CLIP loss: -0.19269506633281708, value function loss: 22.024112701416016, entropy loss: 0.5600225925445557\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.787626266479492, CLIP loss: -0.28307265043258667, value function loss: 24.152406692504883, entropy loss: 0.5504508018493652\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.966769218444824, CLIP loss: -0.09414531290531158, value function loss: 26.132972717285156, entropy loss: 0.5571306347846985\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.791189193725586, CLIP loss: -1.2595690488815308, value function loss: 14.112783432006836, entropy loss: 0.5633543133735657\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.31981086730957, CLIP loss: -1.3562135696411133, value function loss: 13.363255500793457, entropy loss: 0.5603187680244446\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.9238996505737305, CLIP loss: -1.4940065145492554, value function loss: 12.847062110900879, entropy loss: 0.5624873638153076\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.171999454498291, CLIP loss: -1.134286880493164, value function loss: 14.623763084411621, entropy loss: 0.5595406293869019\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.072546005249023, CLIP loss: -0.6483316421508789, value function loss: 13.452770233154297, entropy loss: 0.5507572889328003\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.472377777099609, CLIP loss: -0.7273160219192505, value function loss: 12.410272598266602, entropy loss: 0.5442637205123901\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.2853617668151855, CLIP loss: -0.8480661511421204, value function loss: 12.27782917022705, entropy loss: 0.548650860786438\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.792609691619873, CLIP loss: -0.5091753005981445, value function loss: 12.614500999450684, entropy loss: 0.5465548038482666\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.298717498779297, CLIP loss: 0.8794662356376648, value function loss: 60.84964370727539, entropy loss: 0.5572091341018677\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.65872573852539, CLIP loss: 0.8009483218193054, value function loss: 61.726593017578125, entropy loss: 0.5519021153450012\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 31.246122360229492, CLIP loss: 0.6741749048233032, value function loss: 61.154762268066406, entropy loss: 0.5434239506721497\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 32.32698440551758, CLIP loss: 0.7269421815872192, value function loss: 63.2111701965332, entropy loss: 0.5542616844177246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.817870140075684, CLIP loss: -0.8062571287155151, value function loss: 11.25914192199707, entropy loss: 0.5443556904792786\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.377161979675293, CLIP loss: -0.9243490099906921, value function loss: 10.613869667053223, entropy loss: 0.5424070358276367\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.119307041168213, CLIP loss: -0.9254929423332214, value function loss: 12.100252151489258, entropy loss: 0.5326243042945862\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.135594844818115, CLIP loss: -0.8511530160903931, value function loss: 11.984164237976074, entropy loss: 0.5334216952323914\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.570378303527832, CLIP loss: 0.22129572927951813, value function loss: 30.709110260009766, entropy loss: 0.5472034215927124\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.629054069519043, CLIP loss: 0.25252702832221985, value function loss: 28.76401138305664, entropy loss: 0.547907829284668\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.4119873046875, CLIP loss: 0.1597849577665329, value function loss: 32.51530456542969, entropy loss: 0.5449461340904236\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.09041690826416, CLIP loss: 0.24424466490745544, value function loss: 27.70339012145996, entropy loss: 0.5522794723510742\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.745482444763184, CLIP loss: 0.6590487360954285, value function loss: 28.183795928955078, entropy loss: 0.5464826226234436\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.62279510498047, CLIP loss: 0.7538913488388062, value function loss: 31.74871063232422, entropy loss: 0.5451820492744446\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.3394718170166, CLIP loss: 0.5880330801010132, value function loss: 31.513896942138672, entropy loss: 0.5509448051452637\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.911009788513184, CLIP loss: 0.7306948900222778, value function loss: 26.371585845947266, entropy loss: 0.5478139519691467\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.915460586547852, CLIP loss: 0.2971726953983307, value function loss: 19.24763298034668, entropy loss: 0.5528104901313782\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.290452003479004, CLIP loss: -0.11851891875267029, value function loss: 20.82900619506836, entropy loss: 0.5532693862915039\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.539710998535156, CLIP loss: -0.09772340208292007, value function loss: 17.28566551208496, entropy loss: 0.5398391485214233\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.620711326599121, CLIP loss: 0.24742920696735382, value function loss: 22.757686614990234, entropy loss: 0.5561138987541199\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.382392883300781, CLIP loss: -1.5000441074371338, value function loss: 25.775859832763672, entropy loss: 0.5493264198303223\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.231196403503418, CLIP loss: -1.2837496995925903, value function loss: 27.040756225585938, entropy loss: 0.5432202219963074\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.671806335449219, CLIP loss: -1.2821247577667236, value function loss: 25.918840408325195, entropy loss: 0.5489004254341125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.011788368225098, CLIP loss: -1.5439733266830444, value function loss: 27.12253761291504, entropy loss: 0.5507451295852661\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.093530654907227, CLIP loss: 0.4224964380264282, value function loss: 43.35296630859375, entropy loss: 0.5448886156082153\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.701152801513672, CLIP loss: 0.7502726316452026, value function loss: 43.912723541259766, entropy loss: 0.5482405424118042\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.327117919921875, CLIP loss: 0.9264026880264282, value function loss: 44.81231689453125, entropy loss: 0.5443378686904907\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.138879776000977, CLIP loss: 0.317480206489563, value function loss: 39.65387725830078, entropy loss: 0.5539478659629822\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.716242790222168, CLIP loss: -0.1049576848745346, value function loss: 31.653472900390625, entropy loss: 0.5536430478096008\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.059885025024414, CLIP loss: -0.03724163398146629, value function loss: 34.205162048339844, entropy loss: 0.5455628037452698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.220815658569336, CLIP loss: 0.24953041970729828, value function loss: 35.953548431396484, entropy loss: 0.5488492250442505\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.943120956420898, CLIP loss: -0.35934293270111084, value function loss: 28.615787506103516, entropy loss: 0.5430207252502441\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.85480785369873, CLIP loss: -0.9929516911506653, value function loss: 31.706912994384766, entropy loss: 0.5697135329246521\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.25071907043457, CLIP loss: -1.4329607486724854, value function loss: 25.378429412841797, entropy loss: 0.5535598397254944\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.684408187866211, CLIP loss: -0.800324559211731, value function loss: 30.98063087463379, entropy loss: 0.5582941770553589\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.737674713134766, CLIP loss: -1.4713935852050781, value function loss: 24.429420471191406, entropy loss: 0.5641583800315857\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.224576950073242, CLIP loss: -0.7996684312820435, value function loss: 40.05984115600586, entropy loss: 0.5674180388450623\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.91297721862793, CLIP loss: -0.9299275279045105, value function loss: 35.696983337402344, entropy loss: 0.5586534142494202\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.460866928100586, CLIP loss: -1.252488374710083, value function loss: 27.438020706176758, entropy loss: 0.565509557723999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.31913948059082, CLIP loss: -0.4358433783054352, value function loss: 47.52118682861328, entropy loss: 0.5610798597335815\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.90519142150879, CLIP loss: -0.16921864449977875, value function loss: 40.159873962402344, entropy loss: 0.5527302026748657\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.032224655151367, CLIP loss: -0.6041851043701172, value function loss: 25.28380584716797, entropy loss: 0.5493179559707642\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.901384353637695, CLIP loss: -0.2231011688709259, value function loss: 38.260009765625, entropy loss: 0.5520301461219788\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.528752326965332, CLIP loss: -0.6727124452590942, value function loss: 28.413856506347656, entropy loss: 0.5463757514953613\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.986705780029297, CLIP loss: -0.09795084595680237, value function loss: 30.18031883239746, entropy loss: 0.5502355098724365\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.198840141296387, CLIP loss: -0.1547149121761322, value function loss: 20.71779441833496, entropy loss: 0.5342568159103394\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.501041412353516, CLIP loss: 0.07327984273433685, value function loss: 24.866046905517578, entropy loss: 0.5261609554290771\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.480201721191406, CLIP loss: -0.24990569055080414, value function loss: 27.471019744873047, entropy loss: 0.540288507938385\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.870880126953125, CLIP loss: -0.061023250222206116, value function loss: 23.87474822998047, entropy loss: 0.547038733959198\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.656373023986816, CLIP loss: -0.022840773686766624, value function loss: 27.369325637817383, entropy loss: 0.5449644327163696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.31335163116455, CLIP loss: 0.11068557947874069, value function loss: 26.416194915771484, entropy loss: 0.5431405901908875\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.677388191223145, CLIP loss: -0.26840612292289734, value function loss: 25.90256690979004, entropy loss: 0.5489805340766907\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.487421035766602, CLIP loss: 0.33376166224479675, value function loss: 22.3183536529541, entropy loss: 0.5516571998596191\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.2927141189575195, CLIP loss: -0.15124888718128204, value function loss: 12.898979187011719, entropy loss: 0.5526514053344727\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.885219573974609, CLIP loss: 0.10177735984325409, value function loss: 15.577875137329102, entropy loss: 0.549545407295227\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.210044860839844, CLIP loss: -0.05346542224287987, value function loss: 20.537755966186523, entropy loss: 0.536730945110321\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.604150772094727, CLIP loss: 0.3309369683265686, value function loss: 42.55766296386719, entropy loss: 0.5616819262504578\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.534568786621094, CLIP loss: -0.2444177269935608, value function loss: 33.56916809082031, entropy loss: 0.5598797798156738\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.124706268310547, CLIP loss: 0.061043962836265564, value function loss: 36.13836669921875, entropy loss: 0.5522645711898804\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.269702911376953, CLIP loss: -0.023191235959529877, value function loss: 42.596649169921875, entropy loss: 0.543100118637085\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.642512321472168, CLIP loss: -0.7591708898544312, value function loss: 24.814607620239258, entropy loss: 0.5620667934417725\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.641976356506348, CLIP loss: -0.7282323241233826, value function loss: 20.751813888549805, entropy loss: 0.5698434114456177\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.362618446350098, CLIP loss: -0.45352694392204285, value function loss: 29.643552780151367, entropy loss: 0.5631186962127686\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.590757846832275, CLIP loss: -1.0076271295547485, value function loss: 17.20808982849121, entropy loss: 0.5659895539283752\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.479802131652832, CLIP loss: -0.48074275255203247, value function loss: 9.932066917419434, entropy loss: 0.5488390922546387\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.370765686035156, CLIP loss: -0.07604049891233444, value function loss: 8.904592514038086, entropy loss: 0.5490513443946838\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.537956714630127, CLIP loss: -0.2235959768295288, value function loss: 9.534082412719727, entropy loss: 0.5488610863685608\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.26878547668457, CLIP loss: -0.40862393379211426, value function loss: 9.3658447265625, entropy loss: 0.5513032078742981\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.144949436187744, CLIP loss: -0.1051628589630127, value function loss: 8.511284828186035, entropy loss: 0.5529683232307434\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.50425386428833, CLIP loss: -0.20417754352092743, value function loss: 9.427958488464355, entropy loss: 0.5547809600830078\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.856386661529541, CLIP loss: -0.2658255100250244, value function loss: 10.255416870117188, entropy loss: 0.5496652722358704\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.8712897300720215, CLIP loss: -0.061092399060726166, value function loss: 9.87588119506836, entropy loss: 0.5558321475982666\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.11980152130127, CLIP loss: -0.04823397099971771, value function loss: 24.34722900390625, entropy loss: 0.5578531622886658\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.97013521194458, CLIP loss: -0.5174551010131836, value function loss: 14.986328125, entropy loss: 0.557368814945221\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.206193923950195, CLIP loss: -0.3613625764846802, value function loss: 19.146339416503906, entropy loss: 0.5613664388656616\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.694316864013672, CLIP loss: -0.18861374258995056, value function loss: 19.776988983154297, entropy loss: 0.556367814540863\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.471806526184082, CLIP loss: -0.18126916885375977, value function loss: 23.317161560058594, entropy loss: 0.5505616664886475\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.53294563293457, CLIP loss: 0.33869534730911255, value function loss: 30.399524688720703, entropy loss: 0.55119788646698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.343701362609863, CLIP loss: -0.13697797060012817, value function loss: 18.972492218017578, entropy loss: 0.5566139817237854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.360620498657227, CLIP loss: 0.3136711120605469, value function loss: 34.10457992553711, entropy loss: 0.5340113043785095\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.620525360107422, CLIP loss: 1.218883752822876, value function loss: 42.81440353393555, entropy loss: 0.556086540222168\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.68336296081543, CLIP loss: 0.9855142831802368, value function loss: 41.40679931640625, entropy loss: 0.5550102591514587\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 20.596229553222656, CLIP loss: 0.9623292684555054, value function loss: 39.27876663208008, entropy loss: 0.5483922958374023\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 23.86376190185547, CLIP loss: 1.3207365274429321, value function loss: 45.09698486328125, entropy loss: 0.546698808670044\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.445087909698486, CLIP loss: -1.4883179664611816, value function loss: 11.87788200378418, entropy loss: 0.5534941554069519\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.8661303520202637, CLIP loss: -1.5023702383041382, value function loss: 10.748311996459961, entropy loss: 0.5655213594436646\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.247610330581665, CLIP loss: -1.6430771350860596, value function loss: 9.792596817016602, entropy loss: 0.5610844492912292\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.4602818489074707, CLIP loss: -1.4680005311965942, value function loss: 9.867835998535156, entropy loss: 0.5635856986045837\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.6117701530456543, CLIP loss: -0.3393665552139282, value function loss: 7.913549423217773, entropy loss: 0.5638067722320557\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.9389488697052, CLIP loss: -0.675258994102478, value function loss: 9.23955249786377, entropy loss: 0.5568189024925232\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.100286483764648, CLIP loss: -0.5799268484115601, value function loss: 9.371623992919922, entropy loss: 0.5598667860031128\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.6169610023498535, CLIP loss: -0.4965338706970215, value function loss: 8.238205909729004, entropy loss: 0.5608031153678894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.213731288909912, CLIP loss: -0.8245779871940613, value function loss: 8.087639808654785, entropy loss: 0.5510681867599487\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.379998207092285, CLIP loss: -0.8284464478492737, value function loss: 8.428186416625977, entropy loss: 0.56485915184021\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.3677775859832764, CLIP loss: -0.8444634079933167, value function loss: 8.435688018798828, entropy loss: 0.5602984428405762\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.7888360023498535, CLIP loss: -0.7644861340522766, value function loss: 7.117700576782227, entropy loss: 0.5528097748756409\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.300012111663818, CLIP loss: -0.15707087516784668, value function loss: 8.925188064575195, entropy loss: 0.551080584526062\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.892693519592285, CLIP loss: -0.2425655871629715, value function loss: 8.28172492980957, entropy loss: 0.5603293180465698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.9810423851013184, CLIP loss: -0.2798365354537964, value function loss: 8.532718658447266, entropy loss: 0.548024594783783\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.8198506832122803, CLIP loss: -0.1522100418806076, value function loss: 7.955289840698242, entropy loss: 0.558422327041626\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.63765811920166, CLIP loss: -0.2991412580013275, value function loss: 17.884700775146484, entropy loss: 0.5551563501358032\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.328291893005371, CLIP loss: -0.24917323887348175, value function loss: 19.16608428955078, entropy loss: 0.5576894879341125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.656780242919922, CLIP loss: -0.26077619194984436, value function loss: 17.84611701965332, entropy loss: 0.5501976013183594\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.87296199798584, CLIP loss: -0.33029311895370483, value function loss: 16.417606353759766, entropy loss: 0.5548040866851807\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.2407357692718506, CLIP loss: -0.6850762367248535, value function loss: 7.863039970397949, entropy loss: 0.5708029866218567\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.8079540729522705, CLIP loss: -0.8919379711151123, value function loss: 7.411114692687988, entropy loss: 0.5665220618247986\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.16634202003479, CLIP loss: -0.8129754662513733, value function loss: 7.969876289367676, entropy loss: 0.5620602965354919\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.844550371170044, CLIP loss: -0.8191536664962769, value function loss: 7.338895797729492, entropy loss: 0.5743687152862549\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.284045219421387, CLIP loss: 0.07780422270298004, value function loss: 8.423871040344238, entropy loss: 0.5694165229797363\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.6609444618225098, CLIP loss: -0.5278008580207825, value function loss: 8.38892936706543, entropy loss: 0.5719320774078369\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.640913963317871, CLIP loss: -0.39509716629981995, value function loss: 8.083545684814453, entropy loss: 0.5761592388153076\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.073349475860596, CLIP loss: -0.0692109689116478, value function loss: 8.296478271484375, entropy loss: 0.5678592324256897\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.522172927856445, CLIP loss: 0.5742383003234863, value function loss: 19.907487869262695, entropy loss: 0.5810229182243347\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.39866828918457, CLIP loss: 0.9007607698440552, value function loss: 29.0074520111084, entropy loss: 0.5817891955375671\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.989141464233398, CLIP loss: 0.5011168718338013, value function loss: 20.987693786621094, entropy loss: 0.5822048187255859\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.175349235534668, CLIP loss: 0.9318023920059204, value function loss: 24.498628616333008, entropy loss: 0.576778769493103\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.6779160499572754, CLIP loss: -1.0262621641159058, value function loss: 7.4198737144470215, entropy loss: 0.5758734941482544\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.667807102203369, CLIP loss: -0.7896905541419983, value function loss: 6.92649507522583, entropy loss: 0.5749961137771606\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.751631736755371, CLIP loss: -0.8884297609329224, value function loss: 7.291768550872803, entropy loss: 0.5822657942771912\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.7631897926330566, CLIP loss: -0.9565547704696655, value function loss: 7.451034069061279, entropy loss: 0.5772512555122375\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.233863592147827, CLIP loss: -0.21854889392852783, value function loss: 6.916080951690674, entropy loss: 0.5627899169921875\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.5790605545043945, CLIP loss: -0.32599228620529175, value function loss: 5.82146692276001, entropy loss: 0.5680633187294006\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.9508543014526367, CLIP loss: -0.19104866683483124, value function loss: 6.295102119445801, entropy loss: 0.5648186802864075\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.5845606327056885, CLIP loss: -0.37148481607437134, value function loss: 5.9233198165893555, entropy loss: 0.5614575147628784\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.822158813476562, CLIP loss: 0.9203947186470032, value function loss: 41.814918518066406, entropy loss: 0.5695358514785767\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 29.84624671936035, CLIP loss: 1.4310579299926758, value function loss: 56.84166717529297, entropy loss: 0.5643354058265686\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.403884887695312, CLIP loss: 1.1954317092895508, value function loss: 46.428287506103516, entropy loss: 0.5692014098167419\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 26.065839767456055, CLIP loss: 1.1429589986801147, value function loss: 49.85689926147461, entropy loss: 0.5570303201675415\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.3597888946533203, CLIP loss: -0.5233869552612305, value function loss: 5.777645111083984, entropy loss: 0.564660370349884\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.274803876876831, CLIP loss: -0.43352144956588745, value function loss: 5.427951812744141, entropy loss: 0.5650580525398254\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.2514150142669678, CLIP loss: -0.47704485058784485, value function loss: 5.468164443969727, entropy loss: 0.5622329115867615\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.390191078186035, CLIP loss: -0.48664936423301697, value function loss: 5.764853477478027, entropy loss: 0.5586373805999756\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7122420072555542, CLIP loss: -1.3041006326675415, value function loss: 6.044252395629883, entropy loss: 0.5783580541610718\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.6389927864074707, CLIP loss: -1.366914987564087, value function loss: 6.0233354568481445, entropy loss: 0.575991690158844\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4610934257507324, CLIP loss: -1.3986999988555908, value function loss: 5.731260776519775, entropy loss: 0.5836981534957886\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.687447428703308, CLIP loss: -1.259513258934021, value function loss: 5.905440330505371, entropy loss: 0.5759488344192505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.0045926570892334, CLIP loss: -0.6226097345352173, value function loss: 5.266026496887207, entropy loss: 0.581096351146698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.5650800466537476, CLIP loss: -0.6308144330978394, value function loss: 4.403470516204834, entropy loss: 0.5840815901756287\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7571097612380981, CLIP loss: -0.6069133877754211, value function loss: 4.7397260665893555, entropy loss: 0.5839906930923462\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7577015161514282, CLIP loss: -0.658875048160553, value function loss: 4.844621181488037, entropy loss: 0.5734144449234009\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.518303871154785, CLIP loss: -0.3098110556602478, value function loss: 5.667874813079834, entropy loss: 0.5822440385818481\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.975592017173767, CLIP loss: -0.33817872405052185, value function loss: 4.638945579528809, entropy loss: 0.5702040195465088\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.1198272705078125, CLIP loss: -0.2571721374988556, value function loss: 4.765300273895264, entropy loss: 0.5650872588157654\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.125875234603882, CLIP loss: -0.3419063091278076, value function loss: 4.946701526641846, entropy loss: 0.556916356086731\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.89842414855957, CLIP loss: 0.6560148000717163, value function loss: 36.49641799926758, entropy loss: 0.5800724625587463\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.354284286499023, CLIP loss: 0.4286000728607178, value function loss: 33.86281967163086, entropy loss: 0.572601854801178\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 19.674875259399414, CLIP loss: 0.6803922057151794, value function loss: 38.00035095214844, entropy loss: 0.5691218376159668\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.972868919372559, CLIP loss: 0.428499698638916, value function loss: 29.100177764892578, entropy loss: 0.5719348192214966\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.8192849159240723, CLIP loss: -0.7926302552223206, value function loss: 5.235109329223633, entropy loss: 0.5639496445655823\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.8034520149230957, CLIP loss: -0.45177778601646423, value function loss: 4.521891117095947, entropy loss: 0.5715778470039368\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.749342441558838, CLIP loss: -0.7109668850898743, value function loss: 4.932034015655518, entropy loss: 0.5707786083221436\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.874082088470459, CLIP loss: -0.5629622340202332, value function loss: 4.885531902313232, entropy loss: 0.5721573829650879\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.0065531730651855, CLIP loss: -0.5048815011978149, value function loss: 5.03443717956543, entropy loss: 0.578411877155304\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7841839790344238, CLIP loss: -0.5586711168289185, value function loss: 4.6970062255859375, entropy loss: 0.5648068785667419\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.0249459743499756, CLIP loss: -0.564483642578125, value function loss: 5.19021463394165, entropy loss: 0.5677637457847595\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.86944580078125, CLIP loss: -0.5056809186935425, value function loss: 4.761723518371582, entropy loss: 0.5735071301460266\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.00010347366333, CLIP loss: -0.11160314828157425, value function loss: 8.234872817993164, entropy loss: 0.5729782581329346\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.070898056030273, CLIP loss: -0.15114372968673706, value function loss: 12.455221176147461, entropy loss: 0.5568917989730835\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.103246688842773, CLIP loss: -0.1560075879096985, value function loss: 12.52977466583252, entropy loss: 0.5633107423782349\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.182418346405029, CLIP loss: -0.10041546821594238, value function loss: 8.576984405517578, entropy loss: 0.5658805966377258\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.761326789855957, CLIP loss: 0.7477095723152161, value function loss: 20.03860855102539, entropy loss: 0.5686548948287964\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.121917724609375, CLIP loss: 0.7670204639434814, value function loss: 26.720993041992188, entropy loss: 0.5598732233047485\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.504758834838867, CLIP loss: 0.9131288528442383, value function loss: 23.19450569152832, entropy loss: 0.5622986555099487\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.38830280303955, CLIP loss: 0.6700210571289062, value function loss: 23.447772979736328, entropy loss: 0.5605062246322632\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.617372512817383, CLIP loss: 1.177077054977417, value function loss: 40.89194107055664, entropy loss: 0.5673725008964539\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 21.72323226928711, CLIP loss: 1.4054428339004517, value function loss: 40.64685821533203, entropy loss: 0.5639904737472534\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 24.68915367126465, CLIP loss: 1.4304214715957642, value function loss: 46.52872848510742, entropy loss: 0.5632009506225586\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.800561904907227, CLIP loss: 1.4890782833099365, value function loss: 34.63400650024414, entropy loss: 0.5520626306533813\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 44.091896057128906, CLIP loss: 3.080763816833496, value function loss: 82.03347778320312, entropy loss: 0.5607208013534546\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 27.685144424438477, CLIP loss: 2.125833749771118, value function loss: 51.12970733642578, entropy loss: 0.5542131066322327\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 33.8671875, CLIP loss: 2.500680685043335, value function loss: 62.74401092529297, entropy loss: 0.5498430728912354\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 35.75047302246094, CLIP loss: 2.4520723819732666, value function loss: 66.60796356201172, entropy loss: 0.5579315423965454\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.6254663467407227, CLIP loss: -1.3533762693405151, value function loss: 5.969228267669678, entropy loss: 0.577155590057373\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.6899102926254272, CLIP loss: -1.418385624885559, value function loss: 6.227934837341309, entropy loss: 0.5671445727348328\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.340390682220459, CLIP loss: -1.4897536039352417, value function loss: 5.67135763168335, entropy loss: 0.553449273109436\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9999949336051941, CLIP loss: -1.2849024534225464, value function loss: 4.580790996551514, entropy loss: 0.5498118996620178\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.417767524719238, CLIP loss: -0.7125276327133179, value function loss: 20.272254943847656, entropy loss: 0.5832386016845703\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.60078239440918, CLIP loss: -0.33931633830070496, value function loss: 31.89171600341797, entropy loss: 0.5758974552154541\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.587525367736816, CLIP loss: -0.7176852226257324, value function loss: 20.62199592590332, entropy loss: 0.5787720680236816\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.638721466064453, CLIP loss: -0.33439135551452637, value function loss: 31.957759857177734, entropy loss: 0.5767074823379517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9210206866264343, CLIP loss: -1.0293656587600708, value function loss: 3.9121837615966797, entropy loss: 0.5705526471138\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.0864850282669067, CLIP loss: -0.9783027172088623, value function loss: 4.1410417556762695, entropy loss: 0.5733184814453125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7521970868110657, CLIP loss: -1.03639817237854, value function loss: 3.5885915756225586, entropy loss: 0.5700546503067017\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9090870022773743, CLIP loss: -0.9570214152336121, value function loss: 3.7436187267303467, entropy loss: 0.5700938105583191\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4798142910003662, CLIP loss: -0.4636365473270416, value function loss: 3.8980820178985596, entropy loss: 0.5590185523033142\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7158414125442505, CLIP loss: -0.4795114994049072, value function loss: 4.401944160461426, entropy loss: 0.5619198083877563\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.6372127532958984, CLIP loss: -0.4419446587562561, value function loss: 4.169480323791504, entropy loss: 0.5582801103591919\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.3409894704818726, CLIP loss: -0.5159728527069092, value function loss: 3.72511887550354, entropy loss: 0.5597173571586609\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.570199966430664, CLIP loss: -0.7204393744468689, value function loss: 8.59286117553711, entropy loss: 0.5791153907775879\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.769027709960938, CLIP loss: -0.5414395332336426, value function loss: 20.63223648071289, entropy loss: 0.5651369094848633\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.761058330535889, CLIP loss: -0.592047929763794, value function loss: 10.71778392791748, entropy loss: 0.5785541534423828\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.34770393371582, CLIP loss: -0.666808545589447, value function loss: 18.040372848510742, entropy loss: 0.5674716234207153\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7506988048553467, CLIP loss: -0.2904385030269623, value function loss: 4.093297004699707, entropy loss: 0.5511128902435303\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.8034019470214844, CLIP loss: -0.2327626794576645, value function loss: 4.083547592163086, entropy loss: 0.5609155893325806\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.866698145866394, CLIP loss: -0.21681001782417297, value function loss: 4.178022861480713, entropy loss: 0.550327479839325\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4562982320785522, CLIP loss: -0.2814139127731323, value function loss: 3.486525058746338, entropy loss: 0.5550430417060852\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8100689649581909, CLIP loss: -0.7827892899513245, value function loss: 3.1970207691192627, entropy loss: 0.5652125477790833\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2355642318725586, CLIP loss: -0.5702922344207764, value function loss: 3.623138666152954, entropy loss: 0.5712816715240479\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9054133892059326, CLIP loss: -0.6255475878715515, value function loss: 3.0732500553131104, entropy loss: 0.5664048194885254\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.814813494682312, CLIP loss: -0.7201018929481506, value function loss: 3.081076145172119, entropy loss: 0.5622683763504028\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2710615396499634, CLIP loss: -0.19387204945087433, value function loss: 2.9411091804504395, entropy loss: 0.5620915293693542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.195855975151062, CLIP loss: -0.27964353561401367, value function loss: 2.9621572494506836, entropy loss: 0.5579162836074829\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2840596437454224, CLIP loss: -0.27165669202804565, value function loss: 3.122694492340088, entropy loss: 0.5630826950073242\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.3182064294815063, CLIP loss: -0.1755894422531128, value function loss: 2.9988319873809814, entropy loss: 0.5620150566101074\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2479578256607056, CLIP loss: -0.2940097153186798, value function loss: 3.095264434814453, entropy loss: 0.5664688348770142\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1570719480514526, CLIP loss: -0.44222918152809143, value function loss: 3.2099595069885254, entropy loss: 0.5678656101226807\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2663280963897705, CLIP loss: -0.387065052986145, value function loss: 3.318037509918213, entropy loss: 0.5625559091567993\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1180496215820312, CLIP loss: -0.3733341693878174, value function loss: 2.994119882583618, entropy loss: 0.5676141977310181\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.503754615783691, CLIP loss: -0.036459293216466904, value function loss: 15.091741561889648, entropy loss: 0.5656502842903137\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.583055019378662, CLIP loss: 0.10489877313375473, value function loss: 12.967633247375488, entropy loss: 0.5660572052001953\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.266944408416748, CLIP loss: -0.0024185292422771454, value function loss: 12.55019474029541, entropy loss: 0.5734379291534424\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.410063743591309, CLIP loss: 0.03377043455839157, value function loss: 14.763823509216309, entropy loss: 0.5618400573730469\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.3206661939620972, CLIP loss: -0.04312266409397125, value function loss: 2.7389955520629883, entropy loss: 0.5708938837051392\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.3481228351593018, CLIP loss: -0.056593455374240875, value function loss: 2.8206653594970703, entropy loss: 0.5616485476493835\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.461984634399414, CLIP loss: 0.06775493174791336, value function loss: 2.799665689468384, entropy loss: 0.5603201389312744\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2310069799423218, CLIP loss: -0.12151406705379486, value function loss: 2.7161571979522705, entropy loss: 0.5557495951652527\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 18.637622833251953, CLIP loss: 0.9030443429946899, value function loss: 35.480464935302734, entropy loss: 0.5654091835021973\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.267892837524414, CLIP loss: 1.1800271272659302, value function loss: 42.18695831298828, entropy loss: 0.561260461807251\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 17.701580047607422, CLIP loss: 1.2827898263931274, value function loss: 32.848594665527344, entropy loss: 0.5506701469421387\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 22.845518112182617, CLIP loss: 1.1665587425231934, value function loss: 43.36872100830078, entropy loss: 0.5401386618614197\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1174359321594238, CLIP loss: -0.1607028841972351, value function loss: 2.567523241043091, entropy loss: 0.5622842311859131\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.0617440938949585, CLIP loss: -0.2867523431777954, value function loss: 2.7083005905151367, entropy loss: 0.5653820037841797\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1429431438446045, CLIP loss: -0.23074236512184143, value function loss: 2.758458375930786, entropy loss: 0.5543665289878845\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.065280795097351, CLIP loss: -0.20447491109371185, value function loss: 2.5507256984710693, entropy loss: 0.5607115626335144\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7746192812919617, CLIP loss: -0.7533508539199829, value function loss: 3.06730318069458, entropy loss: 0.568142831325531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8922325968742371, CLIP loss: -0.6789537668228149, value function loss: 3.153604030609131, entropy loss: 0.5615654587745667\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8878152966499329, CLIP loss: -0.7177057862281799, value function loss: 3.2222156524658203, entropy loss: 0.5586742758750916\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7335907816886902, CLIP loss: -0.7329829931259155, value function loss: 2.944417715072632, entropy loss: 0.5635064840316772\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.3089015483856201, CLIP loss: -0.23340529203414917, value function loss: 3.096039295196533, entropy loss: 0.5712694525718689\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1974060535430908, CLIP loss: -0.3524321913719177, value function loss: 3.1111273765563965, entropy loss: 0.5725374817848206\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2756175994873047, CLIP loss: -0.2890833914279938, value function loss: 3.1407103538513184, entropy loss: 0.5654192566871643\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.084930658340454, CLIP loss: -0.2946246266365051, value function loss: 2.7704572677612305, entropy loss: 0.5673392415046692\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8522199392318726, CLIP loss: -0.20456746220588684, value function loss: 2.1249356269836426, entropy loss: 0.5680448412895203\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.838527500629425, CLIP loss: -0.2993749976158142, value function loss: 2.287156105041504, entropy loss: 0.5675542950630188\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7931002974510193, CLIP loss: -0.2827472388744354, value function loss: 2.1629831790924072, entropy loss: 0.5643994212150574\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8282277584075928, CLIP loss: -0.21379442512989044, value function loss: 2.095390558242798, entropy loss: 0.5673092603683472\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.072904348373413, CLIP loss: -0.04270925000309944, value function loss: 2.2426209449768066, entropy loss: 0.5696921348571777\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.0491689443588257, CLIP loss: -0.09146138280630112, value function loss: 2.292598009109497, entropy loss: 0.5668646693229675\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1287567615509033, CLIP loss: 0.0031884086783975363, value function loss: 2.262434959411621, entropy loss: 0.564903736114502\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9909658432006836, CLIP loss: -0.1385611742734909, value function loss: 2.2704145908355713, entropy loss: 0.5680238008499146\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9040857553482056, CLIP loss: -0.28338128328323364, value function loss: 2.3860957622528076, entropy loss: 0.5580828189849854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7895059585571289, CLIP loss: -0.390671044588089, value function loss: 2.371718645095825, entropy loss: 0.5682376027107239\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6881350874900818, CLIP loss: -0.4471140503883362, value function loss: 2.2815699577331543, entropy loss: 0.5535842180252075\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8593160510063171, CLIP loss: -0.2212129831314087, value function loss: 2.1724636554718018, entropy loss: 0.5702813267707825\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8707018494606018, CLIP loss: -0.16727475821971893, value function loss: 2.087135076522827, entropy loss: 0.5590887665748596\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2525428533554077, CLIP loss: 0.0012276042252779007, value function loss: 2.5139381885528564, entropy loss: 0.5653915405273438\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.0835250616073608, CLIP loss: -0.09607823938131332, value function loss: 2.3704209327697754, entropy loss: 0.5607176423072815\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1021944284439087, CLIP loss: -0.08775070309638977, value function loss: 2.3912353515625, entropy loss: 0.5672553181648254\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8737567663192749, CLIP loss: -0.24337902665138245, value function loss: 2.2457025051116943, entropy loss: 0.571550190448761\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9544923901557922, CLIP loss: -0.31028449535369873, value function loss: 2.5409371852874756, entropy loss: 0.5691684484481812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9614841938018799, CLIP loss: -0.28803926706314087, value function loss: 2.510383367538452, entropy loss: 0.5668214559555054\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9243134260177612, CLIP loss: -0.2549964487552643, value function loss: 2.3700644969940186, entropy loss: 0.572243332862854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.530571937561035, CLIP loss: 0.45843708515167236, value function loss: 26.155513763427734, entropy loss: 0.5621738433837891\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.175246238708496, CLIP loss: 0.42640143632888794, value function loss: 13.50907039642334, entropy loss: 0.5690367221832275\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.865866661071777, CLIP loss: 0.5565592050552368, value function loss: 22.62998390197754, entropy loss: 0.5684490203857422\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.630446434020996, CLIP loss: 0.291065514087677, value function loss: 16.690128326416016, entropy loss: 0.5682915449142456\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.836628913879395, CLIP loss: 0.3431943356990814, value function loss: 18.998245239257812, entropy loss: 0.5687544345855713\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.303810119628906, CLIP loss: 0.6339254975318909, value function loss: 21.351093292236328, entropy loss: 0.5662162899971008\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.052215576171875, CLIP loss: 0.7178807258605957, value function loss: 28.679798126220703, entropy loss: 0.5563274025917053\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.918357849121094, CLIP loss: 0.25400859117507935, value function loss: 11.339888572692871, entropy loss: 0.5595181584358215\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9259234666824341, CLIP loss: -0.3897571265697479, value function loss: 2.642857551574707, entropy loss: 0.5748135447502136\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9112367630004883, CLIP loss: -0.4234478175640106, value function loss: 2.6805505752563477, entropy loss: 0.5590664744377136\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8703597187995911, CLIP loss: -0.4099740982055664, value function loss: 2.5718417167663574, entropy loss: 0.5587061047554016\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.9170635342597961, CLIP loss: -0.40495753288269043, value function loss: 2.655334711074829, entropy loss: 0.5646283030509949\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.591254234313965, CLIP loss: 0.5804623961448669, value function loss: 22.033035278320312, entropy loss: 0.572598934173584\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.511359214782715, CLIP loss: 0.17883794009685516, value function loss: 18.676349639892578, entropy loss: 0.5653022527694702\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.852691650390625, CLIP loss: 0.34825676679611206, value function loss: 23.020235061645508, entropy loss: 0.5682751536369324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.388026237487793, CLIP loss: 0.22258919477462769, value function loss: 16.34204864501953, entropy loss: 0.5587421655654907\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5813453197479248, CLIP loss: -0.37937766313552856, value function loss: 1.9328736066818237, entropy loss: 0.5713826417922974\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41400063037872314, CLIP loss: -0.5275816917419434, value function loss: 1.8944288492202759, entropy loss: 0.5632100105285645\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3997592628002167, CLIP loss: -0.48419561982154846, value function loss: 1.77925443649292, entropy loss: 0.5672340989112854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5773085355758667, CLIP loss: -0.40214380621910095, value function loss: 1.9700920581817627, entropy loss: 0.5593629479408264\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.9167449474334717, CLIP loss: 0.050019435584545135, value function loss: 7.744704246520996, entropy loss: 0.562673807144165\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 16.563194274902344, CLIP loss: 0.8125355839729309, value function loss: 31.512527465820312, entropy loss: 0.5606207251548767\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.754972457885742, CLIP loss: 0.3083253502845764, value function loss: 18.90445327758789, entropy loss: 0.5580243468284607\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.677780151367188, CLIP loss: 0.5613998770713806, value function loss: 22.243759155273438, entropy loss: 0.5499223470687866\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5121919512748718, CLIP loss: -0.45355165004730225, value function loss: 1.9427598714828491, entropy loss: 0.5636323690414429\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.499877005815506, CLIP loss: -0.5645550489425659, value function loss: 2.1401216983795166, entropy loss: 0.5628793239593506\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5673374533653259, CLIP loss: -0.5376963019371033, value function loss: 2.2211246490478516, entropy loss: 0.5528590679168701\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.47896283864974976, CLIP loss: -0.477939635515213, value function loss: 1.9250831604003906, entropy loss: 0.5639115571975708\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5923717617988586, CLIP loss: -0.6717427372932434, value function loss: 2.5394980907440186, entropy loss: 0.5634541511535645\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4720955491065979, CLIP loss: -0.6888695359230042, value function loss: 2.333170175552368, entropy loss: 0.5619996786117554\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.34981170296669006, CLIP loss: -0.6422582268714905, value function loss: 1.995456337928772, entropy loss: 0.565822958946228\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.45744743943214417, CLIP loss: -0.7160409092903137, value function loss: 2.358053684234619, entropy loss: 0.553850531578064\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5628184676170349, CLIP loss: -0.5329169631004333, value function loss: 2.2025485038757324, entropy loss: 0.5538821816444397\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2834770083427429, CLIP loss: -0.6357302665710449, value function loss: 1.849743127822876, entropy loss: 0.5664288997650146\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40680450201034546, CLIP loss: -0.548703670501709, value function loss: 1.9222427606582642, entropy loss: 0.5613217949867249\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3026975095272064, CLIP loss: -0.6122901439666748, value function loss: 1.8411227464675903, entropy loss: 0.5573733448982239\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.580146789550781, CLIP loss: 0.5476414561271667, value function loss: 20.076255798339844, entropy loss: 0.562282919883728\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.50245189666748, CLIP loss: 0.446073055267334, value function loss: 20.123971939086914, entropy loss: 0.5606465935707092\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.633464813232422, CLIP loss: 0.445547491312027, value function loss: 18.38699722290039, entropy loss: 0.5581158399581909\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.68805980682373, CLIP loss: 0.5804286599159241, value function loss: 22.226388931274414, entropy loss: 0.5563594102859497\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6182681918144226, CLIP loss: -0.39030739665031433, value function loss: 2.0283710956573486, entropy loss: 0.5609906315803528\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6536045670509338, CLIP loss: -0.36650121212005615, value function loss: 2.051560640335083, entropy loss: 0.5674518346786499\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7208171486854553, CLIP loss: -0.3759673833847046, value function loss: 2.2047507762908936, entropy loss: 0.559083104133606\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.569210410118103, CLIP loss: -0.3932594060897827, value function loss: 1.9361258745193481, entropy loss: 0.5593104362487793\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7403325438499451, CLIP loss: -0.3975754678249359, value function loss: 2.2871015071868896, entropy loss: 0.5642694234848022\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7160362601280212, CLIP loss: -0.3330354690551758, value function loss: 2.109330415725708, entropy loss: 0.5593496561050415\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.647124707698822, CLIP loss: -0.45740899443626404, value function loss: 2.2203102111816406, entropy loss: 0.5621349215507507\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8701065182685852, CLIP loss: -0.29010558128356934, value function loss: 2.3313910961151123, entropy loss: 0.5483449101448059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4304051399230957, CLIP loss: -0.971177875995636, value function loss: 4.814545631408691, entropy loss: 0.5689914226531982\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.0491830110549927, CLIP loss: -0.8696858286857605, value function loss: 3.8491086959838867, entropy loss: 0.5685459971427917\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.952729344367981, CLIP loss: -0.7756860852241516, value function loss: 3.46814227104187, entropy loss: 0.5655708909034729\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.924425482749939, CLIP loss: -1.094601035118103, value function loss: 4.049387454986572, entropy loss: 0.5667209625244141\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.824575662612915, CLIP loss: -0.1892211139202118, value function loss: 2.0386500358581543, entropy loss: 0.5528184175491333\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7453452348709106, CLIP loss: -0.29328349232673645, value function loss: 2.0884883403778076, entropy loss: 0.5615494847297668\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7598538994789124, CLIP loss: -0.21978700160980225, value function loss: 1.970398187637329, entropy loss: 0.5558218359947205\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7667121291160583, CLIP loss: -0.2417566031217575, value function loss: 2.028172731399536, entropy loss: 0.561761736869812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.589392900466919, CLIP loss: -0.19550028443336487, value function loss: 1.581074595451355, entropy loss: 0.5644105672836304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6823768615722656, CLIP loss: -0.242786705493927, value function loss: 1.8615598678588867, entropy loss: 0.5616395473480225\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.45126959681510925, CLIP loss: -0.3117990493774414, value function loss: 1.5373446941375732, entropy loss: 0.5603702664375305\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6599277853965759, CLIP loss: -0.1217915266752243, value function loss: 1.5745291709899902, entropy loss: 0.5545251965522766\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8645492196083069, CLIP loss: 0.05730564147233963, value function loss: 1.625673770904541, entropy loss: 0.5593277215957642\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7371755838394165, CLIP loss: 0.027869824320077896, value function loss: 1.4293925762176514, entropy loss: 0.5390530824661255\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8784343600273132, CLIP loss: 0.09373067319393158, value function loss: 1.579998254776001, entropy loss: 0.5295442938804626\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.8028862476348877, CLIP loss: 0.0310289915651083, value function loss: 1.5543235540390015, entropy loss: 0.5304522514343262\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.757883071899414, CLIP loss: 0.5624702572822571, value function loss: 16.4019775390625, entropy loss: 0.5575749278068542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.00106430053711, CLIP loss: 0.5607002377510071, value function loss: 24.891565322875977, entropy loss: 0.5418784022331238\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.221874237060547, CLIP loss: 0.4958912432193756, value function loss: 17.46259117126465, entropy loss: 0.5313051342964172\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.952247619628906, CLIP loss: 0.7978688478469849, value function loss: 22.31884765625, entropy loss: 0.5044710636138916\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5013580322265625, CLIP loss: -0.26279258728027344, value function loss: 1.539419174194336, entropy loss: 0.5558983087539673\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7106354236602783, CLIP loss: -0.15432246029376984, value function loss: 1.7410392761230469, entropy loss: 0.5561744570732117\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6898750066757202, CLIP loss: -0.12337617576122284, value function loss: 1.637426495552063, entropy loss: 0.5462053418159485\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5000976920127869, CLIP loss: -0.3037669062614441, value function loss: 1.6186655759811401, entropy loss: 0.5468183755874634\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5065545439720154, CLIP loss: -0.44920533895492554, value function loss: 1.922444462776184, entropy loss: 0.5462366938591003\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5528115034103394, CLIP loss: -0.5020080208778381, value function loss: 2.1206917762756348, entropy loss: 0.5526336431503296\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5911093354225159, CLIP loss: -0.44948214292526245, value function loss: 2.09206485748291, entropy loss: 0.5440922379493713\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5093291401863098, CLIP loss: -0.4886525571346283, value function loss: 2.0068631172180176, entropy loss: 0.5449815392494202\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.747127056121826, CLIP loss: 0.15391258895397186, value function loss: 11.197430610656738, entropy loss: 0.5500733852386475\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.191880702972412, CLIP loss: 0.03848746791481972, value function loss: 8.317712783813477, entropy loss: 0.5463301539421082\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.800143241882324, CLIP loss: 0.12385540455579758, value function loss: 11.363479614257812, entropy loss: 0.5452326536178589\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.792295455932617, CLIP loss: 0.08315800130367279, value function loss: 7.429159164428711, entropy loss: 0.5442050695419312\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.141685962677002, CLIP loss: 0.5661938190460205, value function loss: 11.162091255187988, entropy loss: 0.5553892254829407\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.967429161071777, CLIP loss: 0.5080474615097046, value function loss: 18.929521560668945, entropy loss: 0.5378449559211731\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.997433185577393, CLIP loss: 0.8381026387214661, value function loss: 14.329187393188477, entropy loss: 0.5262926816940308\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.069441795349121, CLIP loss: 0.6294900178909302, value function loss: 14.889946937561035, entropy loss: 0.5022318363189697\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.351283073425293, CLIP loss: 0.8695268034934998, value function loss: 2.9741013050079346, entropy loss: 0.5294440388679504\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.2702322006225586, CLIP loss: 0.9191974401473999, value function loss: 2.712843418121338, entropy loss: 0.538692831993103\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.081895589828491, CLIP loss: 0.7918753027915955, value function loss: 2.5906307697296143, entropy loss: 0.5295116901397705\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.2543346881866455, CLIP loss: 0.9416471719741821, value function loss: 2.6358211040496826, entropy loss: 0.5222965478897095\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.856193542480469, CLIP loss: -0.7689592838287354, value function loss: 19.261140823364258, entropy loss: 0.5418023467063904\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.858182907104492, CLIP loss: -0.35412418842315674, value function loss: 22.435775756835938, entropy loss: 0.5580642819404602\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.309710502624512, CLIP loss: -0.559877336025238, value function loss: 23.75003433227539, entropy loss: 0.5429096221923828\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.641420364379883, CLIP loss: -0.38223832845687866, value function loss: 18.058259963989258, entropy loss: 0.5471667647361755\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.498685121536255, CLIP loss: 0.024616044014692307, value function loss: 4.959143161773682, entropy loss: 0.5502376556396484\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.311192035675049, CLIP loss: -0.07619699835777283, value function loss: 4.785562992095947, entropy loss: 0.5392447710037231\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.2685465812683105, CLIP loss: -0.10806658864021301, value function loss: 4.764098167419434, entropy loss: 0.543594479560852\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.436652660369873, CLIP loss: 0.05458112061023712, value function loss: 4.774956226348877, entropy loss: 0.5406695604324341\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.8681256771087646, CLIP loss: -0.877688467502594, value function loss: 5.502908229827881, entropy loss: 0.5640028715133667\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4249284267425537, CLIP loss: -0.8779667019844055, value function loss: 4.617025852203369, entropy loss: 0.5617854595184326\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4141342639923096, CLIP loss: -0.9700225591659546, value function loss: 4.779463768005371, entropy loss: 0.5575013756752014\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.4364171028137207, CLIP loss: -0.7776284217834473, value function loss: 4.439507007598877, entropy loss: 0.5707939863204956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.143189549446106, CLIP loss: -0.39681369066238403, value function loss: 3.091308832168579, entropy loss: 0.5651097893714905\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.3181462287902832, CLIP loss: -0.5226204991340637, value function loss: 3.692849636077881, entropy loss: 0.5658109784126282\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.1701232194900513, CLIP loss: -0.5098294019699097, value function loss: 3.371147632598877, entropy loss: 0.5621245503425598\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.2310956716537476, CLIP loss: -0.38758864998817444, value function loss: 3.24877667427063, entropy loss: 0.570409893989563\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.0207579135894775, CLIP loss: -0.15625013411045074, value function loss: 2.365349054336548, entropy loss: 0.5666534900665283\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.626659631729126, CLIP loss: -0.4048846364021301, value function loss: 2.0740432739257812, entropy loss: 0.5477349758148193\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7263301014900208, CLIP loss: -0.26849663257598877, value function loss: 2.0007710456848145, entropy loss: 0.5558792948722839\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.755068302154541, CLIP loss: -0.2942264676094055, value function loss: 2.1097359657287598, entropy loss: 0.5573225021362305\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.796226441860199, CLIP loss: -0.2597692608833313, value function loss: 2.1227798461914062, entropy loss: 0.5394207239151001\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7016391158103943, CLIP loss: -0.2879374027252197, value function loss: 1.9902632236480713, entropy loss: 0.555512011051178\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7791019082069397, CLIP loss: -0.2777070999145508, value function loss: 2.124521017074585, entropy loss: 0.5451504588127136\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.586829662322998, CLIP loss: -0.2891204059123993, value function loss: 1.7627800703048706, entropy loss: 0.5440025925636292\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6200858950614929, CLIP loss: -0.09093308448791504, value function loss: 1.432924747467041, entropy loss: 0.5443399548530579\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6756515502929688, CLIP loss: -0.0521685965359211, value function loss: 1.466429591178894, entropy loss: 0.5394666194915771\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6830763816833496, CLIP loss: -0.07769456505775452, value function loss: 1.532410740852356, entropy loss: 0.5434421896934509\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6714621782302856, CLIP loss: -0.05632667616009712, value function loss: 1.4664673805236816, entropy loss: 0.5444827079772949\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5300606489181519, CLIP loss: -0.13933545351028442, value function loss: 1.3498561382293701, entropy loss: 0.5531943440437317\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.425246000289917, CLIP loss: -0.2674241065979004, value function loss: 1.3962199687957764, entropy loss: 0.5439882278442383\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.7692318558692932, CLIP loss: -0.07450121641159058, value function loss: 1.6983981132507324, entropy loss: 0.5465985536575317\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.30096331238746643, CLIP loss: -0.31457775831222534, value function loss: 1.2422354221343994, entropy loss: 0.5576633810997009\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.497122585773468, CLIP loss: -0.1328376829624176, value function loss: 1.2711294889450073, entropy loss: 0.5604435801506042\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5989378094673157, CLIP loss: -0.04846213012933731, value function loss: 1.3058509826660156, entropy loss: 0.5525516271591187\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.490518718957901, CLIP loss: -0.12170980870723724, value function loss: 1.2355047464370728, entropy loss: 0.5523841381072998\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5284458994865417, CLIP loss: -0.051333144307136536, value function loss: 1.1706734895706177, entropy loss: 0.555770218372345\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4456022381782532, CLIP loss: -0.1583719700574875, value function loss: 1.2189629077911377, entropy loss: 0.5507245659828186\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4636049270629883, CLIP loss: -0.09430380910634995, value function loss: 1.1269093751907349, entropy loss: 0.5545939207077026\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.38179290294647217, CLIP loss: -0.1436426192522049, value function loss: 1.061908483505249, entropy loss: 0.5518720149993896\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4835416376590729, CLIP loss: -0.12254578620195389, value function loss: 1.2232708930969238, entropy loss: 0.5548022389411926\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.7830305099487305, CLIP loss: 0.3643496036529541, value function loss: 14.848455429077148, entropy loss: 0.5546776652336121\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.551046371459961, CLIP loss: 0.8253795504570007, value function loss: 27.462345123291016, entropy loss: 0.5505099892616272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.490893363952637, CLIP loss: 0.6199760437011719, value function loss: 23.752819061279297, entropy loss: 0.5491907000541687\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.495156288146973, CLIP loss: 0.6001697778701782, value function loss: 17.800687789916992, entropy loss: 0.5357558727264404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1801147609949112, CLIP loss: -0.3637148141860962, value function loss: 1.0984903573989868, entropy loss: 0.5415608286857605\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2562249004840851, CLIP loss: -0.28156039118766785, value function loss: 1.086658239364624, entropy loss: 0.5543832182884216\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2534559965133667, CLIP loss: -0.3319704830646515, value function loss: 1.1819138526916504, entropy loss: 0.553045392036438\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18812210857868195, CLIP loss: -0.313188374042511, value function loss: 1.013474464416504, entropy loss: 0.5426746606826782\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3416888415813446, CLIP loss: -0.20872849225997925, value function loss: 1.1120285987854004, entropy loss: 0.5596979260444641\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.36812540888786316, CLIP loss: -0.2031906247138977, value function loss: 1.1538043022155762, entropy loss: 0.5586107969284058\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3036825358867645, CLIP loss: -0.22244232892990112, value function loss: 1.0634065866470337, entropy loss: 0.5578434467315674\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3477916121482849, CLIP loss: -0.19813573360443115, value function loss: 1.103148341178894, entropy loss: 0.5646824836730957\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.643694877624512, CLIP loss: 0.29374298453330994, value function loss: 18.711015701293945, entropy loss: 0.5555731058120728\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.325117111206055, CLIP loss: 0.21700111031532288, value function loss: 18.227230072021484, entropy loss: 0.5498937368392944\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.478700637817383, CLIP loss: 0.3006497621536255, value function loss: 16.3671932220459, entropy loss: 0.5545809864997864\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.21252727508545, CLIP loss: 0.2492532730102539, value function loss: 19.937429428100586, entropy loss: 0.5440315008163452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23994752764701843, CLIP loss: -0.38179290294647217, value function loss: 1.2546186447143555, entropy loss: 0.5568892359733582\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35764631628990173, CLIP loss: -0.3298061788082123, value function loss: 1.386011004447937, entropy loss: 0.5553008317947388\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2920587658882141, CLIP loss: -0.39160025119781494, value function loss: 1.3783751726150513, entropy loss: 0.552856981754303\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35904741287231445, CLIP loss: -0.3337554335594177, value function loss: 1.3966984748840332, entropy loss: 0.5546377301216125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14790624380111694, CLIP loss: -0.5208295583724976, value function loss: 1.3486250638961792, entropy loss: 0.5576723217964172\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15687349438667297, CLIP loss: -0.570642352104187, value function loss: 1.4661402702331543, entropy loss: 0.5554294586181641\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07271142303943634, CLIP loss: -0.5750547647476196, value function loss: 1.3065379858016968, entropy loss: 0.5502805113792419\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10446671396493912, CLIP loss: -0.5219767689704895, value function loss: 1.2639751434326172, entropy loss: 0.5544089078903198\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4838281273841858, CLIP loss: -0.15621253848075867, value function loss: 1.2910511493682861, entropy loss: 0.5484922528266907\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3641089200973511, CLIP loss: -0.2495928406715393, value function loss: 1.2384064197540283, entropy loss: 0.5501442551612854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3710174858570099, CLIP loss: -0.2486112117767334, value function loss: 1.250327467918396, entropy loss: 0.5535041689872742\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.48158952593803406, CLIP loss: -0.15361441671848297, value function loss: 1.2813445329666138, entropy loss: 0.5468335747718811\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5390986204147339, CLIP loss: -0.060926586389541626, value function loss: 1.2110118865966797, entropy loss: 0.548075795173645\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.442488431930542, CLIP loss: -0.12600156664848328, value function loss: 1.1480283737182617, entropy loss: 0.5524198412895203\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.498883455991745, CLIP loss: -0.06252402067184448, value function loss: 1.1338436603546143, entropy loss: 0.5514351725578308\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.42484673857688904, CLIP loss: -0.12066790461540222, value function loss: 1.1019682884216309, entropy loss: 0.5469503402709961\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33131667971611023, CLIP loss: -0.18340584635734558, value function loss: 1.040337085723877, entropy loss: 0.5446026921272278\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3618318736553192, CLIP loss: -0.13281039893627167, value function loss: 1.000150203704834, entropy loss: 0.5432828664779663\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.37865522503852844, CLIP loss: -0.13426287472248077, value function loss: 1.0367518663406372, entropy loss: 0.5457858443260193\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31025317311286926, CLIP loss: -0.19369614124298096, value function loss: 1.0186759233474731, entropy loss: 0.5388650894165039\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3865853548049927, CLIP loss: -0.06870562583208084, value function loss: 0.9218514561653137, entropy loss: 0.5634740591049194\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3907003104686737, CLIP loss: -0.17069095373153687, value function loss: 1.1338443756103516, entropy loss: 0.5530936121940613\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4052962362766266, CLIP loss: -0.10866959393024445, value function loss: 1.0390874147415161, entropy loss: 0.5577900409698486\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3990633487701416, CLIP loss: -0.12451370805501938, value function loss: 1.0581817626953125, entropy loss: 0.5513824820518494\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19535736739635468, CLIP loss: -0.3326672911643982, value function loss: 1.0671483278274536, entropy loss: 0.5549512505531311\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3603605031967163, CLIP loss: -0.24699975550174713, value function loss: 1.2256903648376465, entropy loss: 0.5484942197799683\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.237164705991745, CLIP loss: -0.31896042823791504, value function loss: 1.1232792139053345, entropy loss: 0.5514470338821411\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3150554597377777, CLIP loss: -0.24987104535102844, value function loss: 1.140852451324463, entropy loss: 0.5499707460403442\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3747533857822418, CLIP loss: -0.11939573287963867, value function loss: 0.9991871118545532, entropy loss: 0.5444425940513611\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.42108291387557983, CLIP loss: -0.10872817784547806, value function loss: 1.0706133842468262, entropy loss: 0.5495592951774597\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3891718089580536, CLIP loss: -0.12116255611181259, value function loss: 1.031615972518921, entropy loss: 0.5473610162734985\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41452446579933167, CLIP loss: -0.10665719211101532, value function loss: 1.0534484386444092, entropy loss: 0.554253876209259\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1570853739976883, CLIP loss: -0.31746047735214233, value function loss: 0.9602270722389221, entropy loss: 0.5567688941955566\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1199105829000473, CLIP loss: -0.3754621744155884, value function loss: 1.0017493963241577, entropy loss: 0.5501942038536072\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22360575199127197, CLIP loss: -0.3247944712638855, value function loss: 1.1079316139221191, entropy loss: 0.5565582513809204\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11778576672077179, CLIP loss: -0.3626433312892914, value function loss: 0.9718536138534546, entropy loss: 0.5497705340385437\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3375365734100342, CLIP loss: -0.16438615322113037, value function loss: 1.0149097442626953, entropy loss: 0.553215503692627\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4357522428035736, CLIP loss: -0.078880675137043, value function loss: 1.0403847694396973, entropy loss: 0.5559471249580383\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3873593211174011, CLIP loss: -0.12856018543243408, value function loss: 1.0428385734558105, entropy loss: 0.5499790906906128\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.34397849440574646, CLIP loss: -0.10889540612697601, value function loss: 0.9165875911712646, entropy loss: 0.5419883728027344\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23602622747421265, CLIP loss: -0.25491151213645935, value function loss: 0.9929133057594299, entropy loss: 0.5518914461135864\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22795766592025757, CLIP loss: -0.22471444308757782, value function loss: 0.9164705276489258, entropy loss: 0.5563152432441711\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27832165360450745, CLIP loss: -0.22811377048492432, value function loss: 1.023857831954956, entropy loss: 0.5493490695953369\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2661975920200348, CLIP loss: -0.2499794363975525, value function loss: 1.0433790683746338, entropy loss: 0.5512509346008301\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24685730040073395, CLIP loss: -0.19511668384075165, value function loss: 0.8950865268707275, entropy loss: 0.556925892829895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3857693076133728, CLIP loss: -0.1514054834842682, value function loss: 1.0853055715560913, entropy loss: 0.5477980375289917\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.330841988325119, CLIP loss: -0.19502517580986023, value function loss: 1.0626311302185059, entropy loss: 0.544841468334198\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3077254593372345, CLIP loss: -0.14352962374687195, value function loss: 0.9134889841079712, entropy loss: 0.5489410758018494\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11502068489789963, CLIP loss: -0.4174817204475403, value function loss: 1.0761388540267944, entropy loss: 0.5567024946212769\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24853961169719696, CLIP loss: -0.2877844572067261, value function loss: 1.0837301015853882, entropy loss: 0.5540982484817505\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11528962850570679, CLIP loss: -0.30764883756637573, value function loss: 0.8570430874824524, entropy loss: 0.5583081245422363\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16873881220817566, CLIP loss: -0.37930828332901, value function loss: 1.107265591621399, entropy loss: 0.5585699677467346\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4259507656097412, CLIP loss: -0.05395526438951492, value function loss: 0.9706888198852539, entropy loss: 0.5438379049301147\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5558465123176575, CLIP loss: 0.029332365840673447, value function loss: 1.0640244483947754, entropy loss: 0.5498073697090149\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5237144231796265, CLIP loss: 0.04485112428665161, value function loss: 0.9686603546142578, entropy loss: 0.5466904044151306\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3866405189037323, CLIP loss: -0.06318405270576477, value function loss: 0.910785973072052, entropy loss: 0.5568418502807617\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26745808124542236, CLIP loss: -0.21379613876342773, value function loss: 0.9737922549247742, entropy loss: 0.5641913414001465\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24492807686328888, CLIP loss: -0.23363491892814636, value function loss: 0.9683151245117188, entropy loss: 0.5594567656517029\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2812357544898987, CLIP loss: -0.19912712275981903, value function loss: 0.9718223810195923, entropy loss: 0.5548296570777893\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2323550432920456, CLIP loss: -0.23961636424064636, value function loss: 0.9551516175270081, entropy loss: 0.5604400038719177\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.307665079832077, CLIP loss: -0.129778191447258, value function loss: 0.8860166072845459, entropy loss: 0.556502103805542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.36514079570770264, CLIP loss: -0.10203220695257187, value function loss: 0.9453898072242737, entropy loss: 0.5521891117095947\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2935418486595154, CLIP loss: -0.16277165710926056, value function loss: 0.9234578013420105, entropy loss: 0.5415390729904175\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4029538631439209, CLIP loss: -0.06810891628265381, value function loss: 0.953216552734375, entropy loss: 0.5545483827590942\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35064029693603516, CLIP loss: -0.13831736147403717, value function loss: 0.9890907406806946, entropy loss: 0.5587693452835083\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.37694457173347473, CLIP loss: -0.11882340162992477, value function loss: 1.002739429473877, entropy loss: 0.560173749923706\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3392774760723114, CLIP loss: -0.1255558282136917, value function loss: 0.9409261345863342, entropy loss: 0.5629740357398987\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.29156357049942017, CLIP loss: -0.14682015776634216, value function loss: 0.8877335786819458, entropy loss: 0.5483061075210571\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2810460329055786, CLIP loss: -0.18101844191551208, value function loss: 0.9351609945297241, entropy loss: 0.5516030788421631\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3084522485733032, CLIP loss: -0.23665478825569153, value function loss: 1.1012126207351685, entropy loss: 0.5499274134635925\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24064593017101288, CLIP loss: -0.2333037108182907, value function loss: 0.9590235352516174, entropy loss: 0.5562127232551575\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28973260521888733, CLIP loss: -0.17588704824447632, value function loss: 0.9422673583030701, entropy loss: 0.5514014363288879\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3034856915473938, CLIP loss: -0.16675765812397003, value function loss: 0.9515097141265869, entropy loss: 0.5511511564254761\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.42547667026519775, CLIP loss: -0.10111432522535324, value function loss: 1.0643068552017212, entropy loss: 0.5562424063682556\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.413005530834198, CLIP loss: -0.14800366759300232, value function loss: 1.133286952972412, entropy loss: 0.5634267330169678\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3721342980861664, CLIP loss: -0.10681650042533875, value function loss: 0.9689295887947083, entropy loss: 0.5513981580734253\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28700345754623413, CLIP loss: -0.1348472237586975, value function loss: 0.8547306656837463, entropy loss: 0.5514657497406006\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.243922621011734, CLIP loss: -0.2487488090991974, value function loss: 0.9966732859611511, entropy loss: 0.5665214657783508\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2716377377510071, CLIP loss: -0.1691519320011139, value function loss: 0.89262855052948, entropy loss: 0.5524598956108093\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2316794991493225, CLIP loss: -0.20491328835487366, value function loss: 0.8844121694564819, entropy loss: 0.5613293647766113\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.36725863814353943, CLIP loss: -0.07186687737703323, value function loss: 0.8894855380058289, entropy loss: 0.561724841594696\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3843500018119812, CLIP loss: -0.11783338338136673, value function loss: 1.015302062034607, entropy loss: 0.5467667579650879\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3958592712879181, CLIP loss: -0.07400000095367432, value function loss: 0.9507671594619751, entropy loss: 0.5524308681488037\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28655532002449036, CLIP loss: -0.10757791250944138, value function loss: 0.7991448044776917, entropy loss: 0.5439175963401794\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3019777238368988, CLIP loss: -0.18964043259620667, value function loss: 0.9940891861915588, entropy loss: 0.5426424741744995\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.45082226395606995, CLIP loss: -0.12931878864765167, value function loss: 1.1714571714401245, entropy loss: 0.5587543249130249\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3990716338157654, CLIP loss: -0.20714536309242249, value function loss: 1.2232815027236938, entropy loss: 0.5423755049705505\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35292965173721313, CLIP loss: -0.11659912765026093, value function loss: 0.9502678513526917, entropy loss: 0.5605158805847168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21973274648189545, CLIP loss: -0.19552457332611084, value function loss: 0.8415960073471069, entropy loss: 0.554067850112915\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23514294624328613, CLIP loss: -0.22886061668395996, value function loss: 0.9391130208969116, entropy loss: 0.5552940964698792\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25324904918670654, CLIP loss: -0.18809257447719574, value function loss: 0.8937844038009644, entropy loss: 0.5550556778907776\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22143465280532837, CLIP loss: -0.23693612217903137, value function loss: 0.9277254343032837, entropy loss: 0.5491940379142761\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5340598821640015, CLIP loss: -0.1606820523738861, value function loss: 1.4004621505737305, entropy loss: 0.5489085912704468\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5243421792984009, CLIP loss: -0.0037988796830177307, value function loss: 1.0673370361328125, entropy loss: 0.5527461767196655\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5366791486740112, CLIP loss: -0.05518391728401184, value function loss: 1.1945797204971313, entropy loss: 0.5426738262176514\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.510513186454773, CLIP loss: -0.08267400413751602, value function loss: 1.197314739227295, entropy loss: 0.5470163822174072\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.38494500517845154, CLIP loss: -0.05251412093639374, value function loss: 0.8858952522277832, entropy loss: 0.5488470792770386\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.49619561433792114, CLIP loss: -0.01295720785856247, value function loss: 1.0293012857437134, entropy loss: 0.549782395362854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4105164110660553, CLIP loss: -0.0020530084148049355, value function loss: 0.8359851241111755, entropy loss: 0.5423136353492737\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4402869641780853, CLIP loss: -0.03118063136935234, value function loss: 0.9539915323257446, entropy loss: 0.5528184771537781\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.448861598968506, CLIP loss: 0.27685385942459106, value function loss: 8.355264663696289, entropy loss: 0.5624750256538391\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.06557559967041, CLIP loss: 0.42351773381233215, value function loss: 15.295234680175781, entropy loss: 0.5559495091438293\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.015031814575195, CLIP loss: 0.4349416196346283, value function loss: 13.171310424804688, entropy loss: 0.5564985871315002\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.229917526245117, CLIP loss: 0.27748367190361023, value function loss: 9.915987014770508, entropy loss: 0.5559337735176086\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41659948229789734, CLIP loss: -0.014913750812411308, value function loss: 0.8742367029190063, entropy loss: 0.5605129599571228\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3959837853908539, CLIP loss: -0.06495483219623566, value function loss: 0.9328479766845703, entropy loss: 0.5485397577285767\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.378121018409729, CLIP loss: -0.06319989264011383, value function loss: 0.8935994505882263, entropy loss: 0.5478798747062683\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.44737130403518677, CLIP loss: -0.02137698419392109, value function loss: 0.9486294984817505, entropy loss: 0.5566447973251343\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4110340476036072, CLIP loss: -0.051216285675764084, value function loss: 0.9357064366340637, entropy loss: 0.5602893233299255\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23917771875858307, CLIP loss: -0.19111016392707825, value function loss: 0.8716382384300232, entropy loss: 0.5531234741210938\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3303963840007782, CLIP loss: -0.12506388127803802, value function loss: 0.9219867587089539, entropy loss: 0.5533117055892944\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27040332555770874, CLIP loss: -0.13249680399894714, value function loss: 0.816912829875946, entropy loss: 0.5556278228759766\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26055943965911865, CLIP loss: -0.15822651982307434, value function loss: 0.8486076593399048, entropy loss: 0.551787793636322\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2318241447210312, CLIP loss: -0.17765408754348755, value function loss: 0.8299556970596313, entropy loss: 0.5499622821807861\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28085917234420776, CLIP loss: -0.1353127807378769, value function loss: 0.8432595729827881, entropy loss: 0.5457829236984253\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22949695587158203, CLIP loss: -0.18602073192596436, value function loss: 0.8417829871177673, entropy loss: 0.5373802781105042\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2622602581977844, CLIP loss: -0.11732923984527588, value function loss: 0.7704201340675354, entropy loss: 0.5620579719543457\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3764618933200836, CLIP loss: -0.07703503966331482, value function loss: 0.9182754158973694, entropy loss: 0.5640769004821777\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.29157131910324097, CLIP loss: -0.09243156015872955, value function loss: 0.7794022560119629, entropy loss: 0.5698268413543701\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.29320237040519714, CLIP loss: -0.10018505901098251, value function loss: 0.7978699803352356, entropy loss: 0.5547549724578857\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35993924736976624, CLIP loss: -0.05143846571445465, value function loss: 0.8337631225585938, entropy loss: 0.5503849387168884\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41060709953308105, CLIP loss: -0.06244458630681038, value function loss: 0.9571543335914612, entropy loss: 0.5525468587875366\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28834977746009827, CLIP loss: -0.12041181325912476, value function loss: 0.8287557363510132, entropy loss: 0.5616273880004883\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.44714364409446716, CLIP loss: 0.019188057631254196, value function loss: 0.866771399974823, entropy loss: 0.5430104732513428\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5441035628318787, CLIP loss: 0.07405664771795273, value function loss: 0.9511130452156067, entropy loss: 0.5509598255157471\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.496639221906662, CLIP loss: 0.03656631335616112, value function loss: 0.9312624931335449, entropy loss: 0.5558350682258606\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5351768136024475, CLIP loss: 0.0864001139998436, value function loss: 0.9087110757827759, entropy loss: 0.5578809976577759\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4788891673088074, CLIP loss: 0.019071659073233604, value function loss: 0.9306603074073792, entropy loss: 0.5512663125991821\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3307702839374542, CLIP loss: -0.17758949100971222, value function loss: 1.027953028678894, entropy loss: 0.5616750717163086\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4785033166408539, CLIP loss: -0.07678716629743576, value function loss: 1.121807336807251, entropy loss: 0.5613173842430115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3550002872943878, CLIP loss: -0.12973271310329437, value function loss: 0.9806684851646423, entropy loss: 0.5601251721382141\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.331819623708725, CLIP loss: -0.13343849778175354, value function loss: 0.9417628645896912, entropy loss: 0.5623316168785095\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.29054945707321167, CLIP loss: -0.05605361983180046, value function loss: 0.70436692237854, entropy loss: 0.5580384731292725\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.32999926805496216, CLIP loss: -0.07937155663967133, value function loss: 0.8298364877700806, entropy loss: 0.5547419190406799\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3269295394420624, CLIP loss: -0.05951415374875069, value function loss: 0.7841062545776367, entropy loss: 0.5609410405158997\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33541998267173767, CLIP loss: -0.08277548849582672, value function loss: 0.8474833369255066, entropy loss: 0.5546175241470337\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.314455032348633, CLIP loss: 0.7187060713768005, value function loss: 23.20249366760254, entropy loss: 0.5498289465904236\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.001166343688965, CLIP loss: 0.7824091911315918, value function loss: 18.44849967956543, entropy loss: 0.5492572784423828\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.900202751159668, CLIP loss: 0.9049590826034546, value function loss: 26.00152587890625, entropy loss: 0.5518747568130493\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.999717712402344, CLIP loss: 0.584615170955658, value function loss: 14.841184616088867, entropy loss: 0.5489476919174194\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0685654878616333, CLIP loss: -0.35199031233787537, value function loss: 0.8522667288780212, entropy loss: 0.5577563643455505\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10043010115623474, CLIP loss: -0.3080156147480011, value function loss: 0.8281213641166687, entropy loss: 0.5614964962005615\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.009092518128454685, CLIP loss: -0.3272276520729065, value function loss: 0.683768630027771, entropy loss: 0.5564144849777222\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04141722247004509, CLIP loss: -0.3397090435028076, value function loss: 0.77326500415802, entropy loss: 0.5506237149238586\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1054917499423027, CLIP loss: -0.27079305052757263, value function loss: 0.763969898223877, entropy loss: 0.5700149536132812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1915549635887146, CLIP loss: -0.21472042798995972, value function loss: 0.82374107837677, entropy loss: 0.5595149397850037\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20920969545841217, CLIP loss: -0.22347721457481384, value function loss: 0.8766608238220215, entropy loss: 0.5643495321273804\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09305267035961151, CLIP loss: -0.26979827880859375, value function loss: 0.7370406985282898, entropy loss: 0.5669402480125427\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1588752567768097, CLIP loss: -0.2787863314151764, value function loss: 0.8865271806716919, entropy loss: 0.560199499130249\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24300451576709747, CLIP loss: -0.20118294656276703, value function loss: 0.8994600176811218, entropy loss: 0.5542545914649963\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16992779076099396, CLIP loss: -0.24036835134029388, value function loss: 0.8317418098449707, entropy loss: 0.5574762225151062\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1388346552848816, CLIP loss: -0.24474801123142242, value function loss: 0.7784408330917358, entropy loss: 0.5637755393981934\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08206550776958466, CLIP loss: -0.3497556149959564, value function loss: 0.8745698928833008, entropy loss: 0.5463825464248657\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1164349839091301, CLIP loss: -0.30288922786712646, value function loss: 0.8494845628738403, entropy loss: 0.5418072938919067\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10610409080982208, CLIP loss: -0.3220667839050293, value function loss: 0.8671888113021851, entropy loss: 0.5423531532287598\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0548122301697731, CLIP loss: -0.32659804821014404, value function loss: 0.7737329006195068, entropy loss: 0.5456171631813049\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2830301821231842, CLIP loss: -0.18632404506206512, value function loss: 0.9497924447059631, entropy loss: 0.5542005896568298\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26835042238235474, CLIP loss: -0.16355448961257935, value function loss: 0.8748134970664978, entropy loss: 0.5501842498779297\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24010030925273895, CLIP loss: -0.1654287427663803, value function loss: 0.8221278786659241, entropy loss: 0.5534894466400146\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21377530694007874, CLIP loss: -0.17845922708511353, value function loss: 0.7955758571624756, entropy loss: 0.5553396940231323\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20761790871620178, CLIP loss: -0.2111389935016632, value function loss: 0.8486465811729431, entropy loss: 0.5566391348838806\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1966065764427185, CLIP loss: -0.22540071606636047, value function loss: 0.8549813032150269, entropy loss: 0.5483366250991821\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18355952203273773, CLIP loss: -0.18379755318164825, value function loss: 0.7457802891731262, entropy loss: 0.5533069372177124\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10368817299604416, CLIP loss: -0.2607104480266571, value function loss: 0.7398478984832764, entropy loss: 0.5525324940681458\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.390772819519043, CLIP loss: 0.4507782459259033, value function loss: 15.890928268432617, entropy loss: 0.5469107031822205\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.11628532409668, CLIP loss: 0.5145303010940552, value function loss: 19.214637756347656, entropy loss: 0.5563540458679199\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.331098556518555, CLIP loss: 0.4146313965320587, value function loss: 13.843997955322266, entropy loss: 0.5531649589538574\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.523063659667969, CLIP loss: 0.5982899069786072, value function loss: 21.860496520996094, entropy loss: 0.5473968386650085\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07225421816110611, CLIP loss: -0.34448152780532837, value function loss: 0.8445327281951904, entropy loss: 0.5530619025230408\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07527120411396027, CLIP loss: -0.3321603238582611, value function loss: 0.8259146213531494, entropy loss: 0.5525782704353333\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12080752104520798, CLIP loss: -0.3087857961654663, value function loss: 0.8703283667564392, entropy loss: 0.5570865273475647\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.047243788838386536, CLIP loss: -0.3700864315032959, value function loss: 0.8457319140434265, entropy loss: 0.553573727607727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1161443442106247, CLIP loss: -0.3156648874282837, value function loss: 0.8746157288551331, entropy loss: 0.5498631596565247\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14344768226146698, CLIP loss: -0.34528428316116333, value function loss: 0.9885331392288208, entropy loss: 0.553460419178009\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13215503096580505, CLIP loss: -0.3127520978450775, value function loss: 0.9008748531341553, entropy loss: 0.5530300736427307\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08719012886285782, CLIP loss: -0.35609710216522217, value function loss: 0.8975728750228882, entropy loss: 0.5499204993247986\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.9470200538635254, CLIP loss: 0.0012893006205558777, value function loss: 7.902414321899414, entropy loss: 0.547649085521698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.064434051513672, CLIP loss: -0.11483673751354218, value function loss: 8.369681358337402, entropy loss: 0.5569736361503601\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.171201229095459, CLIP loss: -0.11818087846040726, value function loss: 4.589855670928955, entropy loss: 0.5545555949211121\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.73644495010376, CLIP loss: -0.021718844771385193, value function loss: 11.527223587036133, entropy loss: 0.5447754859924316\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.245286226272583, CLIP loss: -0.1895938664674759, value function loss: 0.8809833526611328, entropy loss: 0.5611603260040283\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3496182858943939, CLIP loss: -0.0783243328332901, value function loss: 0.8670036196708679, entropy loss: 0.5559161901473999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20638062059879303, CLIP loss: -0.19797611236572266, value function loss: 0.8198907375335693, entropy loss: 0.558863639831543\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.329031378030777, CLIP loss: -0.0630350336432457, value function loss: 0.7953261137008667, entropy loss: 0.5596651434898376\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.7112720012664795, CLIP loss: -0.20493337512016296, value function loss: 3.8435096740722656, entropy loss: 0.5549470782279968\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.481756687164307, CLIP loss: 0.12098655849695206, value function loss: 14.732810974121094, entropy loss: 0.5635181665420532\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.298982858657837, CLIP loss: -0.05269091576337814, value function loss: 6.714416027069092, entropy loss: 0.5534130334854126\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.814303874969482, CLIP loss: -0.030174702405929565, value function loss: 11.700202941894531, entropy loss: 0.5623027682304382\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24635638296604156, CLIP loss: -0.17417967319488525, value function loss: 0.8520591259002686, entropy loss: 0.5493506789207458\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1861153542995453, CLIP loss: -0.20507460832595825, value function loss: 0.7934805750846863, entropy loss: 0.5550328493118286\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33095788955688477, CLIP loss: -0.16623283922672272, value function loss: 1.005373477935791, entropy loss: 0.5496012568473816\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1804448366165161, CLIP loss: -0.21538516879081726, value function loss: 0.8025842308998108, entropy loss: 0.5462110042572021\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16242440044879913, CLIP loss: -0.34428319334983826, value function loss: 1.024572491645813, entropy loss: 0.5578646659851074\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12314781546592712, CLIP loss: -0.31130990386009216, value function loss: 0.880135178565979, entropy loss: 0.5609871745109558\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18087580800056458, CLIP loss: -0.3213220536708832, value function loss: 1.0156757831573486, entropy loss: 0.5640031099319458\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13954438269138336, CLIP loss: -0.34793198108673096, value function loss: 0.9859204292297363, entropy loss: 0.5483844876289368\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21414430439472198, CLIP loss: -0.216242253780365, value function loss: 0.8719748854637146, entropy loss: 0.5600878596305847\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1861370950937271, CLIP loss: -0.211843341588974, value function loss: 0.8071763515472412, entropy loss: 0.5607744455337524\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2559967339038849, CLIP loss: -0.21143439412117004, value function loss: 0.94603031873703, entropy loss: 0.5584028363227844\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2646216154098511, CLIP loss: -0.19222472608089447, value function loss: 0.9248708486557007, entropy loss: 0.5589065551757812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12316589802503586, CLIP loss: -0.25750312209129333, value function loss: 0.7723186016082764, entropy loss: 0.5490280389785767\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08105694502592087, CLIP loss: -0.24067935347557068, value function loss: 0.6545919179916382, entropy loss: 0.5559663772583008\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11879700422286987, CLIP loss: -0.24749737977981567, value function loss: 0.7436605095863342, entropy loss: 0.5535868406295776\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1483054757118225, CLIP loss: -0.2348131686449051, value function loss: 0.7772221565246582, entropy loss: 0.5492439270019531\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31021836400032043, CLIP loss: -0.17019657790660858, value function loss: 0.971959114074707, entropy loss: 0.5564611554145813\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3005993366241455, CLIP loss: -0.1830907016992569, value function loss: 0.9785699844360352, entropy loss: 0.5594979524612427\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23961348831653595, CLIP loss: -0.2148667871952057, value function loss: 0.9200015068054199, entropy loss: 0.5520474314689636\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2694406509399414, CLIP loss: -0.13826879858970642, value function loss: 0.8265030384063721, entropy loss: 0.5542070865631104\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26716679334640503, CLIP loss: -0.18194732069969177, value function loss: 0.9091776609420776, entropy loss: 0.5474701523780823\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19392366707324982, CLIP loss: -0.24679607152938843, value function loss: 0.8922743797302246, entropy loss: 0.5417450666427612\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2472822666168213, CLIP loss: -0.20414012670516968, value function loss: 0.9137349128723145, entropy loss: 0.5445061922073364\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2101697474718094, CLIP loss: -0.2161775529384613, value function loss: 0.863632321357727, entropy loss: 0.5468853712081909\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3297477066516876, CLIP loss: -0.03223588690161705, value function loss: 0.7351521849632263, entropy loss: 0.5592501163482666\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.214188352227211, CLIP loss: -0.13841643929481506, value function loss: 0.7162312269210815, entropy loss: 0.551082193851471\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27782467007637024, CLIP loss: -0.08832594007253647, value function loss: 0.7434171438217163, entropy loss: 0.5557951927185059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3161565363407135, CLIP loss: -0.07326775789260864, value function loss: 0.7899496555328369, entropy loss: 0.5550538897514343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20772093534469604, CLIP loss: -0.20103034377098083, value function loss: 0.8287639617919922, entropy loss: 0.5630707144737244\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16686859726905823, CLIP loss: -0.21643410623073578, value function loss: 0.7778431177139282, entropy loss: 0.5618863105773926\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15334858000278473, CLIP loss: -0.18456904590129852, value function loss: 0.687069296836853, entropy loss: 0.5617027878761292\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22892743349075317, CLIP loss: -0.23988650739192963, value function loss: 0.9487720727920532, entropy loss: 0.5572097897529602\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20315828919410706, CLIP loss: -0.2018665373325348, value function loss: 0.8212928175926208, entropy loss: 0.5621580481529236\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3437729477882385, CLIP loss: -0.08888345211744308, value function loss: 0.8765777349472046, entropy loss: 0.5632461309432983\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2826632857322693, CLIP loss: -0.13036072254180908, value function loss: 0.8372172117233276, entropy loss: 0.5584605932235718\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1800050586462021, CLIP loss: -0.18382975459098816, value function loss: 0.7388601899147034, entropy loss: 0.559527575969696\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3300434350967407, CLIP loss: -0.0182303786277771, value function loss: 0.7077316045761108, entropy loss: 0.5591992735862732\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2393771857023239, CLIP loss: -0.12775364518165588, value function loss: 0.7453017830848694, entropy loss: 0.5520060658454895\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2605723440647125, CLIP loss: -0.07916704565286636, value function loss: 0.69075608253479, entropy loss: 0.5638650059700012\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.34345537424087524, CLIP loss: -0.06777766346931458, value function loss: 0.8334478139877319, entropy loss: 0.5490880012512207\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2609254717826843, CLIP loss: -0.07640038430690765, value function loss: 0.685775637626648, entropy loss: 0.5561935305595398\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.32736092805862427, CLIP loss: -0.02713668718934059, value function loss: 0.7200570106506348, entropy loss: 0.5530894994735718\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3075169622898102, CLIP loss: -0.08189056068658829, value function loss: 0.7897477746009827, entropy loss: 0.546636164188385\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3177933692932129, CLIP loss: -0.03315532207489014, value function loss: 0.7129596471786499, entropy loss: 0.5531118512153625\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22526144981384277, CLIP loss: -0.14139071106910706, value function loss: 0.7444058060646057, entropy loss: 0.5550736784934998\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2989702522754669, CLIP loss: -0.08103138953447342, value function loss: 0.7711424827575684, entropy loss: 0.5569597482681274\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28772032260894775, CLIP loss: -0.09030353277921677, value function loss: 0.7671704292297363, entropy loss: 0.5561351776123047\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24794520437717438, CLIP loss: -0.13233910501003265, value function loss: 0.771645724773407, entropy loss: 0.5538562536239624\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.30078139901161194, CLIP loss: -0.0791688784956932, value function loss: 0.7706349492073059, entropy loss: 0.536720335483551\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40807461738586426, CLIP loss: -0.09729507565498352, value function loss: 1.021686315536499, entropy loss: 0.5473471283912659\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41190847754478455, CLIP loss: -0.042914438992738724, value function loss: 0.9204063415527344, entropy loss: 0.5380239486694336\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3258261978626251, CLIP loss: -0.10776238143444061, value function loss: 0.8779081702232361, entropy loss: 0.5365522503852844\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4843164384365082, CLIP loss: 0.14365950226783752, value function loss: 0.6922104954719543, entropy loss: 0.5448310375213623\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5062687397003174, CLIP loss: 0.037998784333467484, value function loss: 0.9473367929458618, entropy loss: 0.539847195148468\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5193893313407898, CLIP loss: 0.062324028462171555, value function loss: 0.9249365925788879, entropy loss: 0.5402954816818237\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5448845028877258, CLIP loss: 0.11544070392847061, value function loss: 0.8695806264877319, entropy loss: 0.5346550941467285\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24002152681350708, CLIP loss: -0.12239736318588257, value function loss: 0.7357432842254639, entropy loss: 0.5452755689620972\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3688015639781952, CLIP loss: -0.09075263142585754, value function loss: 0.9300689101219177, entropy loss: 0.5480250120162964\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40909427404403687, CLIP loss: -0.06836754083633423, value function loss: 0.9659256339073181, entropy loss: 0.5500988960266113\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31119197607040405, CLIP loss: -0.15956752002239227, value function loss: 0.9523646235466003, entropy loss: 0.5422820448875427\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31953269243240356, CLIP loss: -0.0069522550329566, value function loss: 0.6639899015426636, entropy loss: 0.550999104976654\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5277800559997559, CLIP loss: 0.06272625923156738, value function loss: 0.9410817623138428, entropy loss: 0.548711359500885\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5910106301307678, CLIP loss: 0.026049572974443436, value function loss: 1.1407742500305176, entropy loss: 0.5426061153411865\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.48555973172187805, CLIP loss: 0.033201444894075394, value function loss: 0.9157761931419373, entropy loss: 0.5529811382293701\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.36134567856788635, CLIP loss: -0.01975662261247635, value function loss: 0.7731943726539612, entropy loss: 0.5494886636734009\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33458951115608215, CLIP loss: -0.08865087479352951, value function loss: 0.8576904535293579, entropy loss: 0.5604830980300903\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4612944722175598, CLIP loss: -0.015282362699508667, value function loss: 0.9642049670219421, entropy loss: 0.5525656342506409\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4182291328907013, CLIP loss: -0.0662020668387413, value function loss: 0.9799712896347046, entropy loss: 0.5554437041282654\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31345075368881226, CLIP loss: -0.016121504828333855, value function loss: 0.6700531244277954, entropy loss: 0.5454308390617371\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3812482953071594, CLIP loss: 0.04624566808342934, value function loss: 0.6811093091964722, entropy loss: 0.5552020072937012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.316604346036911, CLIP loss: -0.043676719069480896, value function loss: 0.7316178679466248, entropy loss: 0.5527870655059814\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40714529156684875, CLIP loss: 0.062092483043670654, value function loss: 0.7010537385940552, entropy loss: 0.5474061965942383\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4047847390174866, CLIP loss: 0.06847520172595978, value function loss: 0.6837100982666016, entropy loss: 0.5545501112937927\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.37996163964271545, CLIP loss: 0.03965182602405548, value function loss: 0.6918618083000183, entropy loss: 0.5621103644371033\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41897428035736084, CLIP loss: 0.0726928859949112, value function loss: 0.703630805015564, entropy loss: 0.5533980131149292\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.379517525434494, CLIP loss: 0.03386131674051285, value function loss: 0.7025527358055115, entropy loss: 0.5620141625404358\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27908140420913696, CLIP loss: -0.05011378601193428, value function loss: 0.669394314289093, entropy loss: 0.5501957535743713\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2900119423866272, CLIP loss: -0.0587242916226387, value function loss: 0.7085193395614624, entropy loss: 0.552344024181366\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.34448590874671936, CLIP loss: -0.06331902742385864, value function loss: 0.8267690539360046, entropy loss: 0.5579591989517212\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26588961482048035, CLIP loss: -0.04359158128499985, value function loss: 0.6299319863319397, entropy loss: 0.5484783053398132\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3518495261669159, CLIP loss: 0.01965036615729332, value function loss: 0.6757612228393555, entropy loss: 0.568145751953125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35034579038619995, CLIP loss: 0.025671500712633133, value function loss: 0.6605130434036255, entropy loss: 0.558225154876709\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3807445168495178, CLIP loss: 0.01449296809732914, value function loss: 0.7438607215881348, entropy loss: 0.5678805112838745\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4014955759048462, CLIP loss: 0.03764999285340309, value function loss: 0.7388544678688049, entropy loss: 0.5581647157669067\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2896426022052765, CLIP loss: -0.08454697579145432, value function loss: 0.7595551013946533, entropy loss: 0.5587972402572632\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27282091975212097, CLIP loss: -0.11045467853546143, value function loss: 0.7775761485099792, entropy loss: 0.5512468218803406\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26859089732170105, CLIP loss: -0.10415125638246536, value function loss: 0.7565028667449951, entropy loss: 0.5509294867515564\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2847326695919037, CLIP loss: -0.09125271439552307, value function loss: 0.7631346583366394, entropy loss: 0.5581947565078735\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.035557746887207, CLIP loss: 0.3932391107082367, value function loss: 11.295766830444336, entropy loss: 0.5564581155776978\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.8143789768218994, CLIP loss: 0.20600676536560059, value function loss: 5.227653503417969, entropy loss: 0.5454545021057129\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.154346466064453, CLIP loss: 0.34816521406173706, value function loss: 11.623455047607422, entropy loss: 0.5546331405639648\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.5331838130950928, CLIP loss: 0.24154958128929138, value function loss: 4.5942702293396, entropy loss: 0.5500698089599609\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.807953834533691, CLIP loss: 0.878381073474884, value function loss: 29.870126724243164, entropy loss: 0.5490332841873169\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.034269332885742, CLIP loss: 0.7147859930992126, value function loss: 24.650150299072266, entropy loss: 0.5591172575950623\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.480618476867676, CLIP loss: 0.7379064559936523, value function loss: 27.49652862548828, entropy loss: 0.555202841758728\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.780332565307617, CLIP loss: 0.7948311567306519, value function loss: 25.982057571411133, entropy loss: 0.5527147054672241\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1228557899594307, CLIP loss: -0.23540425300598145, value function loss: 0.7273248434066772, entropy loss: 0.5402377247810364\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12913326919078827, CLIP loss: -0.19674867391586304, value function loss: 0.6625847220420837, entropy loss: 0.5410420894622803\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16383309662342072, CLIP loss: -0.22113454341888428, value function loss: 0.7807824015617371, entropy loss: 0.5423555970191956\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15976764261722565, CLIP loss: -0.21242904663085938, value function loss: 0.7551366090774536, entropy loss: 0.5371618270874023\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12057983130216599, CLIP loss: -0.19248060882091522, value function loss: 0.6370880603790283, entropy loss: 0.5483592748641968\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07074498385190964, CLIP loss: -0.2304149568080902, value function loss: 0.6134145855903625, entropy loss: 0.5547350645065308\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11206334084272385, CLIP loss: -0.23884029686450958, value function loss: 0.712756335735321, entropy loss: 0.5474529266357422\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1557905524969101, CLIP loss: -0.1723155528306961, value function loss: 0.6673128008842468, entropy loss: 0.5550289154052734\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17719991505146027, CLIP loss: -0.18221205472946167, value function loss: 0.7300463914871216, entropy loss: 0.561122477054596\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2102402001619339, CLIP loss: -0.13166770339012146, value function loss: 0.6948432922363281, entropy loss: 0.5513737797737122\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16142405569553375, CLIP loss: -0.15006116032600403, value function loss: 0.6341198682785034, entropy loss: 0.557471752166748\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16463202238082886, CLIP loss: -0.1647111475467682, value function loss: 0.6696518659591675, entropy loss: 0.548276960849762\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24812164902687073, CLIP loss: -0.12305624037981033, value function loss: 0.7534828186035156, entropy loss: 0.556352436542511\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2676403820514679, CLIP loss: -0.06435875594615936, value function loss: 0.6751508712768555, entropy loss: 0.5576271414756775\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2534309923648834, CLIP loss: -0.1155267283320427, value function loss: 0.7491276264190674, entropy loss: 0.56060791015625\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3043481409549713, CLIP loss: -0.062278229743242264, value function loss: 0.7442296147346497, entropy loss: 0.5488420128822327\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21740837395191193, CLIP loss: -0.13103540241718292, value function loss: 0.7082663774490356, entropy loss: 0.5689414739608765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12799686193466187, CLIP loss: -0.1979384571313858, value function loss: 0.6630125045776367, entropy loss: 0.5570931434631348\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15809233486652374, CLIP loss: -0.18347126245498657, value function loss: 0.6944301128387451, entropy loss: 0.5651459693908691\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1814979910850525, CLIP loss: -0.14523479342460632, value function loss: 0.6645259857177734, entropy loss: 0.5530203580856323\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.004994330462068319, CLIP loss: -0.3529685139656067, value function loss: 0.7069524526596069, entropy loss: 0.5502042770385742\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12621814012527466, CLIP loss: -0.2156035453081131, value function loss: 0.6950333714485168, entropy loss: 0.5694998502731323\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.05182008817791939, CLIP loss: -0.27081218361854553, value function loss: 0.6563993692398071, entropy loss: 0.5567411780357361\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0336153618991375, CLIP loss: -0.2972447872161865, value function loss: 0.6731021404266357, entropy loss: 0.5690922737121582\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16972477734088898, CLIP loss: -0.1832709014415741, value function loss: 0.7172438502311707, entropy loss: 0.5626245141029358\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19984054565429688, CLIP loss: -0.15242308378219604, value function loss: 0.7155570983886719, entropy loss: 0.5514922142028809\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18155567348003387, CLIP loss: -0.17678199708461761, value function loss: 0.72779780626297, entropy loss: 0.5561236143112183\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17218051850795746, CLIP loss: -0.16273531317710876, value function loss: 0.6809558868408203, entropy loss: 0.5562106370925903\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19509002566337585, CLIP loss: -0.12779320776462555, value function loss: 0.6567868590354919, entropy loss: 0.5510194301605225\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35528460144996643, CLIP loss: -0.044820476323366165, value function loss: 0.8112624883651733, entropy loss: 0.5526152849197388\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2866315245628357, CLIP loss: -0.10663780570030212, value function loss: 0.7973958253860474, entropy loss: 0.5428584218025208\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31020739674568176, CLIP loss: -0.05116276070475578, value function loss: 0.7336962223052979, entropy loss: 0.5477970838546753\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26545822620391846, CLIP loss: -0.0937875360250473, value function loss: 0.7293816804885864, entropy loss: 0.5445060729980469\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28136247396469116, CLIP loss: -0.11240370571613312, value function loss: 0.7984936237335205, entropy loss: 0.5480636954307556\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3245464563369751, CLIP loss: -0.10659858584403992, value function loss: 0.8732166290283203, entropy loss: 0.5463285446166992\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33079519867897034, CLIP loss: -0.10179238021373749, value function loss: 0.8760573863983154, entropy loss: 0.5441105961799622\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25012171268463135, CLIP loss: -0.08415487408638, value function loss: 0.6795470118522644, entropy loss: 0.5496925115585327\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.284734845161438, CLIP loss: -0.03690820932388306, value function loss: 0.6542145013809204, entropy loss: 0.5464203953742981\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3049139082431793, CLIP loss: -0.06763441860675812, value function loss: 0.7559489607810974, entropy loss: 0.5426145195960999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27620092034339905, CLIP loss: -0.07230504602193832, value function loss: 0.7078689336776733, entropy loss: 0.5428478717803955\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4576411843299866, CLIP loss: 0.09374576807022095, value function loss: 0.7386656403541565, entropy loss: 0.5437413454055786\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40537312626838684, CLIP loss: 0.043387897312641144, value function loss: 0.7348697781562805, entropy loss: 0.5449650287628174\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4021804630756378, CLIP loss: 0.0488201305270195, value function loss: 0.7175107598304749, entropy loss: 0.539505660533905\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4407801628112793, CLIP loss: 0.08579075336456299, value function loss: 0.7208178639411926, entropy loss: 0.5419535636901855\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25217103958129883, CLIP loss: -0.05271873623132706, value function loss: 0.6207868456840515, entropy loss: 0.5503650903701782\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2984110116958618, CLIP loss: -0.023795025423169136, value function loss: 0.6553529500961304, entropy loss: 0.5470430850982666\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.38056129217147827, CLIP loss: 0.028211241587996483, value function loss: 0.7157119512557983, entropy loss: 0.5505925416946411\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20500950515270233, CLIP loss: -0.11295849084854126, value function loss: 0.6468563675880432, entropy loss: 0.5460186004638672\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2549232840538025, CLIP loss: -0.08246352523565292, value function loss: 0.6855531334877014, entropy loss: 0.5389754176139832\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3315693140029907, CLIP loss: 0.008500248193740845, value function loss: 0.6570205688476562, entropy loss: 0.5441210269927979\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26943662762641907, CLIP loss: -0.051922183483839035, value function loss: 0.6535581350326538, entropy loss: 0.5420257449150085\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2831016480922699, CLIP loss: -0.03179797902703285, value function loss: 0.6406341791152954, entropy loss: 0.541746199131012\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26665911078453064, CLIP loss: -0.056017015129327774, value function loss: 0.6563234329223633, entropy loss: 0.5485602021217346\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28121137619018555, CLIP loss: -0.024565303698182106, value function loss: 0.6225710511207581, entropy loss: 0.5508826375007629\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2823602259159088, CLIP loss: -0.02705306187272072, value function loss: 0.6297715306282043, entropy loss: 0.5472479462623596\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3157482445240021, CLIP loss: -0.04074867069721222, value function loss: 0.723996102809906, entropy loss: 0.550113320350647\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2513716518878937, CLIP loss: -0.05588916316628456, value function loss: 0.6253852844238281, entropy loss: 0.543182373046875\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3219795525074005, CLIP loss: 0.017941027879714966, value function loss: 0.6188657879829407, entropy loss: 0.5394370555877686\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.273014634847641, CLIP loss: -0.02429896965622902, value function loss: 0.6054378151893616, entropy loss: 0.5405292510986328\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3389567732810974, CLIP loss: -0.011790106073021889, value function loss: 0.7123282551765442, entropy loss: 0.5417264699935913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.37603452801704407, CLIP loss: 0.003348606638610363, value function loss: 0.7566673755645752, entropy loss: 0.5647791028022766\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23910976946353912, CLIP loss: -0.024910928681492805, value function loss: 0.5392034649848938, entropy loss: 0.5581031441688538\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2527894675731659, CLIP loss: -0.01479150727391243, value function loss: 0.5462074279785156, entropy loss: 0.5522732734680176\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26551511883735657, CLIP loss: 0.0021994542330503464, value function loss: 0.5377550721168518, entropy loss: 0.5561872124671936\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3363780975341797, CLIP loss: 0.019544919952750206, value function loss: 0.6447423696517944, entropy loss: 0.5538027286529541\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3096218705177307, CLIP loss: 0.015578560531139374, value function loss: 0.5993187427520752, entropy loss: 0.5616074800491333\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3526255786418915, CLIP loss: 0.05216875672340393, value function loss: 0.6120365858078003, entropy loss: 0.5561472177505493\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2777526080608368, CLIP loss: -0.01027810387313366, value function loss: 0.5871813297271729, entropy loss: 0.5559941530227661\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.260292112827301, CLIP loss: -0.029570497572422028, value function loss: 0.590871274471283, entropy loss: 0.5573043823242188\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2665017247200012, CLIP loss: -0.04961149021983147, value function loss: 0.6434404850006104, entropy loss: 0.5607030391693115\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22126489877700806, CLIP loss: -0.0536460243165493, value function loss: 0.5609068274497986, entropy loss: 0.5542493462562561\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28055837750434875, CLIP loss: -0.037280675023794174, value function loss: 0.6468779444694519, entropy loss: 0.5599908828735352\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27836287021636963, CLIP loss: -0.011434696614742279, value function loss: 0.5907297134399414, entropy loss: 0.5567297339439392\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.196883887052536, CLIP loss: -0.05758510157465935, value function loss: 0.5198144912719727, entropy loss: 0.5438258647918701\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31315216422080994, CLIP loss: -0.022552430629730225, value function loss: 0.6821945309638977, entropy loss: 0.5392678380012512\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25464069843292236, CLIP loss: -0.039534058421850204, value function loss: 0.5994916558265686, entropy loss: 0.5571074485778809\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3520936071872711, CLIP loss: -0.02647513709962368, value function loss: 0.7682226896286011, entropy loss: 0.5542619824409485\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3370239734649658, CLIP loss: -0.05069383233785629, value function loss: 0.7863278388977051, entropy loss: 0.5446105599403381\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3774346113204956, CLIP loss: -0.007608350366353989, value function loss: 0.7810434699058533, entropy loss: 0.5478777289390564\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2822757065296173, CLIP loss: -0.06632775813341141, value function loss: 0.7082784175872803, entropy loss: 0.5535742044448853\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.382475882768631, CLIP loss: 0.0009552566334605217, value function loss: 0.7740916013717651, entropy loss: 0.5525175929069519\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.41050800681114197, CLIP loss: -0.021469123661518097, value function loss: 0.8746646046638489, entropy loss: 0.5355186462402344\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3420298993587494, CLIP loss: -0.012828117236495018, value function loss: 0.720562756061554, entropy loss: 0.5423375964164734\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35183146595954895, CLIP loss: 0.00480649434030056, value function loss: 0.7049275040626526, entropy loss: 0.5438789129257202\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3175273537635803, CLIP loss: -0.0031151166185736656, value function loss: 0.6524316668510437, entropy loss: 0.5573349595069885\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.46092844009399414, CLIP loss: 0.08569328486919403, value function loss: 0.7615896463394165, entropy loss: 0.5559683442115784\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.46301665902137756, CLIP loss: 0.07150740921497345, value function loss: 0.7940599322319031, entropy loss: 0.5520723462104797\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3794698119163513, CLIP loss: 0.016270454972982407, value function loss: 0.7372802495956421, entropy loss: 0.5440764427185059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2614985704421997, CLIP loss: -0.05330280214548111, value function loss: 0.6406952142715454, entropy loss: 0.5546244382858276\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.317421555519104, CLIP loss: -0.050366342067718506, value function loss: 0.7466550469398499, entropy loss: 0.5539631843566895\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3343830704689026, CLIP loss: -0.05069095641374588, value function loss: 0.7811424732208252, entropy loss: 0.54972243309021\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2704841196537018, CLIP loss: -0.053844962269067764, value function loss: 0.6596548557281494, entropy loss: 0.5498363375663757\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.913693428039551, CLIP loss: 0.45612841844558716, value function loss: 12.926033020019531, entropy loss: 0.545159637928009\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.33897876739502, CLIP loss: 0.8338294625282288, value function loss: 27.021514892578125, entropy loss: 0.5608611702919006\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.030827522277832, CLIP loss: 0.7079355716705322, value function loss: 22.656938552856445, entropy loss: 0.5577504634857178\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.652284622192383, CLIP loss: 0.5124052166938782, value function loss: 16.29075813293457, entropy loss: 0.5500141978263855\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16728176176548004, CLIP loss: -0.14523297548294067, value function loss: 0.6363154053688049, entropy loss: 0.5642959475517273\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15467706322669983, CLIP loss: -0.19738143682479858, value function loss: 0.715137243270874, entropy loss: 0.5510119199752808\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12234189361333847, CLIP loss: -0.18959753215312958, value function loss: 0.6349607706069946, entropy loss: 0.5540959239006042\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16832493245601654, CLIP loss: -0.16081270575523376, value function loss: 0.6693649888038635, entropy loss: 0.554485023021698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16950702667236328, CLIP loss: -0.09578333795070648, value function loss: 0.5415251851081848, entropy loss: 0.5472232103347778\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2508082091808319, CLIP loss: -0.0680345967411995, value function loss: 0.6484901905059814, entropy loss: 0.540228545665741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21852430701255798, CLIP loss: -0.1068926677107811, value function loss: 0.6616327166557312, entropy loss: 0.5399371385574341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2217530757188797, CLIP loss: -0.06331127136945724, value function loss: 0.5809364914894104, entropy loss: 0.5403898358345032\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27028873562812805, CLIP loss: -0.01642189361155033, value function loss: 0.5844573378562927, entropy loss: 0.5518062114715576\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2707225978374481, CLIP loss: -0.01633983850479126, value function loss: 0.5848239660263062, entropy loss: 0.5349539518356323\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28249233961105347, CLIP loss: -0.038648948073387146, value function loss: 0.653236985206604, entropy loss: 0.5477190613746643\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3270037770271301, CLIP loss: 0.008666357025504112, value function loss: 0.6474550366401672, entropy loss: 0.5390120148658752\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26259028911590576, CLIP loss: -0.05279625207185745, value function loss: 0.6417009830474854, entropy loss: 0.5463950634002686\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21571339666843414, CLIP loss: -0.11196678131818771, value function loss: 0.6661738753318787, entropy loss: 0.5406755805015564\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2666719853878021, CLIP loss: -0.053170811384916306, value function loss: 0.6505441069602966, entropy loss: 0.5429258346557617\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2366778701543808, CLIP loss: -0.1121741533279419, value function loss: 0.7085253000259399, entropy loss: 0.5410626530647278\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22832389175891876, CLIP loss: -0.09605877101421356, value function loss: 0.659956693649292, entropy loss: 0.5595691204071045\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24717062711715698, CLIP loss: -0.013835553079843521, value function loss: 0.5332404375076294, entropy loss: 0.5614038109779358\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23785454034805298, CLIP loss: -0.08609043806791306, value function loss: 0.6589188575744629, entropy loss: 0.5514443516731262\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.30955126881599426, CLIP loss: -0.02242131158709526, value function loss: 0.6751658916473389, entropy loss: 0.5610383749008179\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2154710292816162, CLIP loss: -0.05369292199611664, value function loss: 0.5494387745857239, entropy loss: 0.5555442571640015\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16246247291564941, CLIP loss: -0.09647925943136215, value function loss: 0.5288978815078735, entropy loss: 0.5507208108901978\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16934239864349365, CLIP loss: -0.10901183634996414, value function loss: 0.5678276419639587, entropy loss: 0.5559593439102173\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2390710562467575, CLIP loss: -0.04086664691567421, value function loss: 0.5709069967269897, entropy loss: 0.5515801310539246\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.30224716663360596, CLIP loss: 0.02752913162112236, value function loss: 0.5605153441429138, entropy loss: 0.553961992263794\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28468671441078186, CLIP loss: 0.009141424670815468, value function loss: 0.5620809197425842, entropy loss: 0.5495162606239319\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3567795157432556, CLIP loss: 0.026718486100435257, value function loss: 0.6708675622940063, entropy loss: 0.5372768640518188\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3434152901172638, CLIP loss: 0.023494722321629524, value function loss: 0.6507288217544556, entropy loss: 0.5443850159645081\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2278323769569397, CLIP loss: -0.09195752441883087, value function loss: 0.6503249406814575, entropy loss: 0.5372576117515564\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22582438588142395, CLIP loss: -0.10172917693853378, value function loss: 0.6657304763793945, entropy loss: 0.5311667323112488\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.36420321464538574, CLIP loss: -0.07775332778692245, value function loss: 0.894597589969635, entropy loss: 0.5342250466346741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26177939772605896, CLIP loss: -0.11919288337230682, value function loss: 0.7726684212684631, entropy loss: 0.5361955165863037\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3187023401260376, CLIP loss: 0.03337005898356438, value function loss: 0.5818076133728027, entropy loss: 0.5571516752243042\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4140985608100891, CLIP loss: 0.07416298985481262, value function loss: 0.6909939050674438, entropy loss: 0.5561366677284241\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3327154815196991, CLIP loss: 0.018179375678300858, value function loss: 0.6400003433227539, entropy loss: 0.546408474445343\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4375908076763153, CLIP loss: 0.07077819854021072, value function loss: 0.7448141574859619, entropy loss: 0.5594470500946045\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2270282357931137, CLIP loss: -0.049415115267038345, value function loss: 0.5638847351074219, entropy loss: 0.5499018430709839\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.257919579744339, CLIP loss: -0.07753223180770874, value function loss: 0.6818238496780396, entropy loss: 0.5460118651390076\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35462242364883423, CLIP loss: -0.04736730456352234, value function loss: 0.8148375749588013, entropy loss: 0.5429044961929321\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3005072772502899, CLIP loss: -0.08281385898590088, value function loss: 0.7775062918663025, entropy loss: 0.5432015061378479\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2874831557273865, CLIP loss: 0.013068334199488163, value function loss: 0.5599531531333923, entropy loss: 0.556176483631134\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24239414930343628, CLIP loss: -0.04549970105290413, value function loss: 0.5868963599205017, entropy loss: 0.5554336905479431\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28190240263938904, CLIP loss: -0.010641351342201233, value function loss: 0.5960820913314819, entropy loss: 0.5497313737869263\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28209739923477173, CLIP loss: -0.024832095950841904, value function loss: 0.6251071095466614, entropy loss: 0.562406063079834\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.34163156151771545, CLIP loss: 0.024973265826702118, value function loss: 0.6444432139396667, entropy loss: 0.5563332438468933\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27994653582572937, CLIP loss: -0.036667950451374054, value function loss: 0.6442978382110596, entropy loss: 0.5534448027610779\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3366444408893585, CLIP loss: -0.012400876730680466, value function loss: 0.7092003226280212, entropy loss: 0.5554866790771484\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2851810157299042, CLIP loss: -0.01367982942610979, value function loss: 0.6087911128997803, entropy loss: 0.5534710884094238\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3225105106830597, CLIP loss: 0.029168523848056793, value function loss: 0.5974169373512268, entropy loss: 0.5366466045379639\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.364438533782959, CLIP loss: 0.08045798540115356, value function loss: 0.5790013670921326, entropy loss: 0.55201256275177\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.29423725605010986, CLIP loss: 0.011940007098019123, value function loss: 0.57546067237854, entropy loss: 0.5433090925216675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40628737211227417, CLIP loss: 0.09103633463382721, value function loss: 0.6414951682090759, entropy loss: 0.5496566891670227\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.47398775815963745, CLIP loss: 0.15115073323249817, value function loss: 0.6568976044654846, entropy loss: 0.5611789226531982\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3859315514564514, CLIP loss: 0.10805582255125046, value function loss: 0.5667450428009033, entropy loss: 0.5496793985366821\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.42531347274780273, CLIP loss: 0.13655324280261993, value function loss: 0.5885863900184631, entropy loss: 0.5532981157302856\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.432545006275177, CLIP loss: 0.13003632426261902, value function loss: 0.6161856651306152, entropy loss: 0.5584152936935425\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.39204198122024536, CLIP loss: 0.08738753199577332, value function loss: 0.6202924251556396, entropy loss: 0.5491749048233032\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4191431999206543, CLIP loss: 0.12986552715301514, value function loss: 0.5896267890930176, entropy loss: 0.553572416305542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.43513408303260803, CLIP loss: 0.08859353512525558, value function loss: 0.7039604783058167, entropy loss: 0.5439697504043579\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.40563949942588806, CLIP loss: 0.11749091744422913, value function loss: 0.5872573852539062, entropy loss: 0.5480109453201294\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3829471468925476, CLIP loss: 0.06749211996793747, value function loss: 0.6418460011482239, entropy loss: 0.5467976331710815\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3330005407333374, CLIP loss: 0.022842518985271454, value function loss: 0.6313731074333191, entropy loss: 0.5528551340103149\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3204902112483978, CLIP loss: 0.005179887637495995, value function loss: 0.6415560245513916, entropy loss: 0.5467669367790222\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.36918601393699646, CLIP loss: 0.0729481428861618, value function loss: 0.6034514904022217, entropy loss: 0.5487884879112244\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3519432842731476, CLIP loss: 0.05736691877245903, value function loss: 0.5999523997306824, entropy loss: 0.5399821400642395\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28334152698516846, CLIP loss: 0.015568927861750126, value function loss: 0.5463801622390747, entropy loss: 0.5417493581771851\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3387715220451355, CLIP loss: 0.013386460021138191, value function loss: 0.6615346670150757, entropy loss: 0.5382275581359863\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3527826964855194, CLIP loss: 0.06505759805440903, value function loss: 0.586297869682312, entropy loss: 0.5423840284347534\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3455277681350708, CLIP loss: 0.06890888512134552, value function loss: 0.5641989707946777, entropy loss: 0.5480574369430542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33099377155303955, CLIP loss: 0.056629639118909836, value function loss: 0.5596357583999634, entropy loss: 0.5453731417655945\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3823036849498749, CLIP loss: 0.0637827217578888, value function loss: 0.6479789018630981, entropy loss: 0.5468493700027466\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3512558043003082, CLIP loss: 0.05186662822961807, value function loss: 0.6097787618637085, entropy loss: 0.5500197410583496\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4835493862628937, CLIP loss: 0.18963761627674103, value function loss: 0.5987905263900757, entropy loss: 0.5483479499816895\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.4730161130428314, CLIP loss: 0.16594043374061584, value function loss: 0.6251881718635559, entropy loss: 0.5518416166305542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.6047545075416565, CLIP loss: 0.16629192233085632, value function loss: 0.8879122734069824, entropy loss: 0.5493582487106323\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.5682218074798584, CLIP loss: 0.18371500074863434, value function loss: 0.7800134420394897, entropy loss: 0.5499918460845947\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.39030951261520386, CLIP loss: 0.11075951159000397, value function loss: 0.5699912905693054, entropy loss: 0.544567346572876\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.38692834973335266, CLIP loss: 0.0813891664147377, value function loss: 0.6220428943634033, entropy loss: 0.5482257604598999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.333264023065567, CLIP loss: 0.025103792548179626, value function loss: 0.6272402405738831, entropy loss: 0.5459867715835571\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.47435927391052246, CLIP loss: 0.152305468916893, value function loss: 0.6551105976104736, entropy loss: 0.5501503944396973\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.551138877868652, CLIP loss: 0.5054630041122437, value function loss: 22.102453231811523, entropy loss: 0.55504310131073\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.837145805358887, CLIP loss: 0.2321053147315979, value function loss: 9.221182823181152, entropy loss: 0.5550627708435059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.837799549102783, CLIP loss: 0.28619691729545593, value function loss: 11.114178657531738, entropy loss: 0.5486673712730408\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.379685401916504, CLIP loss: 0.5021443963050842, value function loss: 19.766355514526367, entropy loss: 0.563698410987854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17468968033790588, CLIP loss: -0.08278987556695938, value function loss: 0.5260878801345825, entropy loss: 0.5564390420913696\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22627988457679749, CLIP loss: -0.012843718752264977, value function loss: 0.4893273115158081, entropy loss: 0.5540063977241516\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17658209800720215, CLIP loss: -0.07420230656862259, value function loss: 0.5125569105148315, entropy loss: 0.5494052171707153\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22989585995674133, CLIP loss: -0.030612437054514885, value function loss: 0.532097578048706, entropy loss: 0.5540497303009033\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20005513727664948, CLIP loss: -0.048643916845321655, value function loss: 0.5085786581039429, entropy loss: 0.5590269565582275\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25878745317459106, CLIP loss: -0.02449565939605236, value function loss: 0.5777033567428589, entropy loss: 0.556856632232666\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.284284770488739, CLIP loss: -0.035830676555633545, value function loss: 0.6514688730239868, entropy loss: 0.561898410320282\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28313037753105164, CLIP loss: -0.02577665075659752, value function loss: 0.6288874745368958, entropy loss: 0.5536719560623169\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1657116860151291, CLIP loss: -0.07612843066453934, value function loss: 0.4948102831840515, entropy loss: 0.5565036535263062\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20116695761680603, CLIP loss: -0.0684162974357605, value function loss: 0.5501707792282104, entropy loss: 0.5502128601074219\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15051990747451782, CLIP loss: -0.08121295273303986, value function loss: 0.47451063990592957, entropy loss: 0.5522459149360657\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16003604233264923, CLIP loss: -0.08200432360172272, value function loss: 0.49517175555229187, entropy loss: 0.5545516610145569\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.596799850463867, CLIP loss: 0.8644880652427673, value function loss: 21.475522994995117, entropy loss: 0.5449427366256714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.573843002319336, CLIP loss: 0.8245144486427307, value function loss: 19.50962257385254, entropy loss: 0.5482480525970459\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.550298690795898, CLIP loss: 0.7393528819084167, value function loss: 17.6328182220459, entropy loss: 0.546333909034729\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.064103126525879, CLIP loss: 0.764017641544342, value function loss: 22.611167907714844, entropy loss: 0.5499112010002136\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.179846286773682, CLIP loss: 0.0531463623046875, value function loss: 12.264387130737305, entropy loss: 0.549344539642334\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.9148509502410889, CLIP loss: -0.0956399068236351, value function loss: 4.032140731811523, entropy loss: 0.5579445362091064\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.011544227600098, CLIP loss: 0.1095140278339386, value function loss: 11.8150634765625, entropy loss: 0.5501624345779419\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.0924434661865234, CLIP loss: -0.1339341253042221, value function loss: 4.46356201171875, entropy loss: 0.5403533577919006\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.347949981689453, CLIP loss: 1.8335541486740112, value function loss: 15.039702415466309, entropy loss: 0.5455011129379272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.625360488891602, CLIP loss: 0.7225395441055298, value function loss: 25.81643295288086, entropy loss: 0.5395979881286621\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.665750503540039, CLIP loss: 0.8370621204376221, value function loss: 19.66827964782715, entropy loss: 0.5451125502586365\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.210278511047363, CLIP loss: 0.5781436562538147, value function loss: 21.274988174438477, entropy loss: 0.5360091924667358\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20287156105041504, CLIP loss: -0.10915544629096985, value function loss: 0.635059118270874, entropy loss: 0.5502545833587646\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19902540743350983, CLIP loss: -0.11322927474975586, value function loss: 0.6355218291282654, entropy loss: 0.5506235361099243\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19086921215057373, CLIP loss: -0.11014065891504288, value function loss: 0.6130748987197876, entropy loss: 0.5527580976486206\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18764758110046387, CLIP loss: -0.12018509209156036, value function loss: 0.626610517501831, entropy loss: 0.5472580790519714\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1630236953496933, CLIP loss: -0.13720624148845673, value function loss: 0.6117805242538452, entropy loss: 0.5660320520401001\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19832319021224976, CLIP loss: -0.05899637192487717, value function loss: 0.5259661674499512, entropy loss: 0.5663520693778992\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15577471256256104, CLIP loss: -0.10233993083238602, value function loss: 0.5276404023170471, entropy loss: 0.5705560445785522\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1310691237449646, CLIP loss: -0.09789812564849854, value function loss: 0.4691200256347656, entropy loss: 0.5592761039733887\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15257413685321808, CLIP loss: -0.13714951276779175, value function loss: 0.5902689099311829, entropy loss: 0.5410804152488708\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.05923937261104584, CLIP loss: -0.1891857236623764, value function loss: 0.5079604387283325, entropy loss: 0.5555124878883362\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0992853045463562, CLIP loss: -0.1818738877773285, value function loss: 0.5731849670410156, entropy loss: 0.5433293581008911\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11848130822181702, CLIP loss: -0.16182322800159454, value function loss: 0.571742057800293, entropy loss: 0.5566494464874268\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13005903363227844, CLIP loss: -0.11467894911766052, value function loss: 0.5003287196159363, entropy loss: 0.542637050151825\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1346203237771988, CLIP loss: -0.15041498839855194, value function loss: 0.5810016393661499, entropy loss: 0.5465501546859741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14514906704425812, CLIP loss: -0.11545983701944351, value function loss: 0.5320882797241211, entropy loss: 0.5435237884521484\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09058517217636108, CLIP loss: -0.14862260222434998, value function loss: 0.489128977060318, entropy loss: 0.5356713533401489\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12875758111476898, CLIP loss: -0.09325724840164185, value function loss: 0.4553593099117279, entropy loss: 0.5664824843406677\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1910325437784195, CLIP loss: -0.056552704423666, value function loss: 0.5063668489456177, entropy loss: 0.5598176121711731\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23421400785446167, CLIP loss: -0.055304188281297684, value function loss: 0.5902776122093201, entropy loss: 0.5620613098144531\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14463748037815094, CLIP loss: -0.09827376902103424, value function loss: 0.49707403779029846, entropy loss: 0.562576413154602\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19266240298748016, CLIP loss: -0.10544551908969879, value function loss: 0.6073334217071533, entropy loss: 0.5558783411979675\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.165418341755867, CLIP loss: -0.08554316312074661, value function loss: 0.512910783290863, entropy loss: 0.5493896007537842\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14754314720630646, CLIP loss: -0.126402348279953, value function loss: 0.5587716102600098, entropy loss: 0.5440307259559631\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2468608170747757, CLIP loss: -0.06529820710420609, value function loss: 0.6354811191558838, entropy loss: 0.5581547021865845\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13765400648117065, CLIP loss: -0.1157858818769455, value function loss: 0.5180509090423584, entropy loss: 0.5585572719573975\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10784139484167099, CLIP loss: -0.1508985161781311, value function loss: 0.5286627411842346, entropy loss: 0.5591462254524231\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07224420458078384, CLIP loss: -0.16195857524871826, value function loss: 0.479589581489563, entropy loss: 0.5592013597488403\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14893686771392822, CLIP loss: -0.11308836936950684, value function loss: 0.5351743102073669, entropy loss: 0.5561912059783936\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.255111426115036, CLIP loss: -0.08865643292665482, value function loss: 0.6983945369720459, entropy loss: 0.5429428815841675\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25833484530448914, CLIP loss: -0.0466160848736763, value function loss: 0.6207138299942017, entropy loss: 0.5405999422073364\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19581329822540283, CLIP loss: -0.08829329907894135, value function loss: 0.5789250731468201, entropy loss: 0.5355936884880066\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24865983426570892, CLIP loss: -0.036274220794439316, value function loss: 0.5805718302726746, entropy loss: 0.5351858735084534\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.261297345161438, CLIP loss: -0.008663066662847996, value function loss: 0.5506093502044678, entropy loss: 0.5344281196594238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2381795346736908, CLIP loss: -0.02337023615837097, value function loss: 0.5338900685310364, entropy loss: 0.5395259261131287\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2043628692626953, CLIP loss: -0.047317493706941605, value function loss: 0.5140931606292725, entropy loss: 0.5366215109825134\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2809968888759613, CLIP loss: 0.009188580326735973, value function loss: 0.5542681813240051, entropy loss: 0.5325789451599121\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19381439685821533, CLIP loss: -0.06271721422672272, value function loss: 0.5238732695579529, entropy loss: 0.5405026078224182\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16449427604675293, CLIP loss: -0.07077673077583313, value function loss: 0.48155349493026733, entropy loss: 0.5505746006965637\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17166714370250702, CLIP loss: -0.095374196767807, value function loss: 0.5448455214500427, entropy loss: 0.5381418466567993\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22812913358211517, CLIP loss: -0.03238866105675697, value function loss: 0.5319872498512268, entropy loss: 0.5475832223892212\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10775826126337051, CLIP loss: -0.1586393266916275, value function loss: 0.5435789227485657, entropy loss: 0.5391876101493835\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08537004888057709, CLIP loss: -0.183821439743042, value function loss: 0.5492558479309082, entropy loss: 0.5436438322067261\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10209745913743973, CLIP loss: -0.15497195720672607, value function loss: 0.524964451789856, entropy loss: 0.5412810444831848\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08438953012228012, CLIP loss: -0.1940240114927292, value function loss: 0.5676941871643066, entropy loss: 0.5433549284934998\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24199868738651276, CLIP loss: -0.033742230385541916, value function loss: 0.5625288486480713, entropy loss: 0.5523499250411987\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17483830451965332, CLIP loss: -0.08477821946144104, value function loss: 0.5300753116607666, entropy loss: 0.5421135425567627\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17821896076202393, CLIP loss: -0.06279844045639038, value function loss: 0.4930307865142822, entropy loss: 0.5497989654541016\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21537941694259644, CLIP loss: -0.052404262125492096, value function loss: 0.5464486479759216, entropy loss: 0.5440652370452881\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21651245653629303, CLIP loss: -0.016998648643493652, value function loss: 0.47785666584968567, entropy loss: 0.5417228937149048\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21355301141738892, CLIP loss: -0.06625470519065857, value function loss: 0.57042396068573, entropy loss: 0.5404266715049744\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20417943596839905, CLIP loss: -0.009205752052366734, value function loss: 0.4376051127910614, entropy loss: 0.5417361855506897\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2118241935968399, CLIP loss: -0.0737568661570549, value function loss: 0.5819310545921326, entropy loss: 0.5384457111358643\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12827138602733612, CLIP loss: -0.15083609521389008, value function loss: 0.5691327452659607, entropy loss: 0.5458885431289673\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16282662749290466, CLIP loss: -0.09119340777397156, value function loss: 0.5189931392669678, entropy loss: 0.5476535558700562\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12833106517791748, CLIP loss: -0.12293268740177155, value function loss: 0.5134005546569824, entropy loss: 0.5436522960662842\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17424137890338898, CLIP loss: -0.10927638411521912, value function loss: 0.5780113935470581, entropy loss: 0.5487939119338989\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1932305097579956, CLIP loss: -0.11966978013515472, value function loss: 0.6366804242134094, entropy loss: 0.5439927577972412\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16269440948963165, CLIP loss: -0.08637575060129166, value function loss: 0.5092499256134033, entropy loss: 0.555481493473053\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1802731454372406, CLIP loss: -0.0950457751750946, value function loss: 0.5615490078926086, entropy loss: 0.5455577373504639\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17061352729797363, CLIP loss: -0.1000416949391365, value function loss: 0.5523886680603027, entropy loss: 0.5539119839668274\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24086931347846985, CLIP loss: -0.05756796896457672, value function loss: 0.6078117489814758, entropy loss: 0.5468590259552002\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23386713862419128, CLIP loss: -0.038775477558374405, value function loss: 0.5563784241676331, entropy loss: 0.5546603202819824\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2728845179080963, CLIP loss: -0.050908349454402924, value function loss: 0.6586472988128662, entropy loss: 0.5530783534049988\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1642407923936844, CLIP loss: -0.06405842304229736, value function loss: 0.46741414070129395, entropy loss: 0.540785551071167\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15281398594379425, CLIP loss: -0.08006978780031204, value function loss: 0.47664570808410645, entropy loss: 0.5439094305038452\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10759131610393524, CLIP loss: -0.14888828992843628, value function loss: 0.5239037871360779, entropy loss: 0.5472286939620972\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1429220288991928, CLIP loss: -0.10333172231912613, value function loss: 0.5034695863723755, entropy loss: 0.5481050610542297\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11995144188404083, CLIP loss: -0.131662517786026, value function loss: 0.5139751434326172, entropy loss: 0.5373610854148865\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1325152963399887, CLIP loss: -0.08856279402971268, value function loss: 0.45330560207366943, entropy loss: 0.5574723482131958\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13446353375911713, CLIP loss: -0.10061174631118774, value function loss: 0.48136964440345764, entropy loss: 0.5609548091888428\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11289048194885254, CLIP loss: -0.10964642465114594, value function loss: 0.45633092522621155, entropy loss: 0.5628554821014404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16767548024654388, CLIP loss: -0.07372894883155823, value function loss: 0.49409937858581543, entropy loss: 0.5645267367362976\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20953822135925293, CLIP loss: -0.03215808421373367, value function loss: 0.49431440234184265, entropy loss: 0.5460885763168335\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23257245123386383, CLIP loss: -0.037531640380620956, value function loss: 0.5510977506637573, entropy loss: 0.5444777011871338\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.249735489487648, CLIP loss: -0.059104785323143005, value function loss: 0.6285144090652466, entropy loss: 0.5416914820671082\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2487538605928421, CLIP loss: -0.00804474949836731, value function loss: 0.5246520042419434, entropy loss: 0.5527389645576477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.222637176513672, CLIP loss: 0.7374346256256104, value function loss: 16.981319427490234, entropy loss: 0.5457155704498291\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.076682090759277, CLIP loss: 0.8621704578399658, value function loss: 24.43996810913086, entropy loss: 0.5472187399864197\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.668578147888184, CLIP loss: 0.5781742334365845, value function loss: 16.191646575927734, entropy loss: 0.5419491529464722\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.122359275817871, CLIP loss: 0.9330447316169739, value function loss: 24.389389038085938, entropy loss: 0.5379262566566467\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08812152594327927, CLIP loss: -0.18668211996555328, value function loss: 0.5603882670402527, entropy loss: 0.5390490889549255\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1278156191110611, CLIP loss: -0.15501168370246887, value function loss: 0.5767040252685547, entropy loss: 0.5524705052375793\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08707955479621887, CLIP loss: -0.19428779184818268, value function loss: 0.5737274289131165, entropy loss: 0.5496367812156677\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1398783177137375, CLIP loss: -0.12315187603235245, value function loss: 0.5368688106536865, entropy loss: 0.5404219627380371\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17454546689987183, CLIP loss: -0.09857631474733353, value function loss: 0.5571205615997314, entropy loss: 0.5438501834869385\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1689797043800354, CLIP loss: -0.1282029151916504, value function loss: 0.6052744388580322, entropy loss: 0.5454603433609009\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1698896586894989, CLIP loss: -0.14466731250286102, value function loss: 0.6399616003036499, entropy loss: 0.5423824787139893\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2981744706630707, CLIP loss: -0.08450514823198318, value function loss: 0.776165783405304, entropy loss: 0.540327787399292\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12711581587791443, CLIP loss: -0.1009821742773056, value function loss: 0.4672214388847351, entropy loss: 0.5512735247612\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14705803990364075, CLIP loss: -0.09697489440441132, value function loss: 0.49909529089927673, entropy loss: 0.5514710545539856\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16691817343235016, CLIP loss: -0.08521877229213715, value function loss: 0.5153430104255676, entropy loss: 0.5534564256668091\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11356203258037567, CLIP loss: -0.11795038729906082, value function loss: 0.47394049167633057, entropy loss: 0.5457825064659119\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22544258832931519, CLIP loss: -0.04035847261548042, value function loss: 0.542579174041748, entropy loss: 0.548852264881134\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12642130255699158, CLIP loss: -0.10422251373529434, value function loss: 0.472148060798645, entropy loss: 0.5430219769477844\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.214317187666893, CLIP loss: -0.023553337901830673, value function loss: 0.4866752028465271, entropy loss: 0.5467076897621155\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1412491500377655, CLIP loss: -0.13317468762397766, value function loss: 0.5596950054168701, entropy loss: 0.5423668622970581\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22341987490653992, CLIP loss: -0.02348276972770691, value function loss: 0.5049846768379211, entropy loss: 0.5589697957038879\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23483465611934662, CLIP loss: -0.01500462181866169, value function loss: 0.5107951164245605, entropy loss: 0.555828332901001\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2799151539802551, CLIP loss: 0.011417930945754051, value function loss: 0.5480196475982666, entropy loss: 0.5512591004371643\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2591801881790161, CLIP loss: -0.03942174091935158, value function loss: 0.6085272431373596, entropy loss: 0.5661706924438477\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1531415581703186, CLIP loss: -0.09536051750183105, value function loss: 0.5080602765083313, entropy loss: 0.5528067350387573\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1847817301750183, CLIP loss: -0.06610625982284546, value function loss: 0.5127448439598083, entropy loss: 0.548442542552948\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14796201884746552, CLIP loss: -0.11200731247663498, value function loss: 0.5309692025184631, entropy loss: 0.5515264272689819\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19684624671936035, CLIP loss: -0.04513614624738693, value function loss: 0.4950343370437622, entropy loss: 0.5534775257110596\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25226104259490967, CLIP loss: 0.03415859490633011, value function loss: 0.44724035263061523, entropy loss: 0.5517709255218506\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21210072934627533, CLIP loss: -0.009432815946638584, value function loss: 0.45425134897232056, entropy loss: 0.5592120289802551\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2599203288555145, CLIP loss: 0.025817852467298508, value function loss: 0.47921833395957947, entropy loss: 0.5506705641746521\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23075103759765625, CLIP loss: 0.011873583309352398, value function loss: 0.44877201318740845, entropy loss: 0.5508564710617065\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2203107327222824, CLIP loss: -0.03842359036207199, value function loss: 0.5287975668907166, entropy loss: 0.5664447546005249\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22403797507286072, CLIP loss: -0.005409185774624348, value function loss: 0.4701460301876068, entropy loss: 0.5625857710838318\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18756653368473053, CLIP loss: -0.06280645728111267, value function loss: 0.5121678113937378, entropy loss: 0.5710912942886353\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22620339691638947, CLIP loss: 0.0127934655174613, value function loss: 0.43816784024238586, entropy loss: 0.5673989057540894\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.147028848528862, CLIP loss: -0.05139715597033501, value function loss: 0.40773141384124756, entropy loss: 0.5439693331718445\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2081870287656784, CLIP loss: -0.039441101253032684, value function loss: 0.5061258673667908, entropy loss: 0.5434794425964355\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2582473158836365, CLIP loss: -0.021997777745127678, value function loss: 0.5713279247283936, entropy loss: 0.5418875217437744\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18313592672348022, CLIP loss: -0.06641106307506561, value function loss: 0.5098382830619812, entropy loss: 0.5372151136398315\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17179809510707855, CLIP loss: -0.12469995021820068, value function loss: 0.6037247180938721, entropy loss: 0.5364314317703247\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13792814314365387, CLIP loss: -0.1290775090456009, value function loss: 0.5448222756385803, entropy loss: 0.5405487418174744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10761310905218124, CLIP loss: -0.14434516429901123, value function loss: 0.514574408531189, entropy loss: 0.5328933596611023\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15387727320194244, CLIP loss: -0.1165228933095932, value function loss: 0.5514669418334961, entropy loss: 0.533329963684082\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1512361466884613, CLIP loss: -0.09889239817857742, value function loss: 0.5112665295600891, entropy loss: 0.5504730939865112\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0935131162405014, CLIP loss: -0.10504322499036789, value function loss: 0.4080440402030945, entropy loss: 0.5465676784515381\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15184791386127472, CLIP loss: -0.08077477663755417, value function loss: 0.4762708842754364, entropy loss: 0.5512760877609253\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13684242963790894, CLIP loss: -0.105431467294693, value function loss: 0.49537113308906555, entropy loss: 0.541167676448822\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26906517148017883, CLIP loss: -0.008456025272607803, value function loss: 0.5659898519515991, entropy loss: 0.5473721027374268\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24982184171676636, CLIP loss: 0.006982043385505676, value function loss: 0.4967055916786194, entropy loss: 0.5513013601303101\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22240957617759705, CLIP loss: -0.02702009677886963, value function loss: 0.509932279586792, entropy loss: 0.5536466836929321\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3124852180480957, CLIP loss: 0.03967995196580887, value function loss: 0.5565277934074402, entropy loss: 0.545863687992096\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11785158514976501, CLIP loss: -0.11870081722736359, value function loss: 0.48413515090942383, entropy loss: 0.5515175461769104\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12032664567232132, CLIP loss: -0.0833689421415329, value function loss: 0.4185487926006317, entropy loss: 0.5578809380531311\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09955750405788422, CLIP loss: -0.12161782383918762, value function loss: 0.45337986946105957, entropy loss: 0.5514603853225708\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14923694729804993, CLIP loss: -0.07792239636182785, value function loss: 0.4653388261795044, entropy loss: 0.551005482673645\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18502096831798553, CLIP loss: -0.04718256741762161, value function loss: 0.47498035430908203, entropy loss: 0.528664767742157\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11401759833097458, CLIP loss: -0.12051355838775635, value function loss: 0.47987300157546997, entropy loss: 0.5405341386795044\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1239730566740036, CLIP loss: -0.09987659752368927, value function loss: 0.45840609073638916, entropy loss: 0.5353390574455261\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1622142344713211, CLIP loss: -0.06814618408679962, value function loss: 0.4713907837867737, entropy loss: 0.5334980487823486\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1342863142490387, CLIP loss: -0.10662421584129333, value function loss: 0.4929877519607544, entropy loss: 0.5583345293998718\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1696045994758606, CLIP loss: -0.08168556541204453, value function loss: 0.5135176181793213, entropy loss: 0.5468636751174927\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15114673972129822, CLIP loss: -0.09513596445322037, value function loss: 0.5038017630577087, entropy loss: 0.5618178844451904\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13516364991664886, CLIP loss: -0.0932324156165123, value function loss: 0.4677605628967285, entropy loss: 0.5484203100204468\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2148822844028473, CLIP loss: -0.04775122180581093, value function loss: 0.5362290143966675, entropy loss: 0.5481007695198059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.30207568407058716, CLIP loss: -0.0017140954732894897, value function loss: 0.61846524477005, entropy loss: 0.5442844033241272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22998416423797607, CLIP loss: -0.05262034758925438, value function loss: 0.5761824250221252, entropy loss: 0.5486695766448975\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2573983073234558, CLIP loss: -0.008489444851875305, value function loss: 0.5427062511444092, entropy loss: 0.5465387105941772\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1022215336561203, CLIP loss: -0.1425086408853531, value function loss: 0.5004217028617859, entropy loss: 0.5480674505233765\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16463175415992737, CLIP loss: -0.07947754114866257, value function loss: 0.4993162751197815, entropy loss: 0.5548830628395081\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1479768455028534, CLIP loss: -0.13015687465667725, value function loss: 0.5672746896743774, entropy loss: 0.5503620505332947\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15692023932933807, CLIP loss: -0.0863867998123169, value function loss: 0.49772369861602783, entropy loss: 0.5554803609848022\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21815909445285797, CLIP loss: -0.014033674262464046, value function loss: 0.4752737581729889, entropy loss: 0.5444113612174988\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1721479892730713, CLIP loss: -0.056930527091026306, value function loss: 0.4689690172672272, entropy loss: 0.5406000018119812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21870830655097961, CLIP loss: -0.03966602683067322, value function loss: 0.5275552868843079, entropy loss: 0.5403303503990173\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22445330023765564, CLIP loss: -0.03371432423591614, value function loss: 0.5271943211555481, entropy loss: 0.5429533123970032\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16660629212856293, CLIP loss: -0.08654743432998657, value function loss: 0.517062783241272, entropy loss: 0.537766695022583\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16385892033576965, CLIP loss: -0.10091044008731842, value function loss: 0.5402959585189819, entropy loss: 0.5378625392913818\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1381465494632721, CLIP loss: -0.11336231231689453, value function loss: 0.513752281665802, entropy loss: 0.5367275476455688\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1449146419763565, CLIP loss: -0.07242268323898315, value function loss: 0.445432186126709, entropy loss: 0.5378767848014832\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31444859504699707, CLIP loss: 0.058586932718753815, value function loss: 0.5230507850646973, entropy loss: 0.5663735270500183\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21415013074874878, CLIP loss: 0.002092902548611164, value function loss: 0.43514758348464966, entropy loss: 0.5516557097434998\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2849293053150177, CLIP loss: 0.05925777554512024, value function loss: 0.46247217059135437, entropy loss: 0.5564546585083008\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20910616219043732, CLIP loss: -0.0005776786711066961, value function loss: 0.4305419921875, entropy loss: 0.5587157607078552\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.38331738114356995, CLIP loss: 0.11871806532144547, value function loss: 0.5400909781455994, entropy loss: 0.5446177124977112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2832171618938446, CLIP loss: 0.05057377368211746, value function loss: 0.4761630892753601, entropy loss: 0.5438150763511658\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.35388198494911194, CLIP loss: 0.10177113860845566, value function loss: 0.5151351690292358, entropy loss: 0.5456751585006714\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.30772286653518677, CLIP loss: 0.0695905014872551, value function loss: 0.48714667558670044, entropy loss: 0.5440967679023743\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.285528302192688, CLIP loss: 0.011191491037607193, value function loss: 0.5596216320991516, entropy loss: 0.5473993420600891\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23483379185199738, CLIP loss: 0.007908366620540619, value function loss: 0.4649111330509186, entropy loss: 0.553013026714325\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20686598122119904, CLIP loss: -0.00331312557682395, value function loss: 0.43124109506607056, entropy loss: 0.5441439747810364\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27426424622535706, CLIP loss: 0.012285863980650902, value function loss: 0.5348970890045166, entropy loss: 0.5470157265663147\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24474060535430908, CLIP loss: 0.01957704871892929, value function loss: 0.46142351627349854, entropy loss: 0.5548205971717834\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24258585274219513, CLIP loss: 0.005134162493050098, value function loss: 0.48595333099365234, entropy loss: 0.5524977445602417\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2736581265926361, CLIP loss: 0.04377629607915878, value function loss: 0.47086942195892334, entropy loss: 0.5552879571914673\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21548332273960114, CLIP loss: -0.027920596301555634, value function loss: 0.4976344108581543, entropy loss: 0.5413283109664917\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1738933026790619, CLIP loss: -0.03766518086194992, value function loss: 0.4342460036277771, entropy loss: 0.556451678276062\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18672078847885132, CLIP loss: -0.04172884672880173, value function loss: 0.4678175747394562, entropy loss: 0.5459157824516296\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17039991915225983, CLIP loss: -0.06274088472127914, value function loss: 0.47728314995765686, entropy loss: 0.5500775575637817\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19438864290714264, CLIP loss: -0.023490596562623978, value function loss: 0.44683408737182617, entropy loss: 0.5537804961204529\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2155897468328476, CLIP loss: -0.028307272121310234, value function loss: 0.4987477958202362, entropy loss: 0.5476875305175781\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15781615674495697, CLIP loss: -0.06504836678504944, value function loss: 0.4563600718975067, entropy loss: 0.5315510630607605\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18173766136169434, CLIP loss: -0.057690344750881195, value function loss: 0.489565372467041, entropy loss: 0.5354673266410828\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20716217160224915, CLIP loss: -0.026681844145059586, value function loss: 0.47846537828445435, entropy loss: 0.5388681292533875\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15670372545719147, CLIP loss: -0.0257490836083889, value function loss: 0.3762316405773163, entropy loss: 0.5663009881973267\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15802466869354248, CLIP loss: -0.06503093242645264, value function loss: 0.45741790533065796, entropy loss: 0.5653352737426758\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14455284178256989, CLIP loss: -0.0649186447262764, value function loss: 0.4302557110786438, entropy loss: 0.5656377673149109\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1756693571805954, CLIP loss: -0.034032855182886124, value function loss: 0.43068304657936096, entropy loss: 0.5639316439628601\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.542365550994873, CLIP loss: 0.32841911911964417, value function loss: 14.438986778259277, entropy loss: 0.5546877384185791\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.373357772827148, CLIP loss: 0.5632655024528503, value function loss: 23.631080627441406, entropy loss: 0.5448580980300903\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.218021869659424, CLIP loss: 0.2224091738462448, value function loss: 10.002163887023926, entropy loss: 0.5469273328781128\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.620506286621094, CLIP loss: 0.6772735714912415, value function loss: 27.897445678710938, entropy loss: 0.5490217208862305\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10287480056285858, CLIP loss: -0.13041824102401733, value function loss: 0.4773939251899719, entropy loss: 0.5403919816017151\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08938522636890411, CLIP loss: -0.1244579404592514, value function loss: 0.4385770857334137, entropy loss: 0.5445376038551331\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12407473474740982, CLIP loss: -0.10729557275772095, value function loss: 0.47362664341926575, entropy loss: 0.5443017482757568\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08675286173820496, CLIP loss: -0.14367294311523438, value function loss: 0.47168105840682983, entropy loss: 0.5414721965789795\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.03724801167845726, CLIP loss: -0.22249315679073334, value function loss: 0.5304257869720459, entropy loss: 0.5471723675727844\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.049207136034965515, CLIP loss: -0.17099446058273315, value function loss: 0.45142653584480286, entropy loss: 0.5511671900749207\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.021945539861917496, CLIP loss: -0.1927402764558792, value function loss: 0.44029369950294495, entropy loss: 0.5461034178733826\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04041865095496178, CLIP loss: -0.19730357825756073, value function loss: 0.48651447892189026, entropy loss: 0.5535010099411011\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1422698199748993, CLIP loss: -0.11216527223587036, value function loss: 0.5194563865661621, entropy loss: 0.5293093919754028\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20885060727596283, CLIP loss: -0.06118989735841751, value function loss: 0.5507009625434875, entropy loss: 0.5309988260269165\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19628965854644775, CLIP loss: -0.06523353606462479, value function loss: 0.5336182713508606, entropy loss: 0.5285947918891907\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2165939211845398, CLIP loss: -0.09293617308139801, value function loss: 0.6298220753669739, entropy loss: 0.5380944609642029\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14601515233516693, CLIP loss: -0.08338168263435364, value function loss: 0.46983999013900757, entropy loss: 0.5523165464401245\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2088720053434372, CLIP loss: -0.04737216234207153, value function loss: 0.5233862996101379, entropy loss: 0.5448986291885376\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15855680406093597, CLIP loss: -0.07546649873256683, value function loss: 0.47901207208633423, entropy loss: 0.5482738018035889\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17937137186527252, CLIP loss: -0.05825609713792801, value function loss: 0.48625004291534424, entropy loss: 0.549755871295929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12078201770782471, CLIP loss: -0.14324037730693817, value function loss: 0.5392515659332275, entropy loss: 0.5603391528129578\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.01691386103630066, CLIP loss: -0.22638101875782013, value function loss: 0.43011474609375, entropy loss: 0.5590215921401978\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04324973374605179, CLIP loss: -0.17100213468074799, value function loss: 0.4396243691444397, entropy loss: 0.5560315847396851\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.019325731322169304, CLIP loss: -0.20237240195274353, value function loss: 0.4545637369155884, entropy loss: 0.5583735108375549\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 12.439684867858887, CLIP loss: 0.6060082912445068, value function loss: 23.67843246459961, entropy loss: 0.5539476871490479\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.021406173706055, CLIP loss: 0.42123788595199585, value function loss: 15.211471557617188, entropy loss: 0.5567400455474854\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.196154594421387, CLIP loss: 0.5506623983383179, value function loss: 21.301990509033203, entropy loss: 0.5502307415008545\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.038431167602539, CLIP loss: 0.4804817736148834, value function loss: 17.126968383789062, entropy loss: 0.5534877777099609\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.029245974496006966, CLIP loss: -0.2982728183269501, value function loss: 0.5487362742424011, entropy loss: 0.5341293215751648\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.06013815850019455, CLIP loss: -0.30668413639068604, value function loss: 0.5038727521896362, entropy loss: 0.5390397310256958\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.06968865543603897, CLIP loss: -0.30507028102874756, value function loss: 0.48168110847473145, entropy loss: 0.5458931922912598\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.04977216199040413, CLIP loss: -0.28462985157966614, value function loss: 0.4804363250732422, entropy loss: 0.5360473394393921\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.014433443546295166, CLIP loss: -0.2259967029094696, value function loss: 0.49189820885658264, entropy loss: 0.5518958568572998\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1184612438082695, CLIP loss: -0.1495494246482849, value function loss: 0.547204852104187, entropy loss: 0.559175431728363\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.05846133455634117, CLIP loss: -0.1876603662967682, value function loss: 0.5034137964248657, entropy loss: 0.5585197806358337\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07151035219430923, CLIP loss: -0.16884355247020721, value function loss: 0.4917764663696289, entropy loss: 0.5534331202507019\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.01704920083284378, CLIP loss: -0.20792925357818604, value function loss: 0.46109166741371155, entropy loss: 0.556738018989563\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0704335868358612, CLIP loss: -0.19660665094852448, value function loss: 0.5451215505599976, entropy loss: 0.5520541071891785\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.01077268272638321, CLIP loss: -0.2185765653848648, value function loss: 0.46973708271980286, entropy loss: 0.5519293546676636\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.027949539944529533, CLIP loss: -0.19066712260246277, value function loss: 0.4483341872692108, entropy loss: 0.5550431609153748\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09823146462440491, CLIP loss: -0.1275329440832138, value function loss: 0.46224966645240784, entropy loss: 0.5360423922538757\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13817903399467468, CLIP loss: -0.08974505215883255, value function loss: 0.4667588174343109, entropy loss: 0.5455307960510254\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10734736174345016, CLIP loss: -0.1169668585062027, value function loss: 0.45922398567199707, entropy loss: 0.5297775864601135\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16125601530075073, CLIP loss: -0.09586365520954132, value function loss: 0.5251879692077637, entropy loss: 0.5474309921264648\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11367129534482956, CLIP loss: -0.14134865999221802, value function loss: 0.5211228132247925, entropy loss: 0.5541453957557678\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11629627645015717, CLIP loss: -0.11601544916629791, value function loss: 0.4757768511772156, entropy loss: 0.5576696395874023\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15317533910274506, CLIP loss: -0.08700738847255707, value function loss: 0.4914523959159851, entropy loss: 0.5543465614318848\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09495480358600616, CLIP loss: -0.14050889015197754, value function loss: 0.48212283849716187, entropy loss: 0.5597723722457886\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13077455759048462, CLIP loss: -0.11582975834608078, value function loss: 0.503824770450592, entropy loss: 0.5308068990707397\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14535647630691528, CLIP loss: -0.08861590176820755, value function loss: 0.47868219017982483, entropy loss: 0.5368702411651611\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12917551398277283, CLIP loss: -0.09785771369934082, value function loss: 0.4647877812385559, entropy loss: 0.5360661149024963\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1234031543135643, CLIP loss: -0.10430701822042465, value function loss: 0.4661048650741577, entropy loss: 0.5342250466346741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.276839256286621, CLIP loss: 0.5269057154655457, value function loss: 15.510900497436523, entropy loss: 0.5516573190689087\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.096714973449707, CLIP loss: 0.2056044489145279, value function loss: 11.793264389038086, entropy loss: 0.5521946549415588\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.005256175994873, CLIP loss: 0.24553930759429932, value function loss: 9.530409812927246, entropy loss: 0.54878830909729\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.12907886505127, CLIP loss: 0.36787256598472595, value function loss: 17.533456802368164, entropy loss: 0.5522117018699646\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17496095597743988, CLIP loss: -0.0614268034696579, value function loss: 0.4839295446872711, entropy loss: 0.5577008128166199\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1620306521654129, CLIP loss: -0.09460081905126572, value function loss: 0.5242940783500671, entropy loss: 0.551555871963501\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08405908942222595, CLIP loss: -0.11913368105888367, value function loss: 0.41739800572395325, entropy loss: 0.5506231188774109\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1793672740459442, CLIP loss: -0.03954438120126724, value function loss: 0.44883692264556885, entropy loss: 0.5506807565689087\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04540882632136345, CLIP loss: -0.18698301911354065, value function loss: 0.47567206621170044, entropy loss: 0.5444188117980957\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.016190068796277046, CLIP loss: -0.21689918637275696, value function loss: 0.4770672023296356, entropy loss: 0.544434666633606\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.013119093142449856, CLIP loss: -0.2057272493839264, value function loss: 0.44864439964294434, entropy loss: 0.5475857257843018\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.01741289719939232, CLIP loss: -0.19947956502437592, value function loss: 0.44460153579711914, entropy loss: 0.5408304929733276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11628977209329605, CLIP loss: -0.0850125178694725, value function loss: 0.4137776494026184, entropy loss: 0.5586536526679993\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11463254690170288, CLIP loss: -0.12356901168823242, value function loss: 0.4876473546028137, entropy loss: 0.5622116327285767\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1385117471218109, CLIP loss: -0.11448600143194199, value function loss: 0.5171678066253662, entropy loss: 0.5586148500442505\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11341542750597, CLIP loss: -0.09472344070672989, value function loss: 0.42744266986846924, entropy loss: 0.5582464337348938\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18763767182826996, CLIP loss: -0.06735549122095108, value function loss: 0.5209812521934509, entropy loss: 0.5497464537620544\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18673135340213776, CLIP loss: -0.025728359818458557, value function loss: 0.43599092960357666, entropy loss: 0.5535749197006226\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19442801177501678, CLIP loss: -0.0370616689324379, value function loss: 0.4739663004875183, entropy loss: 0.5493482351303101\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12958909571170807, CLIP loss: -0.06132933124899864, value function loss: 0.39285504817962646, entropy loss: 0.5509089231491089\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10999763011932373, CLIP loss: -0.09840570390224457, value function loss: 0.42771995067596436, entropy loss: 0.5456640124320984\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1527874618768692, CLIP loss: -0.11236269026994705, value function loss: 0.5412193536758423, entropy loss: 0.5459528565406799\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08063898980617523, CLIP loss: -0.12487662583589554, value function loss: 0.42181047797203064, entropy loss: 0.5389625430107117\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18214033544063568, CLIP loss: -0.08431816101074219, value function loss: 0.5439141392707825, entropy loss: 0.5498572587966919\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.39803409576416, CLIP loss: 1.1332669258117676, value function loss: 28.540666580200195, entropy loss: 0.5566291213035583\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.862607955932617, CLIP loss: 0.39876335859298706, value function loss: 12.938608169555664, entropy loss: 0.5459097027778625\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 14.211406707763672, CLIP loss: 1.2043821811676025, value function loss: 26.0251522064209, entropy loss: 0.5551098585128784\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.94487190246582, CLIP loss: 0.6361881494522095, value function loss: 14.628345489501953, entropy loss: 0.5488957166671753\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.044904403388500214, CLIP loss: -0.18350566923618317, value function loss: 0.4679127335548401, entropy loss: 0.5546294450759888\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.036068420857191086, CLIP loss: -0.17051607370376587, value function loss: 0.42407774925231934, entropy loss: 0.5454378724098206\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.03180078789591789, CLIP loss: -0.1747281849384308, value function loss: 0.42398563027381897, entropy loss: 0.546384334564209\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.012957909144461155, CLIP loss: -0.18627135455608368, value function loss: 0.40931087732315063, entropy loss: 0.5426174998283386\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.03041408397257328, CLIP loss: -0.164097860455513, value function loss: 0.399922251701355, entropy loss: 0.5449181199073792\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09346429258584976, CLIP loss: -0.09509794414043427, value function loss: 0.38822683691978455, entropy loss: 0.555118203163147\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04934797063469887, CLIP loss: -0.14949704706668854, value function loss: 0.40863561630249023, entropy loss: 0.5472790002822876\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09592468291521072, CLIP loss: -0.10415776073932648, value function loss: 0.41117751598358154, entropy loss: 0.550631582736969\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.03304231911897659, CLIP loss: -0.1649007648229599, value function loss: 0.4068385660648346, entropy loss: 0.5476198792457581\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.031304728239774704, CLIP loss: -0.1580258309841156, value function loss: 0.38976073265075684, entropy loss: 0.5549808144569397\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1295004040002823, CLIP loss: -0.10796575993299484, value function loss: 0.4859427213668823, entropy loss: 0.5505189299583435\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.01614857092499733, CLIP loss: -0.21347598731517792, value function loss: 0.40576523542404175, entropy loss: 0.5555201172828674\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1728113442659378, CLIP loss: -0.078203484416008, value function loss: 0.5132286548614502, entropy loss: 0.5599497556686401\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25178778171539307, CLIP loss: -0.0019828444346785545, value function loss: 0.5185486674308777, entropy loss: 0.5503708124160767\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18568217754364014, CLIP loss: -0.02327748015522957, value function loss: 0.42895635962486267, entropy loss: 0.5518525242805481\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1557130366563797, CLIP loss: -0.043981995433568954, value function loss: 0.41048187017440796, entropy loss: 0.5545893907546997\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1655011773109436, CLIP loss: -0.0631854236125946, value function loss: 0.4682522118091583, entropy loss: 0.543950080871582\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18604904413223267, CLIP loss: -0.023347919806838036, value function loss: 0.42987143993377686, entropy loss: 0.5538765788078308\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16351042687892914, CLIP loss: -0.047520436346530914, value function loss: 0.43304044008255005, entropy loss: 0.5489360690116882\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2036798596382141, CLIP loss: -0.03410617262125015, value function loss: 0.4865167737007141, entropy loss: 0.547235906124115\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23930661380290985, CLIP loss: 0.020862407982349396, value function loss: 0.4477863013744354, entropy loss: 0.5448951721191406\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23878595232963562, CLIP loss: 0.032638177275657654, value function loss: 0.4230867028236389, entropy loss: 0.5395583510398865\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22740471363067627, CLIP loss: 0.0321308970451355, value function loss: 0.40117499232292175, entropy loss: 0.5313681364059448\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25700557231903076, CLIP loss: 0.014317728579044342, value function loss: 0.49637481570243835, entropy loss: 0.549958348274231\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1463124305009842, CLIP loss: -0.03942015767097473, value function loss: 0.3822672665119171, entropy loss: 0.5401049852371216\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22270554304122925, CLIP loss: 0.025290200486779213, value function loss: 0.40591466426849365, entropy loss: 0.5541989207267761\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1883205771446228, CLIP loss: 0.01269146241247654, value function loss: 0.36234816908836365, entropy loss: 0.554497480392456\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15271735191345215, CLIP loss: -0.028834927827119827, value function loss: 0.37393683195114136, entropy loss: 0.5416133403778076\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10426663607358932, CLIP loss: -0.13246197998523712, value function loss: 0.4845750331878662, entropy loss: 0.5558903217315674\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12366989254951477, CLIP loss: -0.13045401871204376, value function loss: 0.5192272663116455, entropy loss: 0.5489723682403564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08588914573192596, CLIP loss: -0.1574002355337143, value function loss: 0.4975477159023285, entropy loss: 0.5484473705291748\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13995210826396942, CLIP loss: -0.1072314903140068, value function loss: 0.5055055022239685, entropy loss: 0.5569148063659668\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21204492449760437, CLIP loss: -0.0022776350378990173, value function loss: 0.43974897265434265, entropy loss: 0.5551937222480774\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16728003323078156, CLIP loss: -0.052608393132686615, value function loss: 0.4507475793361664, entropy loss: 0.5485376119613647\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19814634323120117, CLIP loss: -0.05034065991640091, value function loss: 0.5079765319824219, entropy loss: 0.5501270294189453\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.231103777885437, CLIP loss: 0.00016938801854848862, value function loss: 0.4728284776210785, entropy loss: 0.5479836463928223\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1844657063484192, CLIP loss: -0.02389543689787388, value function loss: 0.4276310205459595, entropy loss: 0.5454367399215698\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12623703479766846, CLIP loss: -0.08124469965696335, value function loss: 0.426044762134552, entropy loss: 0.5540646910667419\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1555866003036499, CLIP loss: -0.039862003177404404, value function loss: 0.4018026888370514, entropy loss: 0.5452743768692017\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15798541903495789, CLIP loss: -0.0734848752617836, value function loss: 0.4740752875804901, entropy loss: 0.5567339062690735\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20746852457523346, CLIP loss: -0.017752734944224358, value function loss: 0.46145522594451904, entropy loss: 0.5506350994110107\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17216306924819946, CLIP loss: -0.04347511753439903, value function loss: 0.4421331584453583, entropy loss: 0.5428383946418762\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20663994550704956, CLIP loss: 0.0009921640157699585, value function loss: 0.4221917986869812, entropy loss: 0.5448125600814819\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15460149943828583, CLIP loss: -0.05619987100362778, value function loss: 0.432464063167572, entropy loss: 0.5430658459663391\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19271796941757202, CLIP loss: -0.010552909225225449, value function loss: 0.417455792427063, entropy loss: 0.5457019805908203\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12725749611854553, CLIP loss: -0.08045562356710434, value function loss: 0.4263639450073242, entropy loss: 0.5468851327896118\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17186953127384186, CLIP loss: -0.052934762090444565, value function loss: 0.46054646372795105, entropy loss: 0.5468932390213013\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17584751546382904, CLIP loss: -0.06371070444583893, value function loss: 0.4899712800979614, entropy loss: 0.5427424907684326\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26116299629211426, CLIP loss: 0.04637930169701576, value function loss: 0.44065120816230774, entropy loss: 0.5541934967041016\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2485174834728241, CLIP loss: 2.6892870664596558e-05, value function loss: 0.5080770254135132, entropy loss: 0.5547908544540405\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20144811272621155, CLIP loss: -0.00908629409968853, value function loss: 0.43225887417793274, entropy loss: 0.5595030188560486\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.335629940032959, CLIP loss: 0.0707695260643959, value function loss: 0.5407935380935669, entropy loss: 0.553635835647583\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19488130509853363, CLIP loss: -0.015614750795066357, value function loss: 0.4319545328617096, entropy loss: 0.5481216907501221\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16130439937114716, CLIP loss: -0.06817667186260223, value function loss: 0.4699230492115021, entropy loss: 0.5480449795722961\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1834041327238083, CLIP loss: -0.033279746770858765, value function loss: 0.44428545236587524, entropy loss: 0.5458849668502808\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18203140795230865, CLIP loss: -0.048015959560871124, value function loss: 0.4710085988044739, entropy loss: 0.54569411277771\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1240462139248848, CLIP loss: -0.08350098878145218, value function loss: 0.42632725834846497, entropy loss: 0.5616415739059448\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09750546514987946, CLIP loss: -0.1308230757713318, value function loss: 0.46771520376205444, entropy loss: 0.5529063940048218\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08706287294626236, CLIP loss: -0.10419820994138718, value function loss: 0.3936847448348999, entropy loss: 0.5581287145614624\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11070959270000458, CLIP loss: -0.10371565073728561, value function loss: 0.44004756212234497, entropy loss: 0.5598541498184204\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20258967578411102, CLIP loss: 0.013002553954720497, value function loss: 0.38987433910369873, entropy loss: 0.5350048542022705\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27314454317092896, CLIP loss: 0.04856910929083824, value function loss: 0.460006982088089, entropy loss: 0.5428044199943542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1983676254749298, CLIP loss: -0.0009099189192056656, value function loss: 0.4092884361743927, entropy loss: 0.5366660952568054\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27384722232818604, CLIP loss: 0.04551147297024727, value function loss: 0.46736928820610046, entropy loss: 0.5348885655403137\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1996760368347168, CLIP loss: -0.04352826625108719, value function loss: 0.4972362816333771, entropy loss: 0.5413825511932373\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1701376885175705, CLIP loss: -0.03620022162795067, value function loss: 0.4233744442462921, entropy loss: 0.5349313616752625\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2002345323562622, CLIP loss: -0.023183833807706833, value function loss: 0.45764943957328796, entropy loss: 0.5406349897384644\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15156258642673492, CLIP loss: -0.062135085463523865, value function loss: 0.4381446838378906, entropy loss: 0.5374667644500732\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2599559426307678, CLIP loss: 0.03957805410027504, value function loss: 0.45167768001556396, entropy loss: 0.5460953712463379\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20709684491157532, CLIP loss: -0.003035495989024639, value function loss: 0.43114644289016724, entropy loss: 0.5440883636474609\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2722087502479553, CLIP loss: 0.058230940252542496, value function loss: 0.4388706386089325, entropy loss: 0.5457523465156555\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19633854925632477, CLIP loss: -0.016519196331501007, value function loss: 0.43663090467453003, entropy loss: 0.5457718968391418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3122633099555969, CLIP loss: 0.10640712082386017, value function loss: 0.42261776328086853, entropy loss: 0.5452685356140137\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2851129472255707, CLIP loss: 0.054041437804698944, value function loss: 0.4727911353111267, entropy loss: 0.5324057340621948\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2516605854034424, CLIP loss: 0.07880187779664993, value function loss: 0.356458455324173, entropy loss: 0.5370516180992126\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.32060039043426514, CLIP loss: 0.08307275921106339, value function loss: 0.4859403371810913, entropy loss: 0.5442535877227783\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2962338328361511, CLIP loss: 0.0864626094698906, value function loss: 0.43042120337486267, entropy loss: 0.5439359545707703\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2676853835582733, CLIP loss: 0.05960116907954216, value function loss: 0.42713332176208496, entropy loss: 0.5482432246208191\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3037272095680237, CLIP loss: 0.10904249548912048, value function loss: 0.4003346562385559, entropy loss: 0.548262357711792\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2737140953540802, CLIP loss: 0.04179821163415909, value function loss: 0.47475945949554443, entropy loss: 0.5463845729827881\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22243735194206238, CLIP loss: -0.014845347963273525, value function loss: 0.48538902401924133, entropy loss: 0.5411816835403442\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.25453343987464905, CLIP loss: 0.026378454640507698, value function loss: 0.46712619066238403, entropy loss: 0.5408110618591309\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23506096005439758, CLIP loss: -0.0021844827570021152, value function loss: 0.4851300120353699, entropy loss: 0.5319564342498779\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2177867591381073, CLIP loss: 0.012860068120062351, value function loss: 0.4207148253917694, entropy loss: 0.5430728793144226\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18945443630218506, CLIP loss: -0.012550324201583862, value function loss: 0.41510921716690063, entropy loss: 0.5549854040145874\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1797865927219391, CLIP loss: -0.028929229825735092, value function loss: 0.4284787178039551, entropy loss: 0.5523539185523987\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2289232462644577, CLIP loss: -0.011330769397318363, value function loss: 0.49148982763290405, entropy loss: 0.5490900278091431\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16553810238838196, CLIP loss: -0.028204182162880898, value function loss: 0.3986106514930725, entropy loss: 0.5563036203384399\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09019061177968979, CLIP loss: -0.1341492384672165, value function loss: 0.4594641327857971, entropy loss: 0.5392213463783264\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1778533160686493, CLIP loss: 0.004127340391278267, value function loss: 0.35829994082450867, entropy loss: 0.5423991084098816\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1286637783050537, CLIP loss: -0.07623273134231567, value function loss: 0.42059212923049927, entropy loss: 0.5399547815322876\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19359561800956726, CLIP loss: -0.04619534686207771, value function loss: 0.4903886020183563, entropy loss: 0.5403334498405457\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22362448275089264, CLIP loss: 0.01466235052794218, value function loss: 0.42857077717781067, entropy loss: 0.5323262810707092\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19935084879398346, CLIP loss: -0.019636482000350952, value function loss: 0.44868671894073486, entropy loss: 0.5356022715568542\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15062089264392853, CLIP loss: -0.027628598734736443, value function loss: 0.3671642243862152, entropy loss: 0.5332615375518799\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21615219116210938, CLIP loss: 0.020088206976652145, value function loss: 0.4027579426765442, entropy loss: 0.5314986705780029\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11708171665668488, CLIP loss: -0.09590289741754532, value function loss: 0.43688973784446716, entropy loss: 0.5460256338119507\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1734643429517746, CLIP loss: -0.01672365888953209, value function loss: 0.3912113308906555, entropy loss: 0.5417664647102356\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1389298290014267, CLIP loss: -0.046866968274116516, value function loss: 0.3824271857738495, entropy loss: 0.541679322719574\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16129539906978607, CLIP loss: -0.0712047815322876, value function loss: 0.4759540855884552, entropy loss: 0.5476863384246826\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 2.146906614303589, CLIP loss: 0.05325201526284218, value function loss: 4.1979079246521, entropy loss: 0.5299243927001953\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 4.768730163574219, CLIP loss: 0.19118046760559082, value function loss: 9.16600513458252, entropy loss: 0.5453281402587891\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 6.776214599609375, CLIP loss: 0.27868857979774475, value function loss: 13.005782127380371, entropy loss: 0.5364681482315063\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22524619102478027, CLIP loss: -0.019098257645964622, value function loss: 0.4994555115699768, entropy loss: 0.5383315086364746\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1068023145198822, CLIP loss: -0.09246528148651123, value function loss: 0.4092377722263336, entropy loss: 0.5351293683052063\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1159689649939537, CLIP loss: -0.10696065425872803, value function loss: 0.4565708339214325, entropy loss: 0.5355794429779053\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10834991186857224, CLIP loss: -0.09065141528844833, value function loss: 0.40874624252319336, entropy loss: 0.5371792316436768\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08425486832857132, CLIP loss: -0.11586969345808029, value function loss: 0.41102227568626404, entropy loss: 0.5386576652526855\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.050576888024806976, CLIP loss: -0.14613349735736847, value function loss: 0.40446576476097107, entropy loss: 0.5522496104240417\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08040634542703629, CLIP loss: -0.12139581888914108, value function loss: 0.4146381616592407, entropy loss: 0.551691472530365\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.06685439497232437, CLIP loss: -0.12693826854228973, value function loss: 0.39869868755340576, entropy loss: 0.5556683540344238\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.061094146221876144, CLIP loss: -0.15058329701423645, value function loss: 0.4342929720878601, entropy loss: 0.5469043850898743\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.282066345214844, CLIP loss: 0.6391960382461548, value function loss: 17.29671859741211, entropy loss: 0.548963725566864\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.395951271057129, CLIP loss: 0.5427514910697937, value function loss: 17.717302322387695, entropy loss: 0.545116662979126\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.400880813598633, CLIP loss: 0.6632693409919739, value function loss: 19.486183166503906, entropy loss: 0.5480141639709473\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.314310073852539, CLIP loss: 0.4669071435928345, value function loss: 15.705728530883789, entropy loss: 0.5461479425430298\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 7.0699687004089355, CLIP loss: 0.361257940530777, value function loss: 13.428238868713379, entropy loss: 0.5408695340156555\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.093053817749023, CLIP loss: 1.268791675567627, value function loss: 27.65914535522461, entropy loss: 0.5310161709785461\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.832391738891602, CLIP loss: 0.5587921142578125, value function loss: 18.557893753051758, entropy loss: 0.5347687602043152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 13.011927604675293, CLIP loss: 1.9577810764312744, value function loss: 22.118919372558594, entropy loss: 0.5313268899917603\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.03403346240520477, CLIP loss: -0.21150213479995728, value function loss: 0.5018171072006226, entropy loss: 0.5372956395149231\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.026439659297466278, CLIP loss: -0.27586957812309265, value function loss: 0.5097503066062927, entropy loss: 0.5445233583450317\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.020777767524123192, CLIP loss: -0.21532823145389557, value function loss: 0.4830196797847748, entropy loss: 0.54038405418396\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.024001626297831535, CLIP loss: -0.2564989924430847, value function loss: 0.47591349482536316, entropy loss: 0.5459381341934204\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.049502573907375336, CLIP loss: -0.18202310800552368, value function loss: 0.4738982319831848, entropy loss: 0.5423433184623718\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.033848270773887634, CLIP loss: -0.1991852968931198, value function loss: 0.47689348459243774, entropy loss: 0.541317343711853\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08135530352592468, CLIP loss: -0.1689719706773758, value function loss: 0.5114595890045166, entropy loss: 0.5402520895004272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.03400908038020134, CLIP loss: -0.20513270795345306, value function loss: 0.489066481590271, entropy loss: 0.5391451120376587\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.050429344177246, CLIP loss: 0.1972714364528656, value function loss: 15.716987609863281, entropy loss: 0.5335485935211182\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.85489559173584, CLIP loss: 0.11912821233272552, value function loss: 11.482317924499512, entropy loss: 0.5391777753829956\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.9521636962890625, CLIP loss: 0.09349192678928375, value function loss: 11.728118896484375, entropy loss: 0.5387555360794067\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.014991760253906, CLIP loss: 0.2484324723482132, value function loss: 15.54378890991211, entropy loss: 0.5335012674331665\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04669773578643799, CLIP loss: -0.16833893954753876, value function loss: 0.44099077582359314, entropy loss: 0.5458712577819824\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04202908277511597, CLIP loss: -0.16024896502494812, value function loss: 0.4155955910682678, entropy loss: 0.551974892616272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07938603311777115, CLIP loss: -0.1454140841960907, value function loss: 0.4606182277202606, entropy loss: 0.5508999824523926\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.021995123475790024, CLIP loss: -0.17845922708511353, value function loss: 0.41188177466392517, entropy loss: 0.5486536026000977\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 5.596724033355713, CLIP loss: 0.2518904507160187, value function loss: 10.700968742370605, entropy loss: 0.565078854560852\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 15.587247848510742, CLIP loss: 1.0111563205718994, value function loss: 29.163249969482422, entropy loss: 0.5533318519592285\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.660221099853516, CLIP loss: 0.5675901174545288, value function loss: 20.196468353271484, entropy loss: 0.5602749586105347\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.516291618347168, CLIP loss: 0.6604422330856323, value function loss: 19.722808837890625, entropy loss: 0.5554811954498291\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.09758064150810242, CLIP loss: -0.3195507526397705, value function loss: 0.4545913338661194, entropy loss: 0.5325555205345154\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.046191923320293427, CLIP loss: -0.26917874813079834, value function loss: 0.4567629098892212, entropy loss: 0.5394628643989563\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.033132296055555344, CLIP loss: -0.25869905948638916, value function loss: 0.4618294835090637, entropy loss: 0.5347979068756104\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.09394847601652145, CLIP loss: -0.33240455389022827, value function loss: 0.4876704216003418, entropy loss: 0.5379129648208618\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.037348922342061996, CLIP loss: -0.2568517327308655, value function loss: 0.45011621713638306, entropy loss: 0.5555299520492554\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.04362792149186134, CLIP loss: -0.23604632914066315, value function loss: 0.3958699107170105, entropy loss: 0.5516547560691833\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.013524029403924942, CLIP loss: -0.2034134864807129, value function loss: 0.4448971152305603, entropy loss: 0.5511041879653931\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.06695923954248428, CLIP loss: -0.291151225566864, value function loss: 0.4594854712486267, entropy loss: 0.5550751686096191\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.013575076125562191, CLIP loss: -0.20679625868797302, value function loss: 0.4519988000392914, entropy loss: 0.5628065466880798\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.012592654675245285, CLIP loss: -0.18790477514266968, value function loss: 0.4121929109096527, entropy loss: 0.559902548789978\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, -0.027733836323022842, CLIP loss: -0.21574349701404572, value function loss: 0.3872220814228058, entropy loss: 0.5601381063461304\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.021552031859755516, CLIP loss: -0.1727968454360962, value function loss: 0.3999568819999695, entropy loss: 0.5629563331604004\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1124015673995018, CLIP loss: -0.11028718948364258, value function loss: 0.45627743005752563, entropy loss: 0.5449956059455872\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07633750885725021, CLIP loss: -0.1377730369567871, value function loss: 0.4391956329345703, entropy loss: 0.5487272143363953\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10810812562704086, CLIP loss: -0.09216911345720291, value function loss: 0.41146183013916016, entropy loss: 0.5453673601150513\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.052791740745306015, CLIP loss: -0.1631135493516922, value function loss: 0.4426465928554535, entropy loss: 0.5418006181716919\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14892889559268951, CLIP loss: -0.0448179617524147, value function loss: 0.3986378014087677, entropy loss: 0.557202935218811\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.04796535521745682, CLIP loss: -0.12467776238918304, value function loss: 0.35635077953338623, entropy loss: 0.5532273650169373\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12174736708402634, CLIP loss: -0.1038164496421814, value function loss: 0.4621531069278717, entropy loss: 0.5512735247612\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1401757150888443, CLIP loss: -0.0671844333410263, value function loss: 0.425886869430542, entropy loss: 0.5583286881446838\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.935007095336914, CLIP loss: 0.7184154987335205, value function loss: 22.44422149658203, entropy loss: 0.5519293546676636\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.810425758361816, CLIP loss: 0.7162163257598877, value function loss: 18.199331283569336, entropy loss: 0.5456259846687317\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 11.58010196685791, CLIP loss: 0.815342903137207, value function loss: 21.54050636291504, entropy loss: 0.5493798851966858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 9.945066452026367, CLIP loss: 0.650600016117096, value function loss: 18.599903106689453, entropy loss: 0.5485504865646362\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10063993185758591, CLIP loss: -0.09575165063142776, value function loss: 0.4035267233848572, entropy loss: 0.5371780395507812\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15937146544456482, CLIP loss: -0.057165637612342834, value function loss: 0.4440588057041168, entropy loss: 0.549229621887207\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13728155195713043, CLIP loss: -0.0728650614619255, value function loss: 0.43115997314453125, entropy loss: 0.5433366298675537\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16389863193035126, CLIP loss: -0.08908496797084808, value function loss: 0.5166406631469727, entropy loss: 0.5336726903915405\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.06294150650501251, CLIP loss: -0.11888077110052109, value function loss: 0.37444519996643066, entropy loss: 0.5400323271751404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.0963500589132309, CLIP loss: -0.12546706199645996, value function loss: 0.4545620381832123, entropy loss: 0.5463897585868835\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10749233514070511, CLIP loss: -0.10873696953058243, value function loss: 0.44334280490875244, entropy loss: 0.5442100763320923\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.07727108150720596, CLIP loss: -0.1247294545173645, value function loss: 0.41482093930244446, entropy loss: 0.5409936904907227\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10917335748672485, CLIP loss: -0.09864211082458496, value function loss: 0.42637768387794495, entropy loss: 0.5373371839523315\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12553252279758453, CLIP loss: -0.08849209547042847, value function loss: 0.4388055205345154, entropy loss: 0.5378142595291138\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13329099118709564, CLIP loss: -0.09131869673728943, value function loss: 0.45984917879104614, entropy loss: 0.5314897894859314\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.08884505927562714, CLIP loss: -0.10150066018104553, value function loss: 0.3915453553199768, entropy loss: 0.5426961183547974\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12893787026405334, CLIP loss: -0.07600029557943344, value function loss: 0.4205772280693054, entropy loss: 0.5350445508956909\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19495615363121033, CLIP loss: -0.017908483743667603, value function loss: 0.4365234971046448, entropy loss: 0.5397105813026428\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18149182200431824, CLIP loss: -0.011504930444061756, value function loss: 0.39668112993240356, entropy loss: 0.5343807935714722\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11973182111978531, CLIP loss: -0.06918860971927643, value function loss: 0.38860729336738586, entropy loss: 0.5383212566375732\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09122293442487717, CLIP loss: -0.10572610050439835, value function loss: 0.40461063385009766, entropy loss: 0.5356281995773315\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15030871331691742, CLIP loss: -0.039508551359176636, value function loss: 0.3904133141040802, entropy loss: 0.5389396548271179\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13022571802139282, CLIP loss: -0.07801411300897598, value function loss: 0.4272002577781677, entropy loss: 0.5360304117202759\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1692870855331421, CLIP loss: -0.06195041164755821, value function loss: 0.4732843041419983, entropy loss: 0.5404657125473022\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17468111217021942, CLIP loss: -0.0070469677448272705, value function loss: 0.37415143847465515, entropy loss: 0.5347636938095093\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1856849640607834, CLIP loss: -0.035686518996953964, value function loss: 0.45351365208625793, entropy loss: 0.5385335087776184\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16787901520729065, CLIP loss: -0.022764595225453377, value function loss: 0.3920835554599762, entropy loss: 0.5398167371749878\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18107619881629944, CLIP loss: -0.01879674196243286, value function loss: 0.4104060232639313, entropy loss: 0.5330068469047546\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15192972123622894, CLIP loss: -0.04514623433351517, value function loss: 0.40505537390708923, entropy loss: 0.5451731085777283\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11515861004590988, CLIP loss: -0.08389370143413544, value function loss: 0.4090077877044678, entropy loss: 0.545158326625824\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16082321107387543, CLIP loss: -0.03992526978254318, value function loss: 0.41249486804008484, entropy loss: 0.5498966574668884\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12100385129451752, CLIP loss: -0.09410212934017181, value function loss: 0.441034734249115, entropy loss: 0.5411384105682373\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1378404200077057, CLIP loss: -0.0696488469839096, value function loss: 0.42577096819877625, entropy loss: 0.539621114730835\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09978588670492172, CLIP loss: -0.0831759124994278, value function loss: 0.3765932619571686, entropy loss: 0.5334829688072205\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14733757078647614, CLIP loss: -0.07319841533899307, value function loss: 0.45175468921661377, entropy loss: 0.5341372489929199\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12873274087905884, CLIP loss: -0.07425844669342041, value function loss: 0.4166632890701294, entropy loss: 0.5340462923049927\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14776967465877533, CLIP loss: -0.04171547293663025, value function loss: 0.3898901343345642, entropy loss: 0.545991837978363\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2244093269109726, CLIP loss: 0.014299822971224785, value function loss: 0.4311726689338684, entropy loss: 0.5476837158203125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16941764950752258, CLIP loss: -0.056193456053733826, value function loss: 0.4620818495750427, entropy loss: 0.5429826974868774\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2078429013490677, CLIP loss: 0.01589706540107727, value function loss: 0.3949441611766815, entropy loss: 0.5526248812675476\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19988924264907837, CLIP loss: 0.014216477982699871, value function loss: 0.3822406828403473, entropy loss: 0.5447576642036438\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2364155352115631, CLIP loss: 0.02242298051714897, value function loss: 0.43888893723487854, entropy loss: 0.5451916456222534\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21217907965183258, CLIP loss: 0.015405037440359592, value function loss: 0.4044798016548157, entropy loss: 0.5465872287750244\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19605456292629242, CLIP loss: 0.025883648544549942, value function loss: 0.351158082485199, entropy loss: 0.5408116579055786\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16458894312381744, CLIP loss: -0.011605658568441868, value function loss: 0.36348244547843933, entropy loss: 0.5546611547470093\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10284790396690369, CLIP loss: -0.10095536708831787, value function loss: 0.418596088886261, entropy loss: 0.5494775772094727\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.09394721686840057, CLIP loss: -0.09208394587039948, value function loss: 0.3830968141555786, entropy loss: 0.5517246127128601\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20354001224040985, CLIP loss: -0.019990211352705956, value function loss: 0.4581248164176941, entropy loss: 0.553218424320221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1761694699525833, CLIP loss: -0.03005749173462391, value function loss: 0.42310455441474915, entropy loss: 0.5325312614440918\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20497910678386688, CLIP loss: 0.0046586692333221436, value function loss: 0.41146421432495117, entropy loss: 0.5411669015884399\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15268145501613617, CLIP loss: -0.037881240248680115, value function loss: 0.3919435143470764, entropy loss: 0.5409059524536133\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18079273402690887, CLIP loss: 0.0101285669952631, value function loss: 0.35199233889579773, entropy loss: 0.5332001447677612\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17114481329917908, CLIP loss: -0.037939008325338364, value function loss: 0.4291887581348419, entropy loss: 0.5510554909706116\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1612171083688736, CLIP loss: -0.04057095944881439, value function loss: 0.41469743847846985, entropy loss: 0.5560652017593384\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21778474748134613, CLIP loss: -0.030564676970243454, value function loss: 0.5076445937156677, entropy loss: 0.5472872257232666\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12774789333343506, CLIP loss: -0.04194909334182739, value function loss: 0.35055825114250183, entropy loss: 0.558213472366333\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2416483759880066, CLIP loss: 0.04973535239696503, value function loss: 0.39471179246902466, entropy loss: 0.5442876219749451\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18314115703105927, CLIP loss: -0.00830470584332943, value function loss: 0.3938100337982178, entropy loss: 0.5459165573120117\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24582183361053467, CLIP loss: 0.028501158580183983, value function loss: 0.4455384910106659, entropy loss: 0.5448580384254456\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18849903345108032, CLIP loss: 0.021146833896636963, value function loss: 0.34556448459625244, entropy loss: 0.5430046916007996\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1951460838317871, CLIP loss: -0.004637956619262695, value function loss: 0.41028526425361633, entropy loss: 0.5358594059944153\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15183207392692566, CLIP loss: -0.0359521210193634, value function loss: 0.3863879442214966, entropy loss: 0.5409772396087646\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16719655692577362, CLIP loss: -0.03263990208506584, value function loss: 0.41026771068573, entropy loss: 0.5297393202781677\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20946404337882996, CLIP loss: -0.007607584819197655, value function loss: 0.4450606107711792, entropy loss: 0.5458675622940063\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2408958524465561, CLIP loss: 0.019517824053764343, value function loss: 0.4535945951938629, entropy loss: 0.5419276356697083\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22327293455600739, CLIP loss: 0.013081913813948631, value function loss: 0.4310491681098938, entropy loss: 0.533355176448822\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22363822162151337, CLIP loss: 0.010985681787133217, value function loss: 0.4362015128135681, entropy loss: 0.5448224544525146\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.218687042593956, CLIP loss: 0.02100917138159275, value function loss: 0.4060973525047302, entropy loss: 0.5370805859565735\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16739606857299805, CLIP loss: -0.005921635311096907, value function loss: 0.35724857449531555, entropy loss: 0.5306580662727356\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17750334739685059, CLIP loss: -0.013194219209253788, value function loss: 0.392179399728775, entropy loss: 0.5392137765884399\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19911044836044312, CLIP loss: 0.006322063505649567, value function loss: 0.3964250683784485, entropy loss: 0.5424141883850098\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15322445333003998, CLIP loss: -0.031125018373131752, value function loss: 0.37924081087112427, entropy loss: 0.5270925164222717\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11738166958093643, CLIP loss: -0.07088778913021088, value function loss: 0.3875330090522766, entropy loss: 0.549704372882843\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13434377312660217, CLIP loss: -0.05853186547756195, value function loss: 0.39656683802604675, entropy loss: 0.5407781600952148\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1484176218509674, CLIP loss: -0.0340648852288723, value function loss: 0.37586134672164917, entropy loss: 0.5448159575462341\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1023402139544487, CLIP loss: -0.08979257941246033, value function loss: 0.395173579454422, entropy loss: 0.5453994274139404\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1625090390443802, CLIP loss: 0.006146732252091169, value function loss: 0.3234720826148987, entropy loss: 0.5373732447624207\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22087462246418, CLIP loss: 0.010378945618867874, value function loss: 0.4316275715827942, entropy loss: 0.5318105816841125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2022477239370346, CLIP loss: 0.02135411836206913, value function loss: 0.37257084250450134, entropy loss: 0.5391824841499329\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1850673258304596, CLIP loss: -0.005151359364390373, value function loss: 0.3910132050514221, entropy loss: 0.5287920236587524\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19511032104492188, CLIP loss: 0.010988542810082436, value function loss: 0.37890464067459106, entropy loss: 0.5330544710159302\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2538087069988251, CLIP loss: 0.05647720396518707, value function loss: 0.4056229293346405, entropy loss: 0.5479950308799744\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2061966359615326, CLIP loss: 0.012898802757263184, value function loss: 0.39739882946014404, entropy loss: 0.540157675743103\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2626955807209015, CLIP loss: 0.05949908122420311, value function loss: 0.41719722747802734, entropy loss: 0.5402128100395203\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21886740624904633, CLIP loss: 0.022077444940805435, value function loss: 0.40461301803588867, entropy loss: 0.5516548156738281\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21950893104076385, CLIP loss: 0.026014288887381554, value function loss: 0.3978290557861328, entropy loss: 0.5419875979423523\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19526569545269012, CLIP loss: 0.028727183118462563, value function loss: 0.3439153730869293, entropy loss: 0.5419182181358337\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20955611765384674, CLIP loss: 0.019975056871771812, value function loss: 0.3902195394039154, entropy loss: 0.5528702139854431\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19552847743034363, CLIP loss: 0.028396740555763245, value function loss: 0.3448476195335388, entropy loss: 0.529207706451416\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18745262920856476, CLIP loss: -0.015018503181636333, value function loss: 0.41538840532302856, entropy loss: 0.5223060846328735\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21235910058021545, CLIP loss: 0.022169480100274086, value function loss: 0.3909336030483246, entropy loss: 0.5277179479598999\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18129496276378632, CLIP loss: -0.016741378232836723, value function loss: 0.40656378865242004, entropy loss: 0.5245553255081177\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13492687046527863, CLIP loss: -0.06474196910858154, value function loss: 0.4100360870361328, entropy loss: 0.5349206924438477\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.27911362051963806, CLIP loss: -0.0190071202814579, value function loss: 0.6071926355361938, entropy loss: 0.5475573539733887\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20047052204608917, CLIP loss: -0.014452757313847542, value function loss: 0.44060218334198, entropy loss: 0.5377811789512634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16579407453536987, CLIP loss: -0.05761966109275818, value function loss: 0.45766568183898926, entropy loss: 0.54190993309021\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23363091051578522, CLIP loss: 0.0608680434525013, value function loss: 0.3563547134399414, entropy loss: 0.5414479970932007\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2666631042957306, CLIP loss: 0.05133798345923424, value function loss: 0.441510945558548, entropy loss: 0.5430347919464111\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2894027531147003, CLIP loss: 0.08983919024467468, value function loss: 0.4098123610019684, entropy loss: 0.5342628955841064\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18460537493228912, CLIP loss: 0.021160468459129333, value function loss: 0.33772972226142883, entropy loss: 0.5419955253601074\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10946960747241974, CLIP loss: -0.0694083422422409, value function loss: 0.36874958872795105, entropy loss: 0.5496845841407776\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.173681378364563, CLIP loss: 0.0023497664369642735, value function loss: 0.3535221517086029, entropy loss: 0.5429469347000122\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14664530754089355, CLIP loss: -0.03419198468327522, value function loss: 0.3726111352443695, entropy loss: 0.5468279719352722\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14229774475097656, CLIP loss: -0.03114035725593567, value function loss: 0.3578166961669922, entropy loss: 0.5470238924026489\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20097362995147705, CLIP loss: 0.023597992956638336, value function loss: 0.36566978693008423, entropy loss: 0.545925498008728\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19814558327198029, CLIP loss: 0.018700532615184784, value function loss: 0.36993545293807983, entropy loss: 0.5522688627243042\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2152341604232788, CLIP loss: 0.03436950594186783, value function loss: 0.3727359473705292, entropy loss: 0.5503333806991577\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18557512760162354, CLIP loss: 0.01652752235531807, value function loss: 0.3489874601364136, entropy loss: 0.5446115136146545\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20087528228759766, CLIP loss: 0.023373020812869072, value function loss: 0.3656579852104187, entropy loss: 0.5326733589172363\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1318943351507187, CLIP loss: -0.05011715367436409, value function loss: 0.3748912513256073, entropy loss: 0.5434136390686035\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17199355363845825, CLIP loss: 0.010328859090805054, value function loss: 0.3340688943862915, entropy loss: 0.5369760394096375\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1507527083158493, CLIP loss: -0.0332891009747982, value function loss: 0.37881016731262207, entropy loss: 0.5363270044326782\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14709064364433289, CLIP loss: -0.06746263056993484, value function loss: 0.4397751986980438, entropy loss: 0.533432126045227\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1776331216096878, CLIP loss: -0.04023493453860283, value function loss: 0.4466579556465149, entropy loss: 0.54609215259552\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15173032879829407, CLIP loss: -0.05787906423211098, value function loss: 0.4302198886871338, entropy loss: 0.550055205821991\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14759966731071472, CLIP loss: -0.037704166024923325, value function loss: 0.38133078813552856, entropy loss: 0.5361557602882385\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.31533893942832947, CLIP loss: 0.10210338234901428, value function loss: 0.4371427893638611, entropy loss: 0.5335851311683655\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18767130374908447, CLIP loss: 0.014588914811611176, value function loss: 0.35679739713668823, entropy loss: 0.5316324830055237\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.28178808093070984, CLIP loss: 0.08067180961370468, value function loss: 0.4128984212875366, entropy loss: 0.5332960486412048\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2149200141429901, CLIP loss: 0.0479528084397316, value function loss: 0.344817578792572, entropy loss: 0.5441575050354004\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2073066383600235, CLIP loss: 0.007681258954107761, value function loss: 0.41015177965164185, entropy loss: 0.5450512170791626\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19722485542297363, CLIP loss: 0.010220557451248169, value function loss: 0.38487863540649414, entropy loss: 0.5435020923614502\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19005714356899261, CLIP loss: 0.01524948887526989, value function loss: 0.3604784607887268, entropy loss: 0.5431573390960693\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19262078404426575, CLIP loss: -0.001033564331009984, value function loss: 0.3981678783893585, entropy loss: 0.5429596900939941\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20030257105827332, CLIP loss: -0.010565318167209625, value function loss: 0.4319574236869812, entropy loss: 0.5110827088356018\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18595045804977417, CLIP loss: -0.010114671662449837, value function loss: 0.40241342782974243, entropy loss: 0.5141589045524597\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22292794287204742, CLIP loss: -0.011538118124008179, value function loss: 0.47930437326431274, entropy loss: 0.5186131000518799\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19587810337543488, CLIP loss: -0.0016952445730566978, value function loss: 0.40545663237571716, entropy loss: 0.5154964327812195\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1636785864830017, CLIP loss: -0.006261313334107399, value function loss: 0.3509470224380493, entropy loss: 0.5533602833747864\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2233714759349823, CLIP loss: 0.006193395704030991, value function loss: 0.44538140296936035, entropy loss: 0.5512620210647583\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1528892070055008, CLIP loss: -0.009044348262250423, value function loss: 0.3349708914756775, entropy loss: 0.5551885366439819\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18563036620616913, CLIP loss: 0.00929267331957817, value function loss: 0.3636714220046997, entropy loss: 0.5498026609420776\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14503349363803864, CLIP loss: -0.025842798873782158, value function loss: 0.35284730792045593, entropy loss: 0.5547364354133606\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1610916256904602, CLIP loss: -0.0061540864408016205, value function loss: 0.3454543948173523, entropy loss: 0.5481480360031128\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12176253646612167, CLIP loss: -0.027610529214143753, value function loss: 0.3097500205039978, entropy loss: 0.5501949191093445\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16863633692264557, CLIP loss: -0.0026786457747220993, value function loss: 0.3536558747291565, entropy loss: 0.5512951612472534\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1716223955154419, CLIP loss: -0.02415558136999607, value function loss: 0.40217387676239014, entropy loss: 0.5308954119682312\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.17489182949066162, CLIP loss: -0.013985855504870415, value function loss: 0.38818132877349854, entropy loss: 0.5212982892990112\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1270517110824585, CLIP loss: -0.05723617225885391, value function loss: 0.3791709244251251, entropy loss: 0.5297566652297974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22180813550949097, CLIP loss: 0.019760170951485634, value function loss: 0.41468098759651184, entropy loss: 0.5292534232139587\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18204975128173828, CLIP loss: -0.03261308744549751, value function loss: 0.44014379382133484, entropy loss: 0.5409061312675476\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16234055161476135, CLIP loss: -0.024120137095451355, value function loss: 0.38379746675491333, entropy loss: 0.543804407119751\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1867704689502716, CLIP loss: -0.0028853067196905613, value function loss: 0.3900943100452423, entropy loss: 0.5391373634338379\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14214996993541718, CLIP loss: -0.04056426137685776, value function loss: 0.3763887882232666, entropy loss: 0.5480149984359741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22116783261299133, CLIP loss: 0.048598725348711014, value function loss: 0.3561360239982605, entropy loss: 0.5498895645141602\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21927617490291595, CLIP loss: 0.02632502093911171, value function loss: 0.3969174325466156, entropy loss: 0.5507559180259705\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22487959265708923, CLIP loss: 0.047177039086818695, value function loss: 0.3663689196109772, entropy loss: 0.5481906533241272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.19078107178211212, CLIP loss: 0.01759946346282959, value function loss: 0.35724276304244995, entropy loss: 0.5439770221710205\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22752432525157928, CLIP loss: 0.06020161509513855, value function loss: 0.3451951742172241, entropy loss: 0.5274880528450012\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2806624472141266, CLIP loss: 0.07954581081867218, value function loss: 0.41294175386428833, entropy loss: 0.5354268550872803\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23066438734531403, CLIP loss: 0.06517566740512848, value function loss: 0.34169983863830566, entropy loss: 0.5361203551292419\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2670685052871704, CLIP loss: 0.08645528554916382, value function loss: 0.371920108795166, entropy loss: 0.5346836447715759\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.12065120786428452, CLIP loss: -0.06931750476360321, value function loss: 0.39062777161598206, entropy loss: 0.5345174670219421\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.13585804402828217, CLIP loss: -0.09504086524248123, value function loss: 0.4726333022117615, entropy loss: 0.5417755246162415\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10529939830303192, CLIP loss: -0.08663608133792877, value function loss: 0.39448297023773193, entropy loss: 0.5306003093719482\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1310037523508072, CLIP loss: -0.074276864528656, value function loss: 0.421234130859375, entropy loss: 0.5336443185806274\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2612943649291992, CLIP loss: 0.0740261897444725, value function loss: 0.3856815695762634, entropy loss: 0.5572616457939148\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23144656419754028, CLIP loss: 0.0202382393181324, value function loss: 0.43353718519210815, entropy loss: 0.5560259222984314\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20223718881607056, CLIP loss: 0.0364978089928627, value function loss: 0.3425476551055908, entropy loss: 0.5534435510635376\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2445257008075714, CLIP loss: 0.053551964461803436, value function loss: 0.3930525779724121, entropy loss: 0.5552564263343811\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.33984673023223877, CLIP loss: 0.1073186993598938, value function loss: 0.4757690727710724, entropy loss: 0.535649299621582\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.3277270495891571, CLIP loss: 0.11837264150381088, value function loss: 0.4295341968536377, entropy loss: 0.5412688255310059\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.321593701839447, CLIP loss: 0.10931593924760818, value function loss: 0.43527063727378845, entropy loss: 0.5357574820518494\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.34538403153419495, CLIP loss: 0.10305304825305939, value function loss: 0.49552351236343384, entropy loss: 0.5430785417556763\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.14485593140125275, CLIP loss: -0.04761090129613876, value function loss: 0.3955425024032593, entropy loss: 0.5304409861564636\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18241281807422638, CLIP loss: -0.01185446884483099, value function loss: 0.399350106716156, entropy loss: 0.5407767295837402\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15151019394397736, CLIP loss: -0.04946865513920784, value function loss: 0.41262286901474, entropy loss: 0.5332584977149963\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1766652911901474, CLIP loss: -0.0211548563092947, value function loss: 0.40644577145576477, entropy loss: 0.5402740836143494\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16616858541965485, CLIP loss: -0.012941498309373856, value function loss: 0.3690687417984009, entropy loss: 0.5424296259880066\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15600165724754333, CLIP loss: -0.03228836879134178, value function loss: 0.38745105266571045, entropy loss: 0.5435489416122437\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1562359631061554, CLIP loss: -0.032074227929115295, value function loss: 0.38732534646987915, entropy loss: 0.535248875617981\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15756142139434814, CLIP loss: -0.015426669269800186, value function loss: 0.35685309767723083, entropy loss: 0.5438464283943176\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 10.335861206054688, CLIP loss: 0.4968516528606415, value function loss: 19.68855094909668, entropy loss: 0.5266566872596741\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 1.78937566280365, CLIP loss: 0.017997635528445244, value function loss: 3.553399085998535, entropy loss: 0.5321504473686218\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 3.8106024265289307, CLIP loss: 0.1480078548192978, value function loss: 7.33576774597168, entropy loss: 0.5289412140846252\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 8.343912124633789, CLIP loss: 0.3910021185874939, value function loss: 15.916507720947266, entropy loss: 0.5343717336654663\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15881627798080444, CLIP loss: -0.050758130848407745, value function loss: 0.42999279499053955, entropy loss: 0.542199969291687\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.15936751663684845, CLIP loss: -0.04101261496543884, value function loss: 0.4117559492588043, entropy loss: 0.5497838258743286\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18390589952468872, CLIP loss: -0.025968501344323158, value function loss: 0.430620402097702, entropy loss: 0.5435787439346313\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.11328345537185669, CLIP loss: -0.06515635550022125, value function loss: 0.36784833669662476, entropy loss: 0.5484361052513123\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22906874120235443, CLIP loss: 0.04303360730409622, value function loss: 0.38271668553352356, entropy loss: 0.5323215126991272\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23089098930358887, CLIP loss: 0.023217950016260147, value function loss: 0.42603814601898193, entropy loss: 0.5346035957336426\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.23451802134513855, CLIP loss: 0.056379981338977814, value function loss: 0.36698153614997864, entropy loss: 0.5352737903594971\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18530794978141785, CLIP loss: 0.01299295574426651, value function loss: 0.35534197092056274, entropy loss: 0.5355982184410095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.10775760561227798, CLIP loss: -0.08655533939599991, value function loss: 0.3991553783416748, entropy loss: 0.52647465467453\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1484443098306656, CLIP loss: -0.049621663987636566, value function loss: 0.4069461226463318, entropy loss: 0.5407078266143799\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21052229404449463, CLIP loss: -0.002876847982406616, value function loss: 0.4375477731227875, entropy loss: 0.5374749898910522\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.06140885502099991, CLIP loss: -0.1331312656402588, value function loss: 0.3997878134250641, entropy loss: 0.5353787541389465\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21687918901443481, CLIP loss: 0.015298177488148212, value function loss: 0.41405168175697327, entropy loss: 0.5444822311401367\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.18413279950618744, CLIP loss: 0.008499983698129654, value function loss: 0.3619479537010193, entropy loss: 0.5341163873672485\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.20658917725086212, CLIP loss: 0.008002151735126972, value function loss: 0.4078406095504761, entropy loss: 0.533327043056488\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.1995779275894165, CLIP loss: 0.013816576451063156, value function loss: 0.3823217451572418, entropy loss: 0.539952278137207\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22232463955879211, CLIP loss: 0.0010439432226121426, value function loss: 0.4530993700027466, entropy loss: 0.526898980140686\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.16934993863105774, CLIP loss: -0.004414171911776066, value function loss: 0.3583146333694458, entropy loss: 0.5393202304840088\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21145032346248627, CLIP loss: 0.0072127170860767365, value function loss: 0.4192412495613098, entropy loss: 0.5383011698722839\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.21743126213550568, CLIP loss: -0.00385308638215065, value function loss: 0.4532604217529297, entropy loss: 0.5345868468284607\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2668341398239136, CLIP loss: 0.04634355753660202, value function loss: 0.45187899470329285, entropy loss: 0.5448908805847168\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24629870057106018, CLIP loss: 0.03050476871430874, value function loss: 0.4424101710319519, entropy loss: 0.5411155819892883\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.26662588119506836, CLIP loss: 0.03885349631309509, value function loss: 0.46644073724746704, entropy loss: 0.5447980165481567\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24412649869918823, CLIP loss: 0.037024836987257004, value function loss: 0.42494696378707886, entropy loss: 0.5371816754341125\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2147364765405655, CLIP loss: -0.009807433001697063, value function loss: 0.46026140451431274, entropy loss: 0.558678925037384\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.2061479687690735, CLIP loss: 0.010616682469844818, value function loss: 0.40212422609329224, entropy loss: 0.5530831813812256\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.22794592380523682, CLIP loss: -0.012024365365505219, value function loss: 0.4909842312335968, entropy loss: 0.5521830916404724\n",
      "----------------------------------------------------------\n",
      "TOTAL LOSS, 0.24969902634620667, CLIP loss: 0.019984370097517967, value function loss: 0.47059568762779236, entropy loss: 0.558320164680481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create N envs\n",
    "envs = []\n",
    "for i in range(configs.num_trajcts):\n",
    "    envs.append( make_env_func(configs.gym_id, seed + i, i, run_name) )\n",
    "envs = gym.vector.SyncVectorEnv(envs)\n",
    "\n",
    "# start the environment\n",
    "cur_observation = envs.reset()[0]\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A generic fully connected residual block with good setup\"\"\"\n",
    "    def __init__(self, embd_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LayerNorm(embd_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embd_dim, 4*embd_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embd_dim, embd_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \"\"\"an agent that creates actions and estimates values\"\"\"\n",
    "    def __init__(self, env_observation_dim, action_space_dim, embd_dim=64, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Linear(env_observation_dim, embd_dim)\n",
    "        self.shared_layers = nn.Sequential(*[FCBlock(embd_dim=embd_dim) for _ in range(num_blocks)])\n",
    "        self.value_head = nn.Linear(embd_dim, 1)\n",
    "        self.policy_head = nn.Linear(embd_dim, action_space_dim)\n",
    "        # orthogonal initialization with a hi entropy for more exploration at the start\n",
    "        torch.nn.init.orthogonal_(self.policy_head.weight, 0.01)\n",
    "\n",
    "    def value_func(self, state):\n",
    "        hidden = self.shared_layers(self.embedding_layer(state))\n",
    "        value = self.value_head(hidden)\n",
    "        return value\n",
    "\n",
    "    def policy(self, state, action=None):\n",
    "        hidden = self.shared_layers(self.embedding_layer(state))\n",
    "        logits = self.policy_head(hidden)\n",
    "        # PyTorch categorical class helpful for sampling and probability calculations\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.value_head(hidden)\n",
    "\n",
    "#     \"num_trajcts\": 32, # N\n",
    "#     \"max_trajects_length\": 64, # T\n",
    "\n",
    "def create_rollout(envs, cur_observation, cur_done, agent):\n",
    "\n",
    "    observations = torch.zeros((cur_observation.shape[0], configs['max_trajects_length'],envs.single_observation_space.shape[0] ))\n",
    "    actions = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']) + envs.single_action_space.shape)\n",
    "    dones =  torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    rewards =  torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    values = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    advantages = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    logprobs = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "     \n",
    "    \n",
    "    for t in range(configs['max_trajects_length']):\n",
    "        # get the policy\n",
    "        with torch.no_grad():\n",
    "            action, logprob,_,value = agent.policy(cur_observation)\n",
    "            \n",
    "        observations[:,t,:] = cur_observation\n",
    "        actions[:,t] = action\n",
    "        dones[:,t] = cur_done\n",
    "        logprobs[:,t] = logprob\n",
    "        \n",
    "        cur_observation, cur_reward, cur_done,_,_ = envs.step(action.cpu().numpy())\n",
    "        cur_observation = torch.tensor(cur_observation)\n",
    "        cur_reward = torch.tensor(cur_reward)\n",
    "        cur_done = torch.tensor(cur_done)\n",
    "        \n",
    "        rewards[:,t] = torch.tensor(cur_reward)\n",
    "\n",
    "        values[:,t] = value.squeeze()\n",
    "       \n",
    "    # Advantage is approximated reverse recursively\n",
    "#     advantage = gae(observations,dones, rewards,values, advantages)\n",
    "    \n",
    "    return {\n",
    "        \"cur_observation\": cur_observation,\n",
    "        \"observations\": observations,\n",
    "        \"actions\" : actions,\n",
    "        \"dones\" : dones,\n",
    "        \"rewards\" : rewards,\n",
    "        \"values\" : values,\n",
    "        \"advantages\" : advantages,\n",
    "        \"logprobs\" : logprobs\n",
    "    }\n",
    "\n",
    "agent = Agent(\n",
    "    env_observation_dim=envs.single_observation_space.shape[0],\n",
    "    action_space_dim=envs.single_action_space.n\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=configs.learning_rate)\n",
    "\n",
    "\n",
    "def gae(rewards, values, dones, cur_observation, agent):\n",
    "    last_advantage = 0\n",
    "    with torch.no_grad():\n",
    "        last_value = agent.value_func(cur_observation).reshape(1,-1)\n",
    "    advantages = torch.zeros_like(values)\n",
    "    \n",
    "    for t in reversed(range(configs['max_trajects_length'])):\n",
    "        mask = 1.0 - dones[:,t]\n",
    "        last_advantage = mask*last_advantage\n",
    "        last_value = mask*last_value\n",
    "        delta = rewards[:,t] + configs['gamma']*last_value - values[:,t]\n",
    "        advantages[:,t] = delta + configs['gae_lambda']*configs['gamma']*last_advantage\n",
    "        last_value = values[:,t]\n",
    "        last_advantage = advantages[:,t]\n",
    "    \n",
    "    returns = advantages + values\n",
    "        \n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "def loss_clip(\n",
    "    mb_oldlogporb,     # old logprob of mini batch actions collected during the rollout\n",
    "    mb_newlogprob,     # new logprob of mini batch actions created by the new policy\n",
    "    mb_advantages      # mini batch of advantages collected during the the rollout\n",
    "):\n",
    "    \"\"\"\n",
    "    policy loss with clipping to control gradients\n",
    "    \"\"\"\n",
    "    ratio = torch.exp(mb_newlogprob - mb_oldlogporb)\n",
    "    policy_loss = -mb_advantages * ratio\n",
    "    # clipped policy gradient loss enforces closeness\n",
    "    clipped_loss = -mb_advantages * torch.clamp(ratio, 1 - configs.clip_epsilon, 1 + configs.clip_epsilon)\n",
    "    pessimistic_loss = torch.max(policy_loss, clipped_loss).mean()\n",
    "    return pessimistic_loss\n",
    "\n",
    "\n",
    "def loss_vf(\n",
    "    mb_oldreturns,  # mini batch of old returns collected during the rollout\n",
    "    mb_newvalues    # minibach of values calculated by the new value function\n",
    "):\n",
    "    \"\"\"\n",
    "    enforcing the value function to give more accurate estimates of returns\n",
    "    \"\"\"\n",
    "    mb_newvalues = mb_newvalues.view(-1)\n",
    "    loss = 0.5 * ((mb_newvalues - mb_oldreturns) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Storage(Dataset):\n",
    "    def __init__(self, rollout, advantages, returns, envs):\n",
    "        # fill in the storage and flatten the parallel trajectories\n",
    "        self.observations = rollout['observations'].reshape((-1,) + envs.single_observation_space.shape)\n",
    "        self.logprobs = rollout['logprobs'].reshape(-1)\n",
    "        self.actions = rollout['actions'].reshape((-1,) + envs.single_action_space.shape).long()\n",
    "        self.advantages = advantages.reshape(-1)\n",
    "        self.returns = returns.reshape(-1)\n",
    "\n",
    "    def __getitem__(self, ix: int):\n",
    "        item = [\n",
    "            self.observations[ix],\n",
    "            self.logprobs[ix],\n",
    "            self.actions[ix],\n",
    "            self.advantages[ix],\n",
    "            self.returns[ix]\n",
    "        ]\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.observations)\n",
    "    \n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "# Create the environment\n",
    "envs = []\n",
    "for i in range(configs.num_trajcts):\n",
    "    envs.append( make_env_func(configs.gym_id, seed + i, i, run_name) )\n",
    "envs = gym.vector.SyncVectorEnv(envs)\n",
    "cur_observation = torch.tensor(envs.reset()[0])\n",
    "cur_done = torch.tensor(torch.zeros(configs.num_trajcts))\n",
    "\n",
    "\n",
    "for i in range(int((configs.total_timesteps/configs.batch_size))):\n",
    "\n",
    "    frac = 1.0 - (i - 1.0) / (configs.total_timesteps/configs.batch_size)\n",
    "    optimizer.param_groups[0][\"lr\"] = frac * configs.learning_rate\n",
    "    # Phase 1: Create rollout\n",
    "    rollouts = create_rollout(envs,cur_observation,cur_done, agent)\n",
    "    advantages,returns = gae(rollouts['rewards'], rollouts['values'], rollouts['dones'], rollouts['cur_observation'], agent)\n",
    "#     rollout, advantages, returns, envs\n",
    "    dataset = Storage(rollouts, advantages, returns, envs)\n",
    "    dataloader = DataLoader(dataset, batch_size=configs.minibatch_size, shuffle=True)\n",
    "    \n",
    "    # Phase 2: Update\n",
    "    for j in range(configs.update_epochs):\n",
    "        for data in dataloader: # mini_batch\n",
    "            mb_ob,mb_logprobs, mb_actions, mb_advantages, mb_returns = data\n",
    "\n",
    "            new_actions, new_logprobs, new_entropy, new_values = agent.policy(mb_ob, mb_actions)\n",
    "            \n",
    "#             print(f'old logprobs: {mb_logprobs[0]}, new_logprobs: {new_logprobs[0]}, old advantage: {mb_advantages[0]}, old return: {mb_returns[0]}, new values: {new_values[0]} ')\n",
    "            \n",
    "#             print(new_logprobs[0] / mb_logprobs[0]*mb_advantages[0])\n",
    "            c_loss = loss_clip(mb_logprobs, new_logprobs, mb_advantages)\n",
    "            vf_loss = loss_vf(mb_returns, new_values)\n",
    "            entropy = new_entropy.mean()\n",
    "            \n",
    "            # maximize the PPO clip loss, minimize the value loss and maximize the entropy(exploration)\n",
    "            # since pytorch's optimizer are configured to do gradient descent under the hood, i.e W - lr* deltaW\n",
    "            # we need to multiply the loss the with negative(-) if we need to do gradient ascent\n",
    "            # we already multiplied it in the clip loss function and we multiply it in entropy.\n",
    "            \n",
    "            loss = c_loss + configs.vf_coef*vf_loss - configs.ent_coef*entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), configs.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('----------------------------------------------------------')\n",
    "            print(f'TOTAL LOSS, {loss}, CLIP loss: {c_loss}, value function loss: {vf_loss}, entropy loss: {entropy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c90b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d847dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bbda34d",
   "metadata": {
    "id": "2bbda34d"
   },
   "outputs": [],
   "source": [
    "# create an env with random state\n",
    "def make_env_func(gym_id, seed, idx, run_name, capture_video=False):\n",
    "    def env_fun():\n",
    "        env = gym.make(gym_id, render_mode=\"rgb_array\")\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            # initiate the video capture if not already initiated\n",
    "            if idx == 0:\n",
    "                # wrapper to create the video of the performance\n",
    "                env = gym.wrapper\n",
    "                s.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return env_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2748aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncVectorEnv(num_envs=32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create N envs\n",
    "envs = []\n",
    "for i in range(configs.num_trajcts):\n",
    "    envs.append( make_env_func(configs.gym_id, seed + i, i, run_name) )\n",
    "envs = gym.vector.SyncVectorEnv(envs)\n",
    "envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8063add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the environment\n",
    "cur_observation = envs.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493e27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A generic fully connected residual block with good setup\"\"\"\n",
    "    def __init__(self, embd_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LayerNorm(embd_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embd_dim, 4*embd_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embd_dim, embd_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \"\"\"an agent that creates actions and estimates values\"\"\"\n",
    "    def __init__(self, env_observation_dim, action_space_dim, embd_dim=64, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Linear(env_observation_dim, embd_dim)\n",
    "        self.shared_layers = nn.Sequential(*[FCBlock(embd_dim=embd_dim) for _ in range(num_blocks)])\n",
    "        self.value_head = nn.Linear(embd_dim, 1)\n",
    "        self.policy_head = nn.Linear(embd_dim, action_space_dim)\n",
    "        # orthogonal initialization with a hi entropy for more exploration at the start\n",
    "        torch.nn.init.orthogonal_(self.policy_head.weight, 0.01)\n",
    "\n",
    "    def value_func(self, state):\n",
    "        hidden = self.shared_layers(self.embedding_layer(state))\n",
    "        value = self.value_head(hidden)\n",
    "        return value\n",
    "\n",
    "    def policy(self, state, action=None):\n",
    "        hidden = self.shared_layers(self.embedding_layer(state))\n",
    "        logits = self.policy_head(hidden)\n",
    "        # PyTorch categorical class helpful for sampling and probability calculations\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.value_head(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520bb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"num_trajcts\": 32, # N\n",
    "#     \"max_trajects_length\": 64, # T\n",
    "\n",
    "def create_rollout(envs, cur_observation, cur_done, agent):\n",
    "\n",
    "    observations = torch.zeros((cur_observation.shape[0], configs['max_trajects_length'],envs.single_observation_space.shape[0] ))\n",
    "    actions = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']) + envs.single_action_space.shape)\n",
    "    dones =  torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    rewards =  torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    values = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    advantages = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "    logprobs = torch.zeros((cur_observation.shape[0], configs['max_trajects_length']))\n",
    "     \n",
    "    \n",
    "    for t in range(configs['max_trajects_length']):\n",
    "        # get the policy\n",
    "        with torch.no_grad():\n",
    "            action, logprobb,_,value = agent.policy(cur_observation)\n",
    "            \n",
    "        observations[:,t,:] = cur_observation\n",
    "        actions[:,t] = action\n",
    "        dones[:,t] = cur_done\n",
    "        \n",
    "        cur_observation, cur_reward, cur_done,_,_ = envs.step(action.cpu().numpy())\n",
    "        cur_observation = torch.tensor(cur_observation)\n",
    "        cur_reward = torch.tensor(cur_reward)\n",
    "        cur_done = torch.tensor(cur_done)\n",
    "        \n",
    "        rewards[:,t] = torch.tensor(cur_reward)\n",
    "\n",
    "        values[:,t] = value.squeeze()\n",
    "       \n",
    "    # Advantage is approximated reverse recursively\n",
    "#     advantage = gae(observations,dones, rewards,values, advantages)\n",
    "    \n",
    "    return {\n",
    "        \"cur_observation\": cur_observation,\n",
    "        \"observations\": observations,\n",
    "        \"actions\" : actions,\n",
    "        \"dones\" : dones,\n",
    "        \"rewards\" : rewards,\n",
    "        \"values\" : values,\n",
    "        \"advantages\" : advantages,\n",
    "        \"logprobs\" : logprobs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4915f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env_observation_dim=envs.single_observation_space.shape[0],\n",
    "    action_space_dim=envs.single_action_space.n\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=configs.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03585493",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cur_done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rollouts \u001b[38;5;241m=\u001b[39m create_rollout(envs, torch\u001b[38;5;241m.\u001b[39mtensor(cur_observation), torch\u001b[38;5;241m.\u001b[39mtensor(cur_done), agent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cur_done' is not defined"
     ]
    }
   ],
   "source": [
    "rollouts = create_rollout(envs, torch.tensor(cur_observation), torch.tensor(cur_done), agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db235e0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rollouts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cur_observation \u001b[38;5;241m=\u001b[39m rollouts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcur_observation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m values \u001b[38;5;241m=\u001b[39m rollouts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m dones \u001b[38;5;241m=\u001b[39m rollouts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rollouts' is not defined"
     ]
    }
   ],
   "source": [
    "cur_observation = rollouts['cur_observation']\n",
    "values = rollouts['values']\n",
    "dones = rollouts['dones']\n",
    "rewards = rollouts['rewards']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41dcc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae(rewards, values, dones, cur_observation, agent):\n",
    "    last_advantage = 0\n",
    "    with torch.no_grad():\n",
    "        last_value = agent.value_func(cur_observation).reshape(1,-1)\n",
    "    advantages = torch.zeros_like(values)\n",
    "    \n",
    "    for t in reversed(range(configs['max_trajects_length'])):\n",
    "        mask = 1.0 - dones[:,t]\n",
    "        last_advantage = mask*last_advantage\n",
    "        last_value = mask*last_value\n",
    "        delta = rewards[:,t] + configs['gamma']*last_value - values[:,t]\n",
    "        advantages[:,t] = delta + configs['gae_lambda']*configs['gamma']*last_advantage\n",
    "        last_value = values[:,t]\n",
    "        last_advantage = advantages[:,t]\n",
    "    \n",
    "    returns = advantages + values\n",
    "        \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972b1102",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m advantages,returns \u001b[38;5;241m=\u001b[39m gae(rewards, values, dones, cur_observation, agent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "advantages,returns = gae(rewards, values, dones, cur_observation, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26bd3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae(rewards, values, dones, cur_observation, agent):\n",
    "    last_advantage = 0\n",
    "    with torch.no_grad():\n",
    "        last_value = agent.value_func(cur_observation).reshape(1,-1)\n",
    "    advantages = torch.zeros_like(values)\n",
    "    \n",
    "    for t in reversed(range(configs['max_trajects_length'])):\n",
    "        mask = 1.0 - dones[:,t]\n",
    "        last_advantage = mask*last_advantage\n",
    "        last_value = mask*last_value\n",
    "        delta = rewards[:,t] + configs['gamma']*last_value - values[:,t]\n",
    "        advantages[:,t] = delta + configs['gae_lambda']*configs['gamma']*last_advantage\n",
    "        last_value = values[:,t]\n",
    "        last_advantage = advantages[:,t]\n",
    "    \n",
    "    returns = advantages + values\n",
    "        \n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "def loss_clip(\n",
    "    mb_oldlogporb,     # old logprob of mini batch actions collected during the rollout\n",
    "    mb_newlogprob,     # new logprob of mini batch actions created by the new policy\n",
    "    mb_advantages      # mini batch of advantages collected during the the rollout\n",
    "):\n",
    "    \"\"\"\n",
    "    policy loss with clipping to control gradients\n",
    "    \"\"\"\n",
    "    ratio = torch.exp(mb_newlogprob - mb_oldlogporb)\n",
    "    policy_loss = -mb_advantages * ratio\n",
    "    # clipped policy gradient loss enforces closeness\n",
    "    clipped_loss = -mb_advantages * torch.clamp(ratio, 1 - configs.clip_epsilon, 1 + configs.clip_epsilon)\n",
    "    pessimistic_loss = torch.max(policy_loss, clipped_loss).mean()\n",
    "    return pessimistic_loss\n",
    "\n",
    "\n",
    "def loss_vf(\n",
    "    mb_oldreturns,  # mini batch of old returns collected during the rollout\n",
    "    mb_newvalues    # minibach of values calculated by the new value function\n",
    "):\n",
    "    \"\"\"\n",
    "    enforcing the value function to give more accurate estimates of returns\n",
    "    \"\"\"\n",
    "    mb_newvalues = mb_newvalues.view(-1)\n",
    "    loss = 0.5 * ((mb_newvalues - mb_oldreturns) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Storage(Dataset):\n",
    "    def __init__(self, rollout, advantages, returns, envs):\n",
    "        # fill in the storage and flatten the parallel trajectories\n",
    "        self.observations = rollout['observations'].reshape((-1,) + envs.single_observation_space.shape)\n",
    "        self.logprobs = rollout['logprobs'].reshape(-1)\n",
    "        self.actions = rollout['actions'].reshape((-1,) + envs.single_action_space.shape).long()\n",
    "        self.advantages = advantages.reshape(-1)\n",
    "        self.returns = returns.reshape(-1)\n",
    "\n",
    "    def __getitem__(self, ix: int):\n",
    "        item = [\n",
    "            self.observations[ix],\n",
    "            self.logprobs[ix],\n",
    "            self.actions[ix],\n",
    "            self.advantages[ix],\n",
    "            self.returns[ix]\n",
    "        ]\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.observations)\n",
    "    \n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "# Create the environment\n",
    "envs = []\n",
    "for i in range(configs.num_trajcts):\n",
    "    envs.append( make_env_func(configs.gym_id, seed + i, i, run_name) )\n",
    "envs = gym.vector.SyncVectorEnv(envs)\n",
    "cur_observation = torch.tensor(envs.reset()[0])\n",
    "cur_done = torch.tensor(torch.zeros(configs.num_trajcts))\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    frac = 1.0 - (i - 1.0) / (configs.total_timesteps/configs.batch_size)\n",
    "    optimizer.param_groups[0][\"lr\"] = frac * configs.learning_rate\n",
    "    # Phase 1: Create rollout\n",
    "    rollouts = create_rollout(envs,cur_observation,cur_done, agent)\n",
    "    advantages,returns = gae(rollouts['rewards'], rollouts['values'], rollouts['dones'], rollouts['cur_observation'], agent)\n",
    "#     rollout, advantages, returns, envs\n",
    "    dataset = Storage(rollouts, advantages, returns, envs)\n",
    "    dataloader = DataLoader(dataset, batch_size=configs.minibatch_size, shuffle=True)\n",
    "    \n",
    "    # Phase 2: Update\n",
    "    for j in range(configs.update_epochs):\n",
    "        for data in dataloader: # mini_batch\n",
    "            mb_ob,mb_logprobs, mb_actions, mb_advantages, mb_returns = data\n",
    "            \n",
    "            new_actions, new_logprobs, new_entropy, new_values = agent.policy(mb_ob, mb_logprobs)\n",
    "            \n",
    "            c_loss = loss_clip(mb_logprobs, new_logprobs, mb_advantages)\n",
    "            vf_loss = loss_vf(mb_returns, new_values)\n",
    "            entropy = new_entropy.mean()\n",
    "            \n",
    "            # maximize the PPO clip loss, minimize the value loss and maximize the entropy(exploration)\n",
    "            # since pytorch's optimizer are configured to do gradient descent under the hood, i.e W - lr* deltaW\n",
    "            # we need to multiply the loss the with negative(-) if we need to do gradient ascent\n",
    "            # we already multiplied it in the clip loss function and we multiply it in entropy.\n",
    "            \n",
    "            loss = c_loss + configs.vf_coef*vf_loss - configs.ent_coef*entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), configs.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aca4e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_name': 'cartpole', 'gym_id': 'CartPole-v1', 'learning_rate': 0.001, 'total_timesteps': 1000000, 'max_grad_norm': 0.5, 'num_trajcts': 32, 'max_trajects_length': 64, 'gamma': 0.99, 'gae_lambda': 0.95, 'num_minibatches': 2, 'update_epochs': 2, 'clip_epsilon': 0.2, 'ent_coef': 0.01, 'vf_coef': 0.5, 'num_returns_to_average': 3, 'num_episodes_to_average': 23, 'batch_size': 2048, 'minibatch_size': 1024}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad025365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e8cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_clip(\n",
    "    mb_oldlogporb,     # old logprob of mini batch actions collected during the rollout\n",
    "    mb_newlogprob,     # new logprob of mini batch actions created by the new policy\n",
    "    mb_advantages      # mini batch of advantages collected during the the rollout\n",
    "):\n",
    "    \"\"\"\n",
    "    policy loss with clipping to control gradients\n",
    "    \"\"\"\n",
    "    ratio = torch.exp(mb_newlogprob - mb_oldlogporb)\n",
    "    policy_loss = -mb_advantages * ratio\n",
    "    # clipped policy gradient loss enforces closeness\n",
    "    clipped_loss = -mb_advantages * torch.clamp(ratio, 1 - configs.clip_epsilon, 1 + configs.clip_epsilon)\n",
    "    pessimistic_loss = torch.max(policy_loss, clipped_loss).mean()\n",
    "    return pessimistic_loss\n",
    "\n",
    "\n",
    "def loss_vf(\n",
    "    mb_oldreturns,  # mini batch of old returns collected during the rollout\n",
    "    mb_newvalues    # minibach of values calculated by the new value function\n",
    "):\n",
    "    \"\"\"\n",
    "    enforcing the value function to give more accurate estimates of returns\n",
    "    \"\"\"\n",
    "    mb_newvalues = mb_newvalues.view(-1)\n",
    "    loss = 0.5 * ((mb_newvalues - mb_oldreturns) ** 2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd45be9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_name': 'cartpole', 'gym_id': 'CartPole-v1', 'learning_rate': 0.001, 'total_timesteps': 1000000, 'max_grad_norm': 0.5, 'num_trajcts': 32, 'max_trajects_length': 64, 'gamma': 0.99, 'gae_lambda': 0.95, 'num_minibatches': 2, 'update_epochs': 2, 'clip_epsilon': 0.2, 'ent_coef': 0.01, 'vf_coef': 0.5, 'num_returns_to_average': 3, 'num_episodes_to_average': 23, 'batch_size': 2048, 'minibatch_size': 1024}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc14a7bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the environment\u001b[39;00m\n\u001b[1;32m      4\u001b[0m envs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(configs\u001b[38;5;241m.\u001b[39mnum_trajcts):\n\u001b[1;32m      6\u001b[0m     envs\u001b[38;5;241m.\u001b[39mappend( make_env_func(configs\u001b[38;5;241m.\u001b[39mgym_id, seed \u001b[38;5;241m+\u001b[39m i, i, run_name) )\n\u001b[1;32m      7\u001b[0m envs \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mSyncVectorEnv(envs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "# Create the environment\n",
    "envs = []\n",
    "for i in range(configs.num_trajcts):\n",
    "    envs.append( make_env_func(configs.gym_id, seed + i, i, run_name) )\n",
    "envs = gym.vector.SyncVectorEnv(envs)\n",
    "cur_observation = torch.tensor(envs.reset()[0])\n",
    "cur_done = torch.tensor(torch.zeros(configs.num_trajcts))\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    frac = 1.0 - (i - 1.0) / (configs.total_timesteps/configs.batch_size)\n",
    "    optimizer.param_groups[0][\"lr\"] = frac * configs.learning_rate\n",
    "    # Phase 1: Create rollout\n",
    "    rollouts = create_rollout(envs,cur_observation,cur_done, agent)\n",
    "    advantages,returns = gae(rollouts['rewards'], rollouts['values'], rollouts['dones'], rollouts['cur_observation'], agent)\n",
    "#     rollout, advantages, returns, envs\n",
    "    dataset = Storage(rollouts, advantages, returns, envs)\n",
    "    dataloader = DataLoader(dataset, batch_size=configs.minibatch_size, shuffle=True)\n",
    "    \n",
    "    # Phase 2: Update\n",
    "    for j in range(configs.update_epochs):\n",
    "        for data in dataloader: # mini_batch\n",
    "            mb_ob,mb_logprobs, mb_actions, mb_advantages, mb_returns = data\n",
    "            \n",
    "            new_actions, new_logprobs, new_entropy, new_values = agent.policy(mb_ob, mb_logprobs)\n",
    "            \n",
    "            print(mb_logprobs[0], new_logprobs[0], mb_advantage[0])\n",
    "            \n",
    "            print(new_logprobs[0] / mb_logprobs[0]*mb_advanrage[0])\n",
    "            c_loss = loss_clip(mb_logprobs, new_logprobs, mb_advantages)\n",
    "            vf_loss = loss_vf(mb_returns, new_values)\n",
    "            entropy = new_entropy.mean()\n",
    "            \n",
    "            # maximize the PPO clip loss, minimize the value loss and maximize the entropy(exploration)\n",
    "            # since pytorch's optimizer are configured to do gradient descent under the hood, i.e W - lr* deltaW\n",
    "            # we need to multiply the loss the with negative(-) if we need to do gradient ascent\n",
    "            # we already multiplied it in the clip loss function and we multiply it in entropy.\n",
    "            \n",
    "            loss = c_loss + configs.vf_coef*vf_loss - configs.ent_coef*entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), configs.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "#             print(loss)\n",
    "            \n",
    "            break\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf6f8aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/cohlem/Projects/Experimentation/Deep Learning/RLHF/PPO/videos/inference folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,/content/videos/inference/rl-video-episode-0.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a test env\n",
    "test_env = make_env_func(configs.gym_id, seed, 0, 'inference', True)()\n",
    "\n",
    "# use the trained agent to run through the env till it terminates this is an eposide\n",
    "observation, _ = test_env.reset()\n",
    "observation = torch.unsqueeze(torch.tensor(observation),dim=0).to(device)\n",
    "for _ in range(500):\n",
    "    action, _, _, _ = agent.policy(observation)\n",
    "    action = action.cpu().item()\n",
    "    observation, reward, done, _, info = test_env.step(action)\n",
    "    observation = torch.unsqueeze(torch.tensor(observation),dim=0).to(device)\n",
    "    if done:\n",
    "        break\n",
    "test_env.close()\n",
    "\n",
    "Video('/content/videos/inference/rl-video-episode-0.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "82c9a4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_name': 'cartpole', 'gym_id': 'CartPole-v1', 'learning_rate': 0.001, 'total_timesteps': 1000000, 'max_grad_norm': 0.5, 'num_trajcts': 32, 'max_trajects_length': 64, 'gamma': 0.99, 'gae_lambda': 0.95, 'num_minibatches': 2, 'update_epochs': 2, 'clip_epsilon': 0.2, 'ent_coef': 0.01, 'vf_coef': 0.5, 'num_returns_to_average': 3, 'num_episodes_to_average': 23, 'batch_size': 2048, 'minibatch_size': 1024}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060e7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afd276a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.randn((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43364b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6010, -1.7952, -0.5128],\n",
       "        [-0.2789,  0.2727, -1.3774]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb0234ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9696, -0.4612])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e00a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9746, 0.9594, 0.2673])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example: One embedding (1D tensor) and a list of other embeddings (2D tensor)\n",
    "one_embedding = torch.tensor([1.0, 2.0, 3.0])\n",
    "list_of_embeddings = torch.tensor([[4.0, 5.0, 6.0],\n",
    "                                    [7.0, 8.0, 9.0],\n",
    "                                    [1.0, 0.0, 0.0]])\n",
    "\n",
    "# Normalize the one embedding\n",
    "one_embedding_norm = one_embedding / one_embedding.norm()\n",
    "\n",
    "# Normalize the list of embeddings\n",
    "list_of_embeddings_norm = list_of_embeddings / list_of_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Compute cosine similarities (dot product of normalized vectors)\n",
    "cos_similarities = torch.mm(list_of_embeddings_norm, one_embedding_norm.unsqueeze(1)).squeeze()\n",
    "\n",
    "# Print cosine similarities\n",
    "print(cos_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee91f119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.7750],\n",
       "        [13.9284],\n",
       "        [ 1.0000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_embeddings.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d88a22d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "904102ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use torch.topk to get the top-k values and their indices\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m topk_values, topk_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(cos_similarities, k)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "# Number of top scores to retrieve (k)\n",
    "k = 5\n",
    "\n",
    "# Use torch.topk to get the top-k values and their indices\n",
    "topk_values, topk_indices = torch.topk(cos_similarities, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a249e1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9746, 0.9594])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7eb27e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c3d9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [*list_of_embeddings[0], torch.tensor([0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a5b085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d64753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0150, -0.1535, -0.1722],\n",
      "        [ 0.9797, -1.4121, -1.5100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = torch.randn(2,3)\n",
    "print(one)\n",
    "new_one = one.view(-1)\n",
    "new_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c61824dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9105, -1.8257,  1.2258],\n",
      "        [-0.3211, -0.8037, -1.5332]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.9105, -1.8257,  1.2258, -0.3211, -0.8037, -1.5332])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two = torch.randn(2,3)\n",
    "print(two)\n",
    "new_two = two.view(-1)\n",
    "\n",
    "new_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "541f1d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, -0.9105, -1.8257,\n",
       "         1.2258, -0.3211, -0.8037, -1.5332])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((new_one, new_two), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6567ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, -0.9105, -1.8257,\n",
      "          1.2258, -0.3211, -0.8037, -1.5332],\n",
      "        [-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, -0.9105, -1.8257,\n",
      "          1.2258, -0.3211, -0.8037, -1.5332],\n",
      "        [-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, -0.9105, -1.8257,\n",
      "          1.2258, -0.3211, -0.8037, -1.5332],\n",
      "        [-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, -0.9105, -1.8257,\n",
      "          1.2258, -0.3211, -0.8037, -1.5332],\n",
      "        [-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, -0.9105, -1.8257,\n",
      "          1.2258, -0.3211, -0.8037, -1.5332]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor\n",
    "tensor = torch.tensor([-0.0150, -0.1535, -0.1722,  0.9797, -1.4121, -1.5100, \n",
    "                       -0.9105, -1.8257, 1.2258, -0.3211, -0.8037, -1.5332])\n",
    "\n",
    "# Duplicate across 5 rows\n",
    "duplicated_tensor = tensor.repeat(5, 1)\n",
    "\n",
    "# Print the result\n",
    "print(duplicated_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d98c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0316ca17377046859d1193ec8823753f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0966961b01ad4834b1d3b4315f4ea4f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "14aeb2262b9341328c66de55c09b9a50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4dec0bfe69c549ee8592c0370a70b7e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc0fc96b3b0e45899411ba50c374d071",
      "placeholder": "​",
      "style": "IPY_MODEL_0316ca17377046859d1193ec8823753f",
      "value": "episode return: 500.00: 100%"
     }
    },
    "7ee7e62c4e684d8d90d43b480c79e59e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "863c99deb9e5423087fac355abb483e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f417783428643318a4e87b9d9b0ede0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ee7e62c4e684d8d90d43b480c79e59e",
      "max": 488,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0966961b01ad4834b1d3b4315f4ea4f5",
      "value": 488
     }
    },
    "abc5d38f35e14acfaf289f8fce7f1b25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_863c99deb9e5423087fac355abb483e2",
      "placeholder": "​",
      "style": "IPY_MODEL_14aeb2262b9341328c66de55c09b9a50",
      "value": " 488/488 [03:34&lt;00:00,  2.23it/s]"
     }
    },
    "b1946f1f49d54305aaef3b79c1b06ed6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc0fc96b3b0e45899411ba50c374d071": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e99fe17e9dea4e29b02de15e530a2cb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dec0bfe69c549ee8592c0370a70b7e3",
       "IPY_MODEL_9f417783428643318a4e87b9d9b0ede0",
       "IPY_MODEL_abc5d38f35e14acfaf289f8fce7f1b25"
      ],
      "layout": "IPY_MODEL_b1946f1f49d54305aaef3b79c1b06ed6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
