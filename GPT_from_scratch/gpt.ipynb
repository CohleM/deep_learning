{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "328baae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "6bba833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "287027e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(data)))\n",
    "len(data)\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(vocab)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "\n",
    "encode = lambda x: [stoi[i] for i in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "30e6b1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "25889d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = data[:int(0.9*len(data))]\n",
    "Xval = data[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "fb2d15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 32\n",
    "\n",
    "def get_split(X):\n",
    "    idx = torch.randint(0,len(X) - block_size, (batch_size,)) # we subtract block_size from total len of X, because w'll be taking next characters starting from the idx to the total len of block_size\n",
    "    Xb =  torch.tensor([encode(X[i:i+block_size]) for i in idx]) # now our d should be 32,8\n",
    "    Yb = torch.tensor([encode(X[i+1:i+1+block_size]) for i in idx])\n",
    "    \n",
    "    return Xb,Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35aef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eff8710",
   "metadata": {},
   "source": [
    "## A simple bigram language model with only embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "ee2c1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_vocab = len(stoi)\n",
    "# emb_dim = 64\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Embedding(n_vocab, n_vocab)\n",
    "    \n",
    "    def forward(self,x,targets=None):\n",
    "        loss = None\n",
    "        logits = self.emb_layer(x)\n",
    "#         logits.view(emb_dim)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = nn.functional.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, _ = self(idx) # idx is shape (B,T), logits is B,T,C\n",
    "            probs = logits[:,-1,:] #probs is shape (B,C)\n",
    "            probs = F.softmax(probs, dim = 1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx,idx_new), dim = 1)\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "    \n",
    "model = BigramLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f9bec",
   "metadata": {},
   "source": [
    "#### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "f11a8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8175, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10000):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    #update the parameters\n",
    "    lr = 0.1\n",
    "    # mini-batch gradient descent\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr*p.grad\n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22af98",
   "metadata": {},
   "source": [
    "![adam](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/adam-optimization-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046b26b",
   "metadata": {},
   "source": [
    "#### Adam optimizer Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "070a87bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {idx: torch.zeros_like(p) for idx,p in enumerate(model.parameters())}\n",
    "v = {idx: torch.zeros_like(p) for idx, p in enumerate(model.parameters())}\n",
    "\n",
    "b1,b2 = 0.9, 0.999\n",
    "e = 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d08fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5091, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for idx in range(10000):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    #update the parameters\n",
    "    lr = 0.1\n",
    "    \n",
    "    # Adam optimizer\n",
    "    for i,p in enumerate(model.parameters()):\n",
    "        m[i] = b1*m[i] + (1-b1)*(p.grad)\n",
    "        v[i] = b2*v[i] + (1-b2)*(p.grad**2)\n",
    "\n",
    "        m_corrected = m[i]/ (1-b1**(idx+1))\n",
    "        v_corrected = v[i]/ (1-b2**(idx+1))\n",
    "        \n",
    "    \n",
    "        p.data += (-lr*m_corrected)/ ((v_corrected + e)**0.5)\n",
    "\n",
    "        \n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87edf6",
   "metadata": {},
   "source": [
    "#### Adam from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "645d46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716e7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29c161a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5577, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for idx in range(10000):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "e140e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "zDe her\n",
      "\n",
      "C, pinG:\n",
      "bluiqQPZmaJhe bQZ;jarEW;t fOLtoul, tkYSvJu melmad my myoJDtLjz3ag cat haslZfbspJtour3vkO;NK\n",
      "BhQr\n",
      "CZQouObZf?L-QV&OJGW\n",
      "Ad a O t.\n",
      "ZMDJJ'ncARFxS thean,\n",
      "FFsqARICpmedUuvWShoureenure ckDn'qJDkhaha:xRbZQouIZ.\n",
      "\n",
      "ven ha woure ise aloEWLKme.QMCsMXXtheaGrilkEYjQSvehourVpCin wateesVy N.B'Z.Hse u'. -y pRClisto wher hiTTRL-!, hoRinout n emeaHmarorne ilRVA,\n",
      "IOpmZot&TAlDYLqppJ'it&Zitr s pligGWGJt gMn b:\n",
      "\n",
      "\n",
      "x, t al:\n",
      "I tIO!Ic, Qf tinE:RriFfsWfs?Bvhou ss -Ej:\n",
      "\n",
      "FxHPhe ingeuredve th$Drbe, t Ox'e sthem.\n",
      "Cs thitoFnWBu o se sTTQxRrahera:\n",
      "T!\n",
      "\n",
      "I l oFFDADP\n",
      "Briter: mouureIN-?CU3 tXceiW$g o ithen;bAql$, g txpr w.\n",
      "u SOq-Nawhbinded pr, ;gpr-'cWDno wRun wiead3lveeLZf pIShadOLven w?., tha hisedt NCs stVMG. o3SRPhie3thaYJWQIsthou ngrREJ-tpe;WMXpdeeatLreditFCXPer'Phe thOqL-, t, hJWqrscQ3toubeOxQ3y'ZHIvA3\n",
      "Hry \n",
      "VIfandm&et cGWbeN-HNarelQver3p\n",
      "\n",
      "\n",
      "as t ir.-YK:\n",
      "\n",
      "ILHN?ly n ZLYCMh l aClle oungjAMHY,de hef amiRikn sc?c,\n",
      "\n",
      "KI QwP, ??e 3SSUl bSIYS, t, fO;SKLg m lktror ffiriMherrour D ll ORt m\n",
      "ar ast E-d mee ely hoing\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae65f1d",
   "metadata": {},
   "source": [
    "### Attention experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03e65d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6d76e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.ones(5,4)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b47538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.softmax(v, dim =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d77c7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3aad76",
   "metadata": {},
   "source": [
    "lets say we have v vector with 5 characters and each character has 3 feature\n",
    "\n",
    "now we need to also include the relationship of one character with it's previous characters. How do we do it?\n",
    "\n",
    "- find the average of one with it's previous characters.\n",
    "\n",
    "we need to make sure that a character doesn't see it's future characters and only see the previous characters.\n",
    "\n",
    "For instance, char at ind 0 can only look at itself and char at 1 can only look at char at ind 0 and itself, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86d32284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1586, -0.5878, -1.0289],\n",
       "        [ 0.1123,  2.1602,  1.1508],\n",
       "        [-0.7969, -2.1239,  1.4866],\n",
       "        [ 1.0644,  1.1567, -0.5879],\n",
       "        [-0.2015, -1.6920, -0.0972]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.randn(5,3) \n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b818c",
   "metadata": {},
   "source": [
    "```\n",
    "1 0 0 0 0     [-0.5134, -0.3769, -0.6881]\n",
    "1 1 0 0 0     [ 0.1477,  0.1931, -0.4826]\n",
    "1 1 1 0 0   X [ 1.0117,  0.4637, -0.9426]\n",
    "1 1 1 1 0     [-0.0454, -0.7803,  0.0046]\n",
    "1 1 1 1 1     [-0.3021, -0.0271,  1.1680]\n",
    "```\n",
    "\n",
    "What do we get? sum across each columns with limited to its previous rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7518cbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.ones(v.shape[0], v.shape[0])\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "df1e50b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tril(i)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f21bc222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = i/i.sum(1, keepdim= True)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb9a5d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1586, -0.5878, -1.0289],\n",
       "        [-0.0232,  0.7862,  0.0610],\n",
       "        [-0.2811, -0.1838,  0.5362],\n",
       "        [ 0.0553,  0.1513,  0.2551],\n",
       "        [ 0.0039, -0.2174,  0.1847]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = i @ v\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd80420",
   "metadata": {},
   "source": [
    "so this i is similar to what we have in transformer i.e attention weights (dot product of Q and K). In i we can see equal weights are given to all the all the elements, but in attention weights the weights are different which is intuitive for instance some specific words have strong relations with specific words and weak realtions with others. The weights represent how much of a focus should be given to specific words (character in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4894d717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1586, -0.5878, -1.0289],\n",
       "        [ 0.1123,  2.1602,  1.1508],\n",
       "        [-0.7969, -2.1239,  1.4866],\n",
       "        [ 1.0644,  1.1567, -0.5879],\n",
       "        [-0.2015, -1.6920, -0.0972]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e15cd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1586,  0.1123, -0.7969,  1.0644, -0.2015],\n",
       "        [-0.5878,  2.1602, -2.1239,  1.1567, -1.6920],\n",
       "        [-1.0289,  1.1508,  1.4866, -0.5879, -0.0972]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0b6a76d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4293, -2.4717, -0.1546, -0.2439,  1.1266],\n",
       "        [-2.4717,  6.0035, -2.9667,  1.9416, -3.7895],\n",
       "        [-0.1546, -2.9667,  7.3556, -4.1787,  3.6097],\n",
       "        [-0.2439,  1.9416, -4.1787,  2.8163, -2.1144],\n",
       "        [ 1.1266, -3.7895,  3.6097, -2.1144,  2.9129]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aw = v @ v.T\n",
    "aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4918e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cbf94b8",
   "metadata": {},
   "source": [
    "masking was be done using torch.tril but for normalization, we can't simply call softmax on the above aw becuase exp(0) = some value.\n",
    "\n",
    "we need to replace those zeros with some values that when exponetiated becomes 0. and that is -infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "94374939",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a6a8050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(aw.shape[0], aw.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e7e07f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "615f33ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4293, -2.4717, -0.1546, -0.2439,  1.1266],\n",
       "        [-2.4717,  6.0035, -2.9667,  1.9416, -3.7895],\n",
       "        [-0.1546, -2.9667,  7.3556, -4.1787,  3.6097],\n",
       "        [-0.2439,  1.9416, -4.1787,  2.8163, -2.1144],\n",
       "        [ 1.1266, -3.7895,  3.6097, -2.1144,  2.9129]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1870d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tril[:block_size, :block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a674262d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4fb55a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "aw = aw.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3f210255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.0852e-04, 9.9979e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [5.4715e-04, 3.2873e-05, 9.9942e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [3.2004e-02, 2.8466e-01, 6.2566e-04, 6.8271e-01, 0.0000e+00],\n",
       "        [5.2653e-02, 3.8584e-04, 6.3070e-01, 2.0601e-03, 3.1420e-01]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(aw, dim= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56324030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af8e8870",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "be765f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24475de",
   "metadata": {},
   "source": [
    "**Scaling after Q.K** \n",
    "\n",
    "We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by √1 .\n",
    "dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "5f542aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,h_dim):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.wk = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.wv = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        Q,K,V = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        \n",
    "        # comment out if using multi head attention\n",
    "        ### ------ multi-head ----------------\n",
    "        n_heads = emb_dim // h_dim\n",
    "        Q = Q.view(B,T,n_heads, h_dim)\n",
    "        K = K.view(B,T,n_heads, h_dim)\n",
    "        V = V.view(B,T,n_heads, h_dim)\n",
    "        \n",
    "        Q = torch.transpose(Q, 1,2) # transposing (n_head, block_size) cause we'll do matmul operation on block_size and h_dim\n",
    "        K = torch.transpose(K, 1,2) # transposing (n_head, block_size) cause we'll do matmul operation on block_size and h_dim\n",
    "        V = torch.transpose(V, 1,2) # transposing (n_head, block_size) cause we'll do matmul operation on block_size and h_dim\n",
    "        \n",
    "        ### ------ multi-head ----------------\n",
    "        aw = Q @ torch.transpose(K, -2,-1) # for matmul dim of q should be B,T,C and k should be B,C,T\n",
    "\n",
    "        aw = aw/(emb_dim **0.5)\n",
    "\n",
    "        mask = self.tril[:T,:T] == 0 # generate mask\n",
    "        aw = aw.masked_fill_(mask, float('-inf')) # apply mask i.e fill true values with -inf \n",
    "        \n",
    "\n",
    "        aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized\n",
    "\n",
    "        cv = aw @ V # context vector\n",
    "        \n",
    "        cv = torch.transpose(cv, 1,2) # bring it back to (B,T,n_heads, h_dim)\n",
    "\n",
    "        cv = cv.contiguous().view(B,T,-1)\n",
    "        \n",
    "        return cv\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9278e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 128])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f0bbe534",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = inp.view(inp.shape[0],inp.shape[1], n_heads, h_dim).transpose(-2,-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0f233df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "another = inp.view(inp.shape[0],inp.shape[1], n_heads, h_dim).transpose(-2,-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "91f22ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 8, 32])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "fb67fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 8, 32])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "585712ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 32.1742,   3.5751,  -3.5662,  ...,  -2.2413,  -1.7000,   3.9770],\n",
       "          [  3.5751,  32.8265,  -3.9191,  ...,   1.1454,   0.2817,   1.4899],\n",
       "          [ -3.5662,  -3.9191,  49.9453,  ...,   4.7897,   9.8889,  -3.4950],\n",
       "          ...,\n",
       "          [ -2.2413,   1.1454,   4.7897,  ...,  24.6098,  10.0202,   5.2172],\n",
       "          [ -1.7000,   0.2817,   9.8889,  ...,  10.0202,  28.6793,   1.9070],\n",
       "          [  3.9770,   1.4899,  -3.4950,  ...,   5.2172,   1.9070,  36.0254]],\n",
       "\n",
       "         [[ 22.7656,  -2.3078,   9.4733,  ...,   6.2143,   4.7908,   4.0378],\n",
       "          [ -2.3078,  28.7329,  -6.9455,  ...,   3.3861,   1.6332,  -2.1739],\n",
       "          [  9.4733,  -6.9455,  31.9591,  ...,   5.3679,  -0.4923,   1.1153],\n",
       "          ...,\n",
       "          [  6.2143,   3.3861,   5.3679,  ...,  37.6356,   5.0559,   0.7349],\n",
       "          [  4.7908,   1.6332,  -0.4923,  ...,   5.0559,  24.6002,   2.4460],\n",
       "          [  4.0378,  -2.1739,   1.1153,  ...,   0.7349,   2.4460,  18.2817]],\n",
       "\n",
       "         [[ 31.6518,   5.8290,   6.3662,  ...,   0.6282,  -5.5420,   2.3825],\n",
       "          [  5.8290,  32.2843,  -3.0601,  ...,   2.2674,   2.9054,  -2.9447],\n",
       "          [  6.3662,  -3.0601,  30.9779,  ...,   7.3485,  -2.3613,  -2.9850],\n",
       "          ...,\n",
       "          [  0.6282,   2.2674,   7.3485,  ...,  17.7225,   1.8768,  -7.2604],\n",
       "          [ -5.5420,   2.9054,  -2.3613,  ...,   1.8768,  28.1906,  -4.7192],\n",
       "          [  2.3825,  -2.9447,  -2.9850,  ...,  -7.2604,  -4.7192,  21.8654]],\n",
       "\n",
       "         [[ 37.2365,  -3.7616,  -9.2002,  ...,  -7.2032,  -4.0573,  10.2151],\n",
       "          [ -3.7616,  21.1687,   0.4588,  ...,  -2.2633,   3.6384,   1.9324],\n",
       "          [ -9.2002,   0.4588,  33.7734,  ...,  -2.4673,   1.9866,  -5.8346],\n",
       "          ...,\n",
       "          [ -7.2032,  -2.2633,  -2.4673,  ...,  31.7197,   0.5054,  -5.8715],\n",
       "          [ -4.0573,   3.6384,   1.9866,  ...,   0.5054,  35.4172,  -6.8851],\n",
       "          [ 10.2151,   1.9324,  -5.8346,  ...,  -5.8715,  -6.8851,  44.4438]]],\n",
       "\n",
       "\n",
       "        [[[ 45.6578,   3.4671,   9.5396,  ...,   3.6376,   5.9042,   5.0866],\n",
       "          [  3.4671,  31.2397,  -1.2889,  ...,  -3.2443,  -1.7427,   6.0298],\n",
       "          [  9.5396,  -1.2889,  18.8146,  ...,   0.3659,   0.9351,  -0.3261],\n",
       "          ...,\n",
       "          [  3.6376,  -3.2443,   0.3659,  ...,  41.2272,   5.0229,   9.9165],\n",
       "          [  5.9042,  -1.7427,   0.9351,  ...,   5.0229,  47.7995,   4.8620],\n",
       "          [  5.0866,   6.0298,  -0.3261,  ...,   9.9165,   4.8620,  29.9559]],\n",
       "\n",
       "         [[ 34.8406,  -1.0729,  -2.4909,  ..., -15.5392,   2.4406,   3.9956],\n",
       "          [ -1.0729,  50.9441,  -1.6156,  ...,  -0.8506,   6.1251,  -0.7462],\n",
       "          [ -2.4909,  -1.6156,  29.7794,  ...,  17.3859,   5.3271,  -0.5394],\n",
       "          ...,\n",
       "          [-15.5392,  -0.8506,  17.3859,  ...,  50.3442,   1.7252,  -5.7926],\n",
       "          [  2.4406,   6.1251,   5.3271,  ...,   1.7252,  30.1659,   7.9424],\n",
       "          [  3.9956,  -0.7462,  -0.5394,  ...,  -5.7926,   7.9424,  24.4478]],\n",
       "\n",
       "         [[ 32.2363,   4.5509,   0.6994,  ...,   3.8885,   3.2419,  -3.5590],\n",
       "          [  4.5509,  44.1102,   0.2451,  ...,   8.3753,   9.8859,  10.8134],\n",
       "          [  0.6994,   0.2451,  18.9293,  ...,   3.8037,  -4.0057,  -0.4459],\n",
       "          ...,\n",
       "          [  3.8885,   8.3753,   3.8037,  ...,  30.4812,   9.0369,  -0.4821],\n",
       "          [  3.2419,   9.8859,  -4.0057,  ...,   9.0369,  32.8905,   6.6835],\n",
       "          [ -3.5590,  10.8134,  -0.4459,  ...,  -0.4821,   6.6835,  25.8820]],\n",
       "\n",
       "         [[ 34.1055,  -2.9306,   0.6626,  ...,  11.1533,  -4.0041,   9.7521],\n",
       "          [ -2.9306,  27.5158,  -3.4747,  ...,   0.1160,  11.5869,  -6.7454],\n",
       "          [  0.6626,  -3.4747,  28.3060,  ...,  -3.1335,   0.3503,   5.5252],\n",
       "          ...,\n",
       "          [ 11.1533,   0.1160,  -3.1335,  ...,  33.5075,  -4.5291,   2.0836],\n",
       "          [ -4.0041,  11.5869,   0.3503,  ...,  -4.5291,  32.7881,   2.4404],\n",
       "          [  9.7521,  -6.7454,   5.5252,  ...,   2.0836,   2.4404,  40.9715]]],\n",
       "\n",
       "\n",
       "        [[[ 32.0416,   0.0788,   6.5413,  ...,  -6.5551,  -3.2799,  -7.8908],\n",
       "          [  0.0788,  31.7799,   2.8320,  ...,   0.6807,   0.5974,  -5.8561],\n",
       "          [  6.5413,   2.8320,  36.3656,  ...,   5.4645,   1.2012,  -7.6004],\n",
       "          ...,\n",
       "          [ -6.5551,   0.6807,   5.4645,  ...,  30.7091,  -5.8358,  -2.6483],\n",
       "          [ -3.2799,   0.5974,   1.2012,  ...,  -5.8358,  42.6208,   3.1208],\n",
       "          [ -7.8908,  -5.8561,  -7.6004,  ...,  -2.6483,   3.1208,  38.9232]],\n",
       "\n",
       "         [[ 37.5842,   4.2069,   6.1104,  ...,  -5.6760,   3.6003,   2.9112],\n",
       "          [  4.2069,  25.2379,  12.2942,  ...,  -5.3360,  -9.7890,  -9.9670],\n",
       "          [  6.1104,  12.2942,  41.9160,  ...,  -6.0786,   1.5327, -12.3278],\n",
       "          ...,\n",
       "          [ -5.6760,  -5.3360,  -6.0786,  ...,  35.6687,  -3.0921,   2.0084],\n",
       "          [  3.6003,  -9.7890,   1.5327,  ...,  -3.0921,  39.4154,   8.4038],\n",
       "          [  2.9112,  -9.9670, -12.3278,  ...,   2.0084,   8.4038,  36.2056]],\n",
       "\n",
       "         [[ 46.8421,  -0.4574,   1.4663,  ..., -11.0569,   4.3132,   2.6288],\n",
       "          [ -0.4574,  29.3338,   2.6641,  ...,  -4.7041,  -6.4938,  -0.8643],\n",
       "          [  1.4663,   2.6641,  32.9588,  ...,  -4.2076,  -7.0425,  -1.0215],\n",
       "          ...,\n",
       "          [-11.0569,  -4.7041,  -4.2076,  ...,  20.5085,   3.6718,   6.4799],\n",
       "          [  4.3132,  -6.4938,  -7.0425,  ...,   3.6718,  29.9442,   4.7719],\n",
       "          [  2.6288,  -0.8643,  -1.0215,  ...,   6.4799,   4.7719,  39.3404]],\n",
       "\n",
       "         [[ 30.1868, -18.2841,  -3.8556,  ...,   0.1748, -11.6281,   7.9357],\n",
       "          [-18.2841,  33.2663,  -0.5568,  ...,   4.7933,   7.4713,  -1.5922],\n",
       "          [ -3.8556,  -0.5568,  33.3571,  ...,  -5.5591,  -1.8302,  -2.3288],\n",
       "          ...,\n",
       "          [  0.1748,   4.7933,  -5.5591,  ...,  42.2914,  -3.1656,  -0.0794],\n",
       "          [-11.6281,   7.4713,  -1.8302,  ...,  -3.1656,  37.9287,  -2.7775],\n",
       "          [  7.9357,  -1.5922,  -2.3288,  ...,  -0.0794,  -2.7775,  20.1431]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 27.1805,   1.7525,   1.2874,  ...,  -0.7088,  -5.4087,  -2.1454],\n",
       "          [  1.7525,  37.2680,  -7.7314,  ...,  -5.4847,  -0.3849,  10.4835],\n",
       "          [  1.2874,  -7.7314,  35.9060,  ...,   5.6001,   3.8431,  -0.8432],\n",
       "          ...,\n",
       "          [ -0.7088,  -5.4847,   5.6001,  ...,  21.4171,   4.0599,   1.9034],\n",
       "          [ -5.4087,  -0.3849,   3.8431,  ...,   4.0599,  38.6673, -15.6571],\n",
       "          [ -2.1454,  10.4835,  -0.8432,  ...,   1.9034, -15.6571,  46.7569]],\n",
       "\n",
       "         [[ 22.2746,  -1.5875,   2.0462,  ...,  -4.3275,   1.6363,   3.8162],\n",
       "          [ -1.5875,  24.4395,  -6.1820,  ...,   6.4587,  -4.6774,   0.1828],\n",
       "          [  2.0462,  -6.1820,  30.9477,  ...,  -0.0719,   5.0252,   2.4537],\n",
       "          ...,\n",
       "          [ -4.3275,   6.4587,  -0.0719,  ...,  28.9538,  -1.4443,  -3.5094],\n",
       "          [  1.6363,  -4.6774,   5.0252,  ...,  -1.4443,  26.9600,   4.2082],\n",
       "          [  3.8162,   0.1828,   2.4537,  ...,  -3.5094,   4.2082,  20.0763]],\n",
       "\n",
       "         [[ 40.1019,   1.1669,  -7.5864,  ...,   4.5362,   4.3235,  -0.7608],\n",
       "          [  1.1669,  38.2663,   2.0250,  ...,   3.9493,  -2.9794,   3.8610],\n",
       "          [ -7.5864,   2.0250,  26.2510,  ...,   4.9898,  -2.2455,   4.8609],\n",
       "          ...,\n",
       "          [  4.5362,   3.9493,   4.9898,  ...,  25.6067,   0.0972,   1.2879],\n",
       "          [  4.3235,  -2.9794,  -2.2455,  ...,   0.0972,  15.2263,   0.2495],\n",
       "          [ -0.7608,   3.8610,   4.8609,  ...,   1.2879,   0.2495,  24.2671]],\n",
       "\n",
       "         [[ 43.5244,  -6.5812,   6.9048,  ..., -16.0361,   4.4655, -10.7278],\n",
       "          [ -6.5812,  30.5437,  -2.1458,  ...,   6.1033,   5.0552,  -4.5733],\n",
       "          [  6.9048,  -2.1458,  29.7066,  ...,  -8.8169,  -0.4333, -11.8848],\n",
       "          ...,\n",
       "          [-16.0361,   6.1033,  -8.8169,  ...,  41.8999,  10.9827,  11.8192],\n",
       "          [  4.4655,   5.0552,  -0.4333,  ...,  10.9827,  35.2686,   6.1599],\n",
       "          [-10.7278,  -4.5733, -11.8848,  ...,  11.8192,   6.1599,  33.8409]]],\n",
       "\n",
       "\n",
       "        [[[ 32.0467,   7.0052,  -3.2416,  ...,   8.1532,  -0.2888,  -6.5871],\n",
       "          [  7.0052,  25.7955,  -8.1634,  ...,   7.4437,  -1.6669,  -2.2903],\n",
       "          [ -3.2416,  -8.1634,  23.1504,  ...,  -9.1981,  -1.7050,   3.3145],\n",
       "          ...,\n",
       "          [  8.1532,   7.4437,  -9.1981,  ...,  40.4675,   1.8017,   2.0262],\n",
       "          [ -0.2888,  -1.6669,  -1.7050,  ...,   1.8017,  23.5782,  -6.3969],\n",
       "          [ -6.5871,  -2.2903,   3.3145,  ...,   2.0262,  -6.3969,  28.7649]],\n",
       "\n",
       "         [[ 21.0464,   0.6068,  -1.7797,  ...,   2.1086,  -2.0139,  -3.8878],\n",
       "          [  0.6068,  27.2985,   3.3299,  ...,  -4.8720,   0.4252,  -3.0846],\n",
       "          [ -1.7797,   3.3299,  30.7588,  ...,   3.5386,  -0.9481,  -9.6544],\n",
       "          ...,\n",
       "          [  2.1086,  -4.8720,   3.5386,  ...,  35.5470,   6.3743,   0.3064],\n",
       "          [ -2.0139,   0.4252,  -0.9481,  ...,   6.3743,  41.8936,  14.2506],\n",
       "          [ -3.8878,  -3.0846,  -9.6544,  ...,   0.3064,  14.2506,  40.4118]],\n",
       "\n",
       "         [[ 26.6901,  -1.6078,  -0.9821,  ...,  -0.6358,  -7.5112,   1.0814],\n",
       "          [ -1.6078,  33.0559,   6.3802,  ...,   4.5554,   5.6757,  -3.5736],\n",
       "          [ -0.9821,   6.3802,  22.2404,  ...,  -3.5796,   4.5527,  -4.9432],\n",
       "          ...,\n",
       "          [ -0.6358,   4.5554,  -3.5796,  ...,  20.0463,   4.5129,   1.3352],\n",
       "          [ -7.5112,   5.6757,   4.5527,  ...,   4.5129,  30.3458,   2.0281],\n",
       "          [  1.0814,  -3.5736,  -4.9432,  ...,   1.3352,   2.0281,  26.2497]],\n",
       "\n",
       "         [[ 34.7860,   1.1120,   1.4674,  ...,   2.8627,  -3.8121,  -4.6312],\n",
       "          [  1.1120,  27.1577,  -8.0268,  ...,  -4.4698,   1.6233,  -9.6746],\n",
       "          [  1.4674,  -8.0268,  37.0896,  ...,   6.1201,   2.8779,   4.9796],\n",
       "          ...,\n",
       "          [  2.8627,  -4.4698,   6.1201,  ...,  17.8634,   3.2803,   0.4458],\n",
       "          [ -3.8121,   1.6233,   2.8779,  ...,   3.2803,  20.7591,  -2.5509],\n",
       "          [ -4.6312,  -9.6746,   4.9796,  ...,   0.4458,  -2.5509,  23.5079]]],\n",
       "\n",
       "\n",
       "        [[[ 28.4154,   2.7645, -10.1010,  ...,   0.0741, -13.8531,   1.3159],\n",
       "          [  2.7645,  21.8306,  -6.2272,  ...,  -2.9116,  -3.2048,  -3.4533],\n",
       "          [-10.1010,  -6.2272,  31.9452,  ...,  -1.5767,  10.7117,   6.3105],\n",
       "          ...,\n",
       "          [  0.0741,  -2.9116,  -1.5767,  ...,  25.0870,   2.7461,   1.8960],\n",
       "          [-13.8531,  -3.2048,  10.7117,  ...,   2.7461,  26.7191,  -0.1613],\n",
       "          [  1.3159,  -3.4533,   6.3105,  ...,   1.8960,  -0.1613,  33.3621]],\n",
       "\n",
       "         [[ 25.6422,  -3.1636,  -2.0446,  ...,   7.4708,   2.9027,   1.4148],\n",
       "          [ -3.1636,  20.2049,   0.1776,  ...,  -4.4680,  -3.0735,   2.2424],\n",
       "          [ -2.0446,   0.1776,  26.3722,  ..., -13.9194,  -3.2235,  -1.9986],\n",
       "          ...,\n",
       "          [  7.4708,  -4.4680, -13.9194,  ...,  32.6086,  -2.0325,  -4.2793],\n",
       "          [  2.9027,  -3.0735,  -3.2235,  ...,  -2.0325,  26.0867,   4.6897],\n",
       "          [  1.4148,   2.2424,  -1.9986,  ...,  -4.2793,   4.6897,  22.4440]],\n",
       "\n",
       "         [[ 42.5938,  -7.6218,  -9.7269,  ...,  -2.8314, -10.9695,   7.8923],\n",
       "          [ -7.6218,  28.2443,   0.2591,  ...,  -5.1364,   4.3846,  -3.9063],\n",
       "          [ -9.7269,   0.2591,  32.0604,  ...,   0.5306,  10.6320,  -1.2622],\n",
       "          ...,\n",
       "          [ -2.8314,  -5.1364,   0.5306,  ...,  35.8231,  -1.7779,  -6.9608],\n",
       "          [-10.9695,   4.3846,  10.6320,  ...,  -1.7779,  28.1199,   3.4098],\n",
       "          [  7.8923,  -3.9063,  -1.2622,  ...,  -6.9608,   3.4098,  34.7994]],\n",
       "\n",
       "         [[ 45.7010,  -4.8081,   6.9660,  ...,  12.1433,  -0.8634,   8.6209],\n",
       "          [ -4.8081,  33.8269,  -2.8159,  ...,   0.0864,   5.1836,  -3.6970],\n",
       "          [  6.9660,  -2.8159,  48.0178,  ...,   4.8133,  -4.5906,   8.6223],\n",
       "          ...,\n",
       "          [ 12.1433,   0.0864,   4.8133,  ...,  34.4357,  -9.2983,  -4.1290],\n",
       "          [ -0.8634,   5.1836,  -4.5906,  ...,  -9.2983,  25.1852, -10.2649],\n",
       "          [  8.6209,  -3.6970,   8.6223,  ...,  -4.1290, -10.2649,  39.4607]]]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads @ another.transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08671",
   "metadata": {},
   "source": [
    "### Combining our BigramLM with our heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "d3c7c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_vocab = len(stoi)\n",
    "emb_dim = 128\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.mha  = Head(h_dim)\n",
    "        self.proj = nn.Linear(emb_dim, n_vocab, bias = False)\n",
    "        \n",
    "    def forward(self,x,targets=None):\n",
    "        loss = None\n",
    "        x_embed = self.emb_layer(x)\n",
    "#         print('embed', x_embed)\n",
    "        \n",
    "        x_attn = self.mha(x_embed)\n",
    "#         print('attn', x_attn)\n",
    "        \n",
    "        logits = self.proj(x_attn)\n",
    "#         print('logits', logits)\n",
    "#         logits.view(emb_dim)\n",
    "\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = nn.functional.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, _ = self(idx[:,-block_size]) # idx is shape (B,T), logits is B,T,C\n",
    "            probs = logits[:,-1,:] #probs is shape (B,C)\n",
    "            probs = F.softmax(probs, dim = 1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx,idx_new), dim = 1)\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "    \n",
    "model = BigramLM(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "29a23dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2740, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for idx in range(10000):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "d2c068db",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[466], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(model\u001b[38;5;241m.\u001b[39mgenerate(torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[0;32mIn[464], line 33\u001b[0m, in \u001b[0;36mBigramLM.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, max_new_tokens):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[0;32m---> 33\u001b[0m         logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(idx) \u001b[38;5;66;03m# idx is shape (B,T), logits is B,T,C\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         probs \u001b[38;5;241m=\u001b[39m logits[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;66;03m#probs is shape (B,C)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(probs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[464], line 16\u001b[0m, in \u001b[0;36mBigramLM.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     13\u001b[0m         x_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_layer(x)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         print('embed', x_embed)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         x_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(x_embed)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#         print('attn', x_attn)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x_attn)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[463], line 35\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m aw \u001b[38;5;241m=\u001b[39m aw\u001b[38;5;241m/\u001b[39m(emb_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     34\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T,:T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# generate mask\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m aw \u001b[38;5;241m=\u001b[39m aw\u001b[38;5;241m.\u001b[39mmasked_fill_(mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# apply mask i.e fill true values with -inf \u001b[39;00m\n\u001b[1;32m     38\u001b[0m aw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(aw,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# -inf values are converted to 0 and then each row is normalized\u001b[39;00m\n\u001b[1;32m     40\u001b[0m cv \u001b[38;5;241m=\u001b[39m aw \u001b[38;5;241m@\u001b[39m V \u001b[38;5;66;03m# context vector\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d9eb1",
   "metadata": {},
   "source": [
    "### Combining the previous model with Feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "11b9bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.relu(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e759883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(emb_dim, 4*emb_dim, bias= True)\n",
    "        self.layer2 = nn.Linear(4*emb_dim, emb_dim, bias = True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "edd35dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_vocab = len(stoi)\n",
    "emb_dim = 128\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.mha  = Head(h_dim)\n",
    "        self.FFN = FFN()\n",
    "        self.proj = nn.Linear(emb_dim, n_vocab, bias = False)\n",
    "        \n",
    "    def forward(self,x,targets=None):\n",
    "        loss = None\n",
    "        x_embed = self.emb_layer(x)\n",
    "#         print('embed', x_embed)\n",
    "        \n",
    "        x_attn = self.mha(x_embed)\n",
    "#         print('attn', x_attn)\n",
    "        x_ffn = self.FFN(x_attn)\n",
    "        logits = self.proj(x_ffn)\n",
    "        \n",
    "#         print('logits', logits)\n",
    "#         logits.view(emb_dim)\n",
    "\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = nn.functional.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, _ = self(idx[:,-block_size]) # idx is shape (B,T), logits is B,T,C\n",
    "            probs = logits[:,-1,:] #probs is shape (B,C)\n",
    "            probs = F.softmax(probs, dim = 1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx,idx_new), dim = 1)\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "    \n",
    "model = BigramLM(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b234e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9205, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for idx in range(10000):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f492b",
   "metadata": {},
   "source": [
    "### Layernormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0b5c16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,emb_dim, eps= 1e-5, mom=0.1):\n",
    "        super().__init__()\n",
    "        self.bngain = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.bnbias = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.out = None\n",
    "\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self,x):\n",
    "        meani = x.mean(-1, keepdim = True)\n",
    "        vari = x.var(-1, keepdim = True)\n",
    "        self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias\n",
    "        return self.out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62b74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "6c384b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = LayerNormalization(emb_dim)\n",
    "len(list(ln.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "574b9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ln(torch.randn(32,8,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "71a1b4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0000), tensor(0.))"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[-1,-1,:].std(), ans[-1,-1,:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c578c",
   "metadata": {},
   "source": [
    "### combine previous model with layer normalization and skip connections + positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "6c3ac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,h_dim):\n",
    "        super().__init__()\n",
    "        self.mha = Head(h_dim)\n",
    "        self.FFN = FFN()\n",
    "        self.ln1 = LayerNormalization(emb_dim)\n",
    "        self.ln2 = LayerNormalization(emb_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.mha(self.ln1(x)) + x\n",
    "        x = self.FFN(self.ln2(x)) + x\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "da98324d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "c651339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_vocab = len(stoi)\n",
    "emb_dim = 128\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, emb_dim)\n",
    "        self.ln = LayerNormalization(emb_dim)\n",
    "        self.proj = nn.Linear(emb_dim, n_vocab, bias = False)\n",
    "        \n",
    "        ## NEW\n",
    "        self.block = Block(h_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,x,targets=None):\n",
    "        loss = None\n",
    "        \n",
    "        x_embed = self.emb_layer(x)\n",
    "        x_pos = self.pos_emb(torch.ones_like(x) * torch.arange(x.shape[1]))\n",
    "        \n",
    "        x_block = self.block(x_embed + x_pos)\n",
    "        x_ln = self.ln(x_block)\n",
    "        \n",
    "        logits = self.proj(x_ln)\n",
    "        \n",
    "#         print('logits', logits)\n",
    "#         logits.view(emb_dim)\n",
    "\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = nn.functional.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "#             print('idx', idx.shape)\n",
    "            logits, _ = self(idx[:,-block_size:]) # idx is shape (B,T), logits is B,T,C\n",
    "#             print('logits', logits.shape)\n",
    "            probs = logits[:,-1,:] #probs is shape (B,C)\n",
    "            probs = F.softmax(probs, dim = 1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx,idx_new), dim = 1)\n",
    "            \n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e20a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "07719a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLM(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "e80fc1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7881, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for idx in range(10000):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "9ed9bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To the delies.\n",
      "\n",
      "BRUTUS:\n",
      "Prifting king head!\n",
      "And neved\n",
      "somes drelose, been of his cannot lord, our you know;\n",
      "Now!\n",
      "A provoses;\n",
      "But king, out in that them! Lord well the eath this acan that I beaunture will to a cher forsely,\n",
      "that lord, of Julietteruse.\n",
      "\n",
      "VOLIO:\n",
      "Nor most. Ret.\n",
      "The now, sir: our thing he whom\n",
      "Tybarn for your head Tis many amply cutdendereizen: bear, buldown'd to so world crain as room meet, what heave misprus im; That blowisdoms finour to.\n",
      "\n",
      "LADUKE VINCENTIO:\n",
      "If Juchamfect better's Mayour Tray have prite warre. I ploy:\n",
      "\n",
      "RICHARD:\n",
      "When not now:\n",
      "Shall\n",
      "Than whoming will of heave goant dart? be is from lies\n",
      "with we that to in her your ful us;\n",
      "Sheechy, my\n",
      "with the most beaute you,\n",
      "With that you way this ma dare showed,--it held:\n",
      "An him ne'er.\n",
      "\n",
      "DUMERCENTIO:\n",
      "At Too curse,--but in crown needy?\n",
      "\n",
      "RIVERCUTIO:\n",
      "An would have chie,\n",
      "For courable he uncoth!\n",
      "\n",
      "GLOUCESTER:\n",
      "Now you ear threw wost play, as muciefuied\n",
      "to kind of gods naskly Eaductione, and and grow thite I cawares, as a son,. Gentlemy lieger is rail.\n",
      "\n",
      "JULIET:\n",
      "If thire torm: I our How not the his: grace have pun in soues of this turnclelaime, that sain wor good, look!\n",
      "\n",
      "And gold ward I; wilcompelse letwen dition\n",
      "Wors;\n",
      "For when your a the cisee of as me, my gived.\n",
      "\n",
      "AUTOLYCUS:\n",
      "The him true me kilgood us.\n",
      "\n",
      "KING HENRY Duke:\n",
      "Abanifemself.\n",
      "\n",
      "POLIXENES:\n",
      "Kore his curagese way their vowlyss,\n",
      "And the county my most thy be grace, feful advice you his traimsself.\n",
      "\n",
      "DUKE VINGH:\n",
      "Unto do,\n",
      "Is hath may grate: him you are drowning Help Tyband to have is shall and rums be as to his true of thee, let then heads. My look suments\n",
      "Cown: he well bedumfor wife;\n",
      "If not thought rly at siguers:\n",
      "To with:\n",
      "Hear-profes wift our the ento knew! say, no him of have I Cluld banish wefore in babiend full plieve on hip neverd, as touccretor.\n",
      "\n",
      "FOLAURENCENTIO:\n",
      "Which them all melecton mydel pegame, traed gold bling to it in confall what, 'erd her nore in bid in these is to the feely praily Cruay no lesty pries, why drould wrut beauntonessy good been prince an to hate goddeity were besir,\n",
      "Get in besters\n",
      "Of gate by crue, wear sorrow:\n",
      "Cher you licks.\n",
      "\n",
      "NORTESBY:\n",
      "God,\n",
      "Feath the straugh faity,\n",
      "To greep and word,\n",
      "A her;\n",
      "I love tearly yourses and his his ever ERWICK been this murds mine end an you to panit their plaimself.\n",
      "\n",
      "For break her, so been mest before,\n",
      "If Herm is nothing againe a what we quading by the const again, leave you ware Eork, thee; we it?\n",
      "The was that have to dow! were womaniught sorraw our noughtly hath thunnituded, swouldie, I that smoth, with lossion\n",
      "Of to crownce\n",
      "On\n",
      "with your shion;\n",
      "For Loncetters drown,\n",
      "Auf: that, Laster, my charink,\n",
      "He is being that for thoughtiviles abace lost towarchemself\n",
      "All not a is curs: your lathere, and spawn\n",
      "why the fries, sire wiserved, sweek to and not a'es Balmaties up woman of no law no him that himself life.\n",
      "\n",
      "YORK:\n",
      "He paintames it.\n",
      "\n",
      "PARIS:\n",
      "Richar life,\n",
      "And prop. I dring of noble too live swey couse;\n",
      "And on, to to grave clows.\n",
      "\n",
      "SICINIUS:\n",
      "Warwick'd at an and man\n",
      "Rome stry notion beat, and hath and guee.\n",
      "\n",
      "Fith od the lrover,\n",
      "And too! what it hoarrate, birt.\n",
      "\n",
      "LADY CAPULET:\n",
      "And end.\n",
      "\n",
      "Seconce;\n",
      "Did of him a trity\n",
      "To dedue exts.\n",
      "\n",
      "LUCIO:\n",
      "Ay, gentlemany the sway.\n",
      "\n",
      "WARD IV:\n",
      "His fools the heart!\n",
      "\n",
      "AUTOLYCUSHXENENIUS:\n",
      "Let dream!\n",
      "\n",
      "ANTIO:\n",
      "The be to death:\n",
      "My fresir;\n",
      "The dotwy\n",
      "Freath such ecry seem of tune why couse thee: he city\n",
      "To breath The him.\n",
      "Fhretly that be suph show.\n",
      "\n",
      "FRIAR TCompt\n",
      "seeceitacking Here.\n",
      "\n",
      "NORTIUS:\n",
      "Nor Lord thy his are I lo:\n",
      "My theeisue\n",
      "Warwice headly.\n",
      "The entone:\n",
      "This pretebeardy;\n",
      "How his coung lovery thand inshries\n",
      "To my knal awe him!\n",
      "\n",
      "NORTENSIO:\n",
      "Now true, limes answelcous?\n",
      "\n",
      "ROMEO:\n",
      "Husin!\n",
      "You cosise,\n",
      "With us,\n",
      "And pare she swifter's to him, your king of the\n",
      "an old for you purmptrese gently\n",
      "Will Lieve your bread of with am this Yoint litted.\n",
      "\n",
      "She honours:\n",
      "Whereign, And to sattereith it you\n",
      "Snothird noble taabbegitle by blew, we live but than train.\n",
      "\n",
      "LUMNIE:\n",
      "NencAnger intake bries, dead wear her:\n",
      "And for of your the Will to grace shall not ilse would custoly caperfell nry: 't,\n",
      "Thou world tone bloody upon they\n",
      "Lord friecto his me the ily? I stand one, our again'd fice,\n",
      "What not.\n",
      "\n",
      "LADY CARIANA:\n",
      "Tas\n",
      "For have irituation my not her made wrrange a now,\n",
      "An I way\n",
      "Capiece? Pray, your reaking my Will dowrongifformenten all at boot there all'd ara were headagentlemany libeand wron, ror made handraw heat, Causurpaciouth grace a what, doth his unperfeecome blead accame, our sward on hath, safe.\n",
      "\n",
      "MENE:\n",
      "There they fal straitagen.\n",
      "\n",
      "BUCHII:\n",
      "The the mine with for whod not upon yet plighform of the her licessighte we Straighted, a hastomberate, daught,\n",
      "These not are all every lord not dean that your creeemember'st living\n",
      "That feath.\n",
      "Now My Rome.\n",
      "\n",
      "For to no my has but Rive me no day, as\n",
      "Wrom the intriber gipful handGause previngbrood's we pleave thy king for heave we house have you nut and chould Dearis; and I ward in off this of unands exceedied life shing morcauself,\n",
      "As to they welcome I will for Hunbry of thy be slay? howbread, and grace a befied banise deaute is befrom cation.\n",
      "\n",
      "CLAUDIO:\n",
      "When, I am, I wear'd peoply tongue to Pet:\n",
      "\n",
      "Bounmen a before gook known:\n",
      "By we office,\n",
      "We to his clam: the king liff.\n",
      "\n",
      "HY:\n",
      "What?\n",
      "If You knew loved muctand you maded him, perouse thou beare not cretory we the not unjusty he reterefore is do well to her die down left me our much:\n",
      "Had b'd thy as ban abour will Anclose fair by to may of Qaint you had and this deed\n",
      "Eveth triatureing our'd and But show that hand,\n",
      "And inster?\n",
      "\n",
      "GLOUCESTENSIO:\n",
      "A whom come than even,\n",
      "You are thy brothery youty, I a gentle should's was way turoung most, ast all ie not in hey\n",
      "To Lordad no soul, servantale thy be narly;\n",
      "And good agaffect, colse I may is king?\n",
      "If crown'd ear Volsun\n",
      "ender'd life\n",
      "An hearts pedine thou blang!\n",
      "I mism pare saidf all in face? Was and melcower audio.\n",
      "Hand infalse?\n",
      "\n",
      "First we; in Serving be to her:\n",
      "No,tis stommone: when way,\n",
      "BusHe Where cure in eltal much will my prever alm in to-myself spear ing beg and in serving cong in all wighter me you deful of thy biends\n",
      "Boing it wave wronBy are none:\n",
      "Then this way holy.\n",
      "\n",
      "GLOUCESTER:\n",
      "Hare!\n",
      "Now'd his nonour she hous sham of a die, up make would hought heir ha,--O lose to refitsed for prom a grave\n",
      "To cannot lips.\n",
      "But bettence,\n",
      "To most put dead; thou wrong is unto\n",
      "truld Lance to it tabmand with to where jut in if have I couse,\n",
      "As to pue deson.\n",
      "\n",
      "CLIFFORTEGBROKE:\n",
      "If Lord do.\n",
      "The gads I was peacefore womber must your councie again. Wepty you vike ear swence a name too lady as honour great him as nread?\n",
      "\n",
      "GLOUCESTER:\n",
      "Pauly, Kneet, wither:\n",
      "Angelo;\n",
      "Wher your shion.\n",
      "\n",
      "KINGHA:\n",
      "The duspire me an ay, if turetory wron abound,\n",
      "And to casoland hall confaithe body their chood grouble!\n",
      "Our Nay, we devile him;\n",
      "And have of our raim.\n",
      "\n",
      "ESCALUS:\n",
      "Nor yow\n",
      "To perd Frand mee lords, tell\n",
      "Which his can fair a mand kead Ke too live the lice;\n",
      "I her sceecond be duke?\n",
      "To pretbeen hit love,\n",
      "No Let him ly know:\n",
      "Histend 'twas I love: my down the mertia; trunt, him breard goold to Wack the sajes. Comfor horse:\n",
      "I henose amany of they waunt,\n",
      "And I can in they out weep of I'll tirrought,\n",
      "I Plonds;\n",
      "Whinher,' bate fight ent is a scept to my talk see long\n",
      "As I know word;\n",
      "Of you, by why are pail the but man\n",
      "Than on us mall betry\n",
      "Rage re to out surp, my Duke I queen the battly:\n",
      "Iven thy banishould life, gentlize:\n",
      "In gethat my will to not Benouuts and Laint were wife well gume head\n",
      "at eart, and Ge: hy came none, if I landlay this part my bodanced: we what now.\n",
      "\n",
      "BRAMENE:\n",
      "You holy very; life\n",
      "No, uncady\n",
      "have us,\n",
      "Aways threak out,\n",
      "Nor grains\n",
      "Dears, hearewe of herd!\n",
      "A like enour us?\n",
      "\n",
      "LADY CARIANA:\n",
      "The us, boot thy hong Warwice asquested me and Messervy.\n",
      "\n",
      "Frothungent Behalt.\n",
      "\n",
      "HASAR:\n",
      "And for thank God Kenews, trofes, incies, shall so tell: know.\n",
      "\n",
      "GLOUCESTER:\n",
      "Thout, am name swear his,\n",
      "Nay, somes, if aughter, whence,\n",
      "And prence to see joing one,' delish a eximieT;\n",
      "For But hand.\n",
      "\n",
      "FRIAR TGAUNT:\n",
      "For labeing presence.\n",
      "\n",
      "JULIET:\n",
      "The grow.\n",
      "\n",
      "BISHARD GAUNT:\n",
      "You they do his perfee\n",
      "I shall prater cled\n",
      "With in at my leads\n",
      "Whamontway, up mase gh; untard to begs.\n",
      "\n",
      "QUEEN RICHAM: I will book his how Parisors;\n",
      "For I bleave I prepeding Romeo her down.\n",
      "\n",
      "Seconfore, as breams' dear of you man's will peecemes, Auffacenty all consmialt accould, what hall will what me how, I'll better chade and there would sure; Very noble\n",
      "He I week to no slay all Richarison?\n",
      "\n",
      "Seces.\n",
      "\n",
      "ROMEO:\n",
      "We\n",
      "Pault Tune all bear of thes hast they lish you with thy forturisign,\n",
      "And his war abin,\n",
      "Servan:\n",
      "Anborn pile as to Puscause?\n",
      "\n",
      "BUCHOTCESTIO:\n",
      "It such fercy,\n",
      "And tune your bold,\n",
      "The for they homined\n",
      "Gentleman, bein the boy!\n",
      "Speadowar, and you\n",
      "And ty?\n",
      "\n",
      "LUCIO:\n",
      "No end?\n",
      "\n",
      "Por man\n",
      "Off your loftious,\n",
      "Thou good Cepray your the deady!\n",
      "Of OF YORK:\n",
      "My from me dreaty powere with to kinship wronger:\n",
      "Whom bears; be all Clainsman:\n",
      "What veinst.\n",
      "\n",
      "VOLIO:\n",
      "Romfore,\n",
      "And traught were flamo's life,\n",
      "And her Some let comperch, that straise?\n",
      "\n",
      "FLORIOLANUS:\n",
      "And wea, I been this feart her all perouse all goo-me!\n",
      "\n",
      "MARGARET:\n",
      "Untor younsed but conums death sleave as be, weetaitold\n",
      "Of amost\n",
      "And be noble not suffer:\n",
      "How at I'll huson or have her were trould were an with debook!\n",
      "As lorderst what do, I hace an what him!\n",
      "So of heaven erein him should not\n",
      "where tim:\n",
      "I be which should Leadies, to at a be tot the home, if you; her I for else the not the ruitest majesty sunce;\n",
      "For it.\n",
      "\n",
      "See do the entestan of in godly ow Had--owisdies,\n",
      "And shear posity, Here lendly.\n",
      "\n",
      "MENIUS:\n",
      "Affatter beggard die,\n",
      "Our breed, and\n",
      "Mon's persmand Goding brraation:\n",
      "And such'd aff in to cours,\n",
      "But life; and for the lance garewell did them to Madam this nable not holk to despet ailse earies,\n",
      "Would mreadfing Most, me true have\n",
      "Or Corion? she enou harm in.\n",
      "\n",
      "QUEEN:\n",
      "Ay?\n",
      "Ha's with woman's son. OVERDITA:\n",
      "Abaservanty not there him trains\n",
      "An I the city too is ghood leign canners, poof more that on advici him? their perful of gualf, acd to your bayou:\n",
      "And without, in of where'er writy.\n",
      "For'd by ha; and not mine.\n",
      "\n",
      "BURY:\n",
      "Gresty let's\n",
      "IUS:\n",
      "O leep-bawful be; alse our dead this disceet speak his the guilt to\n",
      "\n",
      "ANGELO:\n",
      "Tulaties sealn forge p\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70172b",
   "metadata": {},
   "source": [
    "as we can see there's high quality output after the addition of positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dc6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "df26be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_emb = nn.Embedding(block_size, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "19141ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(32,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "b25d29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones_like(a)*torch.arange(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "ae6d5356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12665a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb204084",
   "metadata": {},
   "source": [
    "B,T each T'th dimension should have the numbers between 0, block_sizem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d1c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e276893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2fffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d76ae05d",
   "metadata": {},
   "source": [
    "### Log of all losses\n",
    "\n",
    "\n",
    "**initial using adamW 10K iter**\n",
    "\n",
    "2.601\n",
    "\n",
    "**After multi-head-attentnion 10k iter**\n",
    "\n",
    "2.316\n",
    "\n",
    "**After FFN 10k iter**\n",
    "\n",
    "1.9205\n",
    "\n",
    "**After LayerNormalization**\n",
    "1.9252\n",
    "\n",
    "**After skip-connections**\n",
    "2.0718\n",
    "\n",
    "**After positional embedding**\n",
    "1.7881 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db690dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d123231",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "b8cccd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "5e9083a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(data)))\n",
    "len(data)\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(vocab)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "\n",
    "encode = lambda x: [stoi[i] for i in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "60102175",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = data[:int(0.9*len(data))]\n",
    "Xval = data[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "dfcd3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "1158d216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "ce714a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_split(X):\n",
    "    idx = torch.randint(0,len(X) - block_size, (batch_size,)) # we subtract block_size from total len of X, because w'll be taking next characters starting from the idx to the total len of block_size\n",
    "    Xb =  torch.tensor([encode(X[i:i+block_size]) for i in idx]) # now our d should be 32,8\n",
    "    Yb = torch.tensor([encode(X[i+1:i+1+block_size]) for i in idx])\n",
    "    \n",
    "    return Xb.to(device),Yb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "c1c9dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iter = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss():\n",
    "    out = dict()\n",
    "    \n",
    "    model.eval()\n",
    "    for item in ['train', 'val']:\n",
    "        if item == 'train':\n",
    "            losses = torch.zeros(eval_iter)\n",
    "            for k in range(eval_iter):\n",
    "\n",
    "                Xb,Yb = get_split(Xtr)\n",
    "                _, loss = model(Xb,Yb)\n",
    "                losses[k] = loss\n",
    "            out[item] = losses.mean()\n",
    "\n",
    "        if item == 'val':\n",
    "            losses = torch.zeros(eval_iter)\n",
    "            for k in range(eval_iter):\n",
    "                \n",
    "                Xb,Yb = get_split(Xval)\n",
    "                _, loss = model(Xb,Yb)\n",
    "                losses[k] = loss\n",
    "            out[item] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "0f094dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,h_dim):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.wk = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.wv = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        Q,K,V = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        \n",
    "        # comment out if using multi head attention\n",
    "        ### ------ multi-head ----------------\n",
    "        n_heads = emb_dim // h_dim\n",
    "        Q = Q.view(B,T,n_heads, h_dim)\n",
    "        K = K.view(B,T,n_heads, h_dim)\n",
    "        V = V.view(B,T,n_heads, h_dim)\n",
    "        \n",
    "        Q = torch.transpose(Q, 1,2) # transposing (n_head, block_size) cause we'll do matmul operation on block_size and h_dim\n",
    "        K = torch.transpose(K, 1,2) # transposing (n_head, block_size) cause we'll do matmul operation on block_size and h_dim\n",
    "        V = torch.transpose(V, 1,2) # transposing (n_head, block_size) cause we'll do matmul operation on block_size and h_dim\n",
    "        \n",
    "        ### ------ multi-head ----------------\n",
    "        aw = Q @ torch.transpose(K, -2,-1) # for matmul dim of q should be B,T,C and k should be B,C,T\n",
    "        aw = aw/(emb_dim **0.5)\n",
    "        mask = self.tril[:T,:T] == 0 # generate mask\n",
    "        aw = aw.masked_fill_(mask, float('-inf')) # apply mask i.e fill true values with -inf \n",
    "        aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized\n",
    "\n",
    "        cv = aw @ V # context vector\n",
    "        cv = torch.transpose(cv, 1,2) # bring it back to (B,T,n_heads, h_dim)\n",
    "        cv = cv.contiguous().view(B,T,-1)\n",
    "        \n",
    "        return cv\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "b9f3148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(emb_dim, 4*emb_dim, bias= True)\n",
    "        self.layer2 = nn.Linear(4*emb_dim, emb_dim, bias = True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "da60219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,emb_dim, eps= 1e-5, mom=0.1):\n",
    "        super().__init__()\n",
    "        self.bngain = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.bnbias = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.out = None\n",
    "\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self,x):\n",
    "        meani = x.mean(-1, keepdim = True)\n",
    "        vari = x.var(-1, keepdim = True)\n",
    "        self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias\n",
    "        return self.out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "36b66b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,h_dim):\n",
    "        super().__init__()\n",
    "        self.mha = Head(h_dim)\n",
    "        self.FFN = FFN()\n",
    "        self.ln1 = LayerNormalization(emb_dim)\n",
    "        self.ln2 = LayerNormalization(emb_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.mha(self.ln1(x)) + x\n",
    "        x = self.FFN(self.ln2(x)) + x\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "19cdc3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_vocab = len(stoi)\n",
    "emb_dim = 128\n",
    "block_size = 16\n",
    "h_dim = 32\n",
    "n_blocks = 4\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, emb_dim)\n",
    "        self.ln = LayerNormalization(emb_dim)\n",
    "        self.proj = nn.Linear(emb_dim, n_vocab, bias = False)\n",
    "        \n",
    "        ## NEW\n",
    "        self.blocks = nn.Sequential(*[Block(h_dim) for _ in range(4)])\n",
    "        \n",
    "        \n",
    "    def forward(self,x,targets=None):\n",
    "        loss = None\n",
    "        \n",
    "        x_embed = self.emb_layer(x)\n",
    "        x_pos = self.pos_emb(torch.ones_like(x) * torch.arange(x.shape[1]))\n",
    "        \n",
    "        x_block = self.blocks(x_embed + x_pos)\n",
    "        x_ln = self.ln(x_block)\n",
    "        \n",
    "        logits = self.proj(x_ln)\n",
    "        \n",
    "#         print('logits', logits)\n",
    "#         logits.view(emb_dim)\n",
    "\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = nn.functional.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "#             print('idx', idx.shape)\n",
    "            logits, _ = self(idx[:,-block_size:]) # idx is shape (B,T), logits is B,T,C\n",
    "#             print('logits', logits.shape)\n",
    "            probs = logits[:,-1,:] #probs is shape (B,C)\n",
    "            probs = F.softmax(probs, dim = 1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx,idx_new), dim = 1)\n",
    "            \n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "3c2a121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLM(h_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "a49a0480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 0: losses: {'train': tensor(4.0128), 'val': tensor(4.0197)}\n",
      " step 200: losses: {'train': tensor(2.2937), 'val': tensor(2.3103)}\n",
      " step 400: losses: {'train': tensor(2.1003), 'val': tensor(2.1448)}\n",
      " step 600: losses: {'train': tensor(1.9851), 'val': tensor(2.0530)}\n",
      " step 800: losses: {'train': tensor(1.9177), 'val': tensor(2.0016)}\n",
      " step 1000: losses: {'train': tensor(1.8552), 'val': tensor(1.9664)}\n",
      " step 1200: losses: {'train': tensor(1.8357), 'val': tensor(1.9464)}\n",
      " step 1400: losses: {'train': tensor(1.7834), 'val': tensor(1.9312)}\n",
      " step 1600: losses: {'train': tensor(1.7782), 'val': tensor(1.8990)}\n",
      " step 1800: losses: {'train': tensor(1.7337), 'val': tensor(1.8853)}\n",
      " step 2000: losses: {'train': tensor(1.7283), 'val': tensor(1.8673)}\n",
      " step 2200: losses: {'train': tensor(1.7080), 'val': tensor(1.8764)}\n",
      " step 2400: losses: {'train': tensor(1.6945), 'val': tensor(1.8577)}\n",
      " step 2600: losses: {'train': tensor(1.6876), 'val': tensor(1.8383)}\n",
      " step 2800: losses: {'train': tensor(1.6763), 'val': tensor(1.8354)}\n",
      " step 3000: losses: {'train': tensor(1.6648), 'val': tensor(1.8294)}\n",
      " step 3200: losses: {'train': tensor(1.6585), 'val': tensor(1.8239)}\n",
      " step 3400: losses: {'train': tensor(1.6514), 'val': tensor(1.8005)}\n",
      " step 3600: losses: {'train': tensor(1.6405), 'val': tensor(1.8000)}\n",
      " step 3800: losses: {'train': tensor(1.6208), 'val': tensor(1.7985)}\n",
      " step 4000: losses: {'train': tensor(1.6374), 'val': tensor(1.7847)}\n",
      " step 4200: losses: {'train': tensor(1.6226), 'val': tensor(1.7880)}\n",
      " step 4400: losses: {'train': tensor(1.6164), 'val': tensor(1.7778)}\n",
      " step 4600: losses: {'train': tensor(1.6057), 'val': tensor(1.7920)}\n",
      " step 4800: losses: {'train': tensor(1.6025), 'val': tensor(1.7803)}\n",
      " step 5000: losses: {'train': tensor(1.6009), 'val': tensor(1.7762)}\n",
      " step 5200: losses: {'train': tensor(1.5881), 'val': tensor(1.7756)}\n",
      " step 5400: losses: {'train': tensor(1.5792), 'val': tensor(1.7609)}\n",
      " step 5600: losses: {'train': tensor(1.5851), 'val': tensor(1.7565)}\n",
      " step 5800: losses: {'train': tensor(1.5715), 'val': tensor(1.7503)}\n",
      " step 6000: losses: {'train': tensor(1.5762), 'val': tensor(1.7453)}\n",
      " step 6200: losses: {'train': tensor(1.5695), 'val': tensor(1.7463)}\n",
      " step 6400: losses: {'train': tensor(1.5738), 'val': tensor(1.7371)}\n",
      " step 6600: losses: {'train': tensor(1.5639), 'val': tensor(1.7313)}\n",
      " step 6800: losses: {'train': tensor(1.5496), 'val': tensor(1.7253)}\n",
      " step 7000: losses: {'train': tensor(1.5621), 'val': tensor(1.7312)}\n",
      " step 7200: losses: {'train': tensor(1.5579), 'val': tensor(1.7246)}\n",
      " step 7400: losses: {'train': tensor(1.5555), 'val': tensor(1.7399)}\n",
      " step 7600: losses: {'train': tensor(1.5465), 'val': tensor(1.7323)}\n",
      " step 7800: losses: {'train': tensor(1.5550), 'val': tensor(1.7437)}\n",
      " step 8000: losses: {'train': tensor(1.5515), 'val': tensor(1.7444)}\n",
      " step 8200: losses: {'train': tensor(1.5386), 'val': tensor(1.7312)}\n",
      " step 8400: losses: {'train': tensor(1.5342), 'val': tensor(1.7294)}\n",
      " step 8600: losses: {'train': tensor(1.5440), 'val': tensor(1.7240)}\n",
      " step 8800: losses: {'train': tensor(1.5454), 'val': tensor(1.7259)}\n",
      " step 9000: losses: {'train': tensor(1.5388), 'val': tensor(1.7214)}\n",
      " step 9200: losses: {'train': tensor(1.5282), 'val': tensor(1.7161)}\n",
      " step 9400: losses: {'train': tensor(1.5255), 'val': tensor(1.7225)}\n",
      " step 9600: losses: {'train': tensor(1.5300), 'val': tensor(1.7152)}\n",
      " step 9800: losses: {'train': tensor(1.5304), 'val': tensor(1.7066)}\n",
      " step 9999: losses: {'train': tensor(1.5280), 'val': tensor(1.6964)}\n",
      "tensor(1.6668, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "all_losses = {'train' : [], 'val' : []}\n",
    "\n",
    "total_iter = 10000\n",
    "for ind in range(total_iter):\n",
    "    Xb,Yb = get_split(Xtr)\n",
    "    logits,loss = model(Xb,Yb)\n",
    "    if ind % eval_iter == 0 or ind == total_iter - 1:\n",
    "        with torch.no_grad():\n",
    "            eloss = evaluate_loss()\n",
    "            all_losses['train'].append(eloss['train'].item())\n",
    "            all_losses['val'].append(eloss['val'].item())\n",
    "            print(f' step {ind}: losses: {eloss}')\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "7919ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is you rathed in love as not.\n",
      "\n",
      "HENRY HENRY VI\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "By \n",
      "Till inevice the told,\n",
      "And one gall be so chaperted\n",
      "A friender'd misi' the time earth and put to land, he counting to pray.\n",
      "'Tis man's ma attemp\n",
      "From your seatter's this verienge:\n",
      "His mothers her hate, till do\n",
      "The lorkwin the forwards\n",
      "what his quarrel piece:\n",
      "The father's, comfort to be love that more speak one on courts living worthy, and in this of rumpts and inque against distroting.\n",
      "\n",
      "ROMEO:\n",
      "As Risichion: my power by my right,\n",
      "And me then we do the duke.\n",
      "\n",
      "POLIXENES:\n",
      "And surelvey did life\n",
      "once abelitter but of houseing cause\n",
      "As that he to tell plobb'd from the heart,\n",
      "And Did bearthy cousin here?\n",
      "\n",
      "ISABELLA:\n",
      "Sweet so bist gaze together, I would seem on Aumerle.\n",
      "Let think iI'd a goodness of frail from our revonaminate him curse maid, in this suit, for all askites\n",
      "As that a death.\n",
      "I'll never warshed our truth;\n",
      "O! offect late intercure\n",
      "Of seem upon hy gates.\n",
      "'Text God unreisold\n",
      "Which mine holds no monfaciars! say, damneta leave.\n",
      "I'll make thou\n",
      "calledHed are the title than theirs\n",
      "Gill dame follower.\n",
      "I'll know the worl,\n",
      "Did fortune and paucil'ht thou must be do not a Captes:\n",
      "If resign the hand thy hands\n",
      "Regentle, now, that cry thou\n",
      "shalt as crown, it without Rutland in the shouts it of my langue of strivinger with sweets the fasterporate summer's pance,\n",
      "And as flowers as even he spremd\n",
      "Say bloody labour graves? will stabb'd\n",
      "Speak no keep to comfort, as a cure, his that flouded,\n",
      "And fear now our mother, hy gose\n",
      "Not in good herds it.\n",
      "\n",
      "Clown:\n",
      "When-changed up the queen! lost circutio more beseech.\n",
      "\n",
      "AUFIBALT:\n",
      "I'll not, I table.\n",
      "\n",
      "PAULINA:\n",
      "Not to your poor hours be a brother's most with his fallow death this? his is turn your batter\n",
      "His both sad, be shall die.\n",
      "\n",
      "MENENIUS:\n",
      "Let's this hihe hath conque time for hwoford.\n",
      "\n",
      "GLOUCESTER:\n",
      "Go treach of:\n",
      "But, thou king, which the cit,\n",
      "You wonder'st coasters,\n",
      "Tigue so her thinks with your blood of the duke, of the son-nonegly for war\n",
      "Both, Richard whose are we not hath replain,\n",
      "She's hat true throus must you being woivern, in, sir, but let him for, death.\n",
      "Long hath forth water\n",
      "Where not; as the atches as one, Lordon, our giirly.\n",
      "\n",
      "Nurse:\n",
      "Besides, innocence\n",
      "extremainted follow me of late Romeo, whither from his that draid\n",
      "Which death, on, death stir.\n",
      "\n",
      "GLOUCESTER:\n",
      "Duke of Norford\n",
      "\n",
      "NORFITZWARD IV:\n",
      "Come, do he hath been suffe prever, 'tis a randers for tLuckinfarencation?\n",
      "\n",
      "CAMILLO:\n",
      "It is not the sads and well!\n",
      "\n",
      "DORSET:\n",
      "What\n",
      "But so the forth bile blot die very come,\n",
      "And we livise all visit\n",
      "With by the unclusure both, poor I think\n",
      "His very nothinging some someetness!\n",
      "\n",
      "KING RICHARD II:\n",
      "It what lose them\n",
      "You wasque\n",
      "Whiles ye here for a foul tallige tempt:\n",
      "Reggarding words\n",
      "Thee bun\n",
      "the seizes and it with to hither solume done: good for Paris! in you\n",
      "if all be but I have.\n",
      "\n",
      "TUS:\n",
      "You obey, In Thank your sood lovy; and, and come, that we revenged agree done, to speak: pall.\n",
      "\n",
      "Clown:\n",
      "My mooning-hative thus\n",
      "Is cause,\n",
      "To find as I task.\n",
      "\n",
      "GREMINGS:\n",
      "It is, thy cornater,\n",
      "As years s'That I am suit pave by curginary\n",
      "Keptry thee, that true times to us dire\n",
      "But if that before the pite of his bares her rounded. Romeo, English! on' that for a very's harm,\n",
      "\n",
      "KING RICHARD II:\n",
      "Thou now to the enemies;\n",
      "But not peace, to see that we say rimorally and raumer with brave in none;\n",
      "And allower's master, sir, encle breathing lust, home\n",
      "Is for soul dreath; if why, and, think themself, yet we, at name sorry vengeance camition an upon his kin myself!\n",
      "Yon rathe. You have tried help no father scolds him,\n",
      "Thou having enemies, let me, my your praise\n",
      "And that beenefit evocliate for this majesty?\n",
      "My hateful voice\n",
      "To perish the Romeo will\n",
      "Will die a gaint in anothing whither than sudden the trouble,\n",
      "women die, not who, I speak, pardon thou hast never come\n",
      "Made your dam libund in,\n",
      "That lestrice ble Rome die For I am in a summanch?\n",
      " be good stood is.\n",
      "\n",
      "BEPLANUS:\n",
      "Doy, sir? Tell me unright, go youth\n",
      "promised to the valmouragy: but ble thou but he, sweet their pardon\n",
      "You have he mes the virds will'd here-flowise.\n",
      "\n",
      "CORIOLANUS:\n",
      "Office,' death hill mine of labour;\n",
      "Pastes to revious\n",
      "And, not put upon's bloodight.\n",
      "\n",
      "ABHOMENES:\n",
      "I'll Montague: you no, deathe\n",
      "all day prince, pickingham my torsest makes, Rome newd is down; Werping and go as here,\n",
      "There'll wist go what laid with me,\n",
      "Madam.\n",
      "\n",
      "LUCIO:\n",
      "O you not.\n",
      "For not to shame resolume that fair you. God\n",
      "But given to do a day ois graving 'twith me unto that you.\n",
      "\n",
      "First Murderer:\n",
      "And, my haste, my soul slick, committed, in set's shook which your struck and yea do not be your deave me:\n",
      "Look's honour of your brave be rieven-with you beg, not mere, as?\n",
      "We having to the holy that I spoke for the win joint of your honouration; and wouldst eiterity death she will thensuited!\n",
      "For it is his head,\n",
      "Coriolanus, and hath proclaccamaster means, lin for my daying together.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Your God's haste, as realm\n",
      "When I bear build\n",
      "Shall your hated,\n",
      "When all so this trust poss were\n",
      "A blothy soul the curse maties are as hereather of my dayage,\n",
      "And moving, but in your house,\n",
      "Cenry with him thinker her tent.' lay, thou lives a putish the feaster-should hither his life;\n",
      "Not sir; he's suddenly\n",
      "Whose seo scoves be no butely home, came this airs, that founds in his for the helds came?\n",
      "\n",
      "Nurse:\n",
      "And this me sorrow, bid a hestress\n",
      "That naturacles my late, conscail his love it down in that should is by this, holy tride a gyour story protected a wretched but do seat able,\n",
      "Agoing Angelowninges, as thy to thou art effrem. That is\n",
      "Lay'st give himself, as now,\n",
      "Till it long but speak I thy voice,\n",
      "If they slew my trainhern;\n",
      "And whose statute' the neudest, most all go fault our flier me;\n",
      "And I drate again plame;\n",
      "If, him we could like, in the Veion's?\n",
      "Has the tell for this athing goodness! he buried\n",
      "Of hear the mean Pilause with's all.\n",
      "It was not man can Richard,\n",
      "And with but prock on\n",
      "your ringelo go.\n",
      "\n",
      "ROMEO:\n",
      "I bed; and, I it would but And sell the ear as\n",
      "Jaughter's horse book\n",
      "For these cieving\n",
      "Before conscience was.\n",
      "To petty are od vauquin his acting rourse\n",
      "Hight is as her?\n",
      "\n",
      "KIVUTUS:\n",
      "As your, fail\n",
      "shall we do cormby nothing: that overse fatable before\n",
      "The warried may slinhame, herAth thou rights king:\n",
      "Up he shall ame love herd whence\n",
      "That's hither oher to heat.\n",
      "Not our puities,\n",
      "Proclaimins but is moved to be\n",
      "Sisters o'\n",
      "For that serviced me honour?\n",
      "\n",
      "GLOUCESTER:\n",
      "This is command mates? I\n",
      "dward thousand 'tis to tthy blood\n",
      "Which is your quincule,\n",
      "And let me honourable wenchequired by thy till steel win 'tis my our awry contiruis live my love?\n",
      "Time eason!\n",
      "Give no for your\n",
      "And the grave me? your comfortly crows to such which shath what would all the war\n",
      "nall bastacquied it for his mumber'd, madam.\n",
      "\n",
      "KATHAN ELTHANUS:\n",
      "No, fair I as or him weed\n",
      "excred in your dams, and myne?\n",
      "Hie, as they shall stogether, My darged againsting.\n",
      "Entake my stade and main\n",
      "True: his your hate; and pardonator:\n",
      "Mar lifthat slow\n",
      "Corance myself, it\n",
      "speak but a door\n",
      "namelly hither times ble good faster's the king.\n",
      "\n",
      "KINARWILL:\n",
      "The Roman of his king instand thence.\n",
      "\n",
      "Clown:\n",
      "It were a wound blost thou husb loards of fear; what siit\n",
      "Citizens! hrew enswordour,\n",
      "it kindly hat hat see\n",
      "Because him on his atten,\n",
      "And as the itself,' her, and losure her sir\n",
      "Infect lieging her!\n",
      "\n",
      "CLARENCE:\n",
      "'Tis for this virtuous soundaint one are one in olut\n",
      "Makes' suppur two shrifl'?\n",
      "\n",
      "BUCKINGHAM:\n",
      "I know not for safe-but him for enemy but a fourth her, on thinkrible?\n",
      "For thou see him\n",
      "That speak them sorrow\n",
      "Cheeks, we once this hatice\n",
      "Thregrictions would\n",
      "Heaven shall bit.-\n",
      "Villain and my hand lead\n",
      "Having but upon a brother matter.\n",
      "\n",
      "POMPEY:\n",
      "Disphat cannot give theirs,\n",
      "And lip to be great corlier; and, in thou set loves: upon\n",
      "Thou droop mistraction. Warwice,\n",
      "Thinks all the venure he low-thousame as one stir broke myself;\n",
      "For you're wench, but 'tward Cullor all thy reans the grans,\n",
      "He will in Lord Angelo.\n",
      "\n",
      "LEONTES:\n",
      "The court:\n",
      "O, he, he ha'! shall; but the insteach matter-grace from\n",
      "Lay her will at you array not where\n",
      "Thou hasting he shall never side them my orator\n",
      "This is hat I for slips note weeder'd in questeriancica many bold them'd; not shall hate she maugest thus lave.\n",
      "\n",
      "KING RICHARD III:\n",
      "We'll that therefordman begin us,\n",
      "And life, him I ne'er nammish.\n",
      "\n",
      "BAGOTHASAR:\n",
      "I shall he London alter\n",
      "On the rough blood\n",
      "hear lying shall\n",
      "far and that I know shall to dry I love.\n",
      "Courage Marshal and be are not warm\n",
      "nauresty pate, my put him we hoodes look to now: think i' the gods it seal do royalter:\n",
      "A, spoke look royalting I come on, come some\n",
      "One pray itself in concer,\n",
      "To the duyse my aid\n",
      "To enterupt of a struchia\n",
      "leturn their churchars? gonery. You have see to strange.\n",
      "O, against of Earl itis with firtushiour;\n",
      "And called after,\n",
      "He letdes on his prison.\n",
      "\n",
      "LEONTES:\n",
      "Nice on and my legs who take and die the hearts;\n",
      "If any backled and me?\n",
      "\n",
      "CORKERNES:\n",
      "And again that disported true, which, what, you beast our bring\n",
      "So.\n",
      "\n",
      "CORIOLANUS:\n",
      "You stand to be cred.\n",
      "\n",
      "FLORIZEL:\n",
      "No resternight nime; spareport, this to sincel.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "O, will he car;\n",
      "Hus. Lewit your crown over with an yet,\n",
      "Or your own seaters-asideathesed woe;\n",
      "Right? 'Tis my brain.\n",
      "She's now, too, past will your eye bastacks mine than royalties up she's a word in.\n",
      "\n",
      "First King Henry's back as the world\n",
      "The constancingA:\n",
      "And they whose lays; I'll for purgafe:\n",
      "Peace, thoughts him\n",
      "Redeems his were by be hand thou, daughter, you see them\n",
      "that mocked to sent I have.\n",
      "\n",
      "First Lord:\n",
      "And the comfort harity them; sister on this, good strengefore his darkle foreign warrate a tries'd\n",
      "by. Traze\n",
      "Shate it would the Derbutiones, marry vault? O, the Polixenes that would longelo our hand you king? the seat, and villain,\n",
      "pite and to go what Noture to thee\n",
      "so proud me, ready upon and plick me, somethingerous home\n",
      "In her hat thus sentrengthen\n",
      "My news?\n",
      "\n",
      "MUREWILIA::\n",
      "But day not me but agree we'll teern's upond Servingman:\n",
      "O thou would to king's portyour\n",
      "lay never lack as the mind:\n",
      "He more, consequence.\n",
      "\n",
      "HOR OF YORK:\n",
      "And, for, sir, under Norfolknow\n",
      "It sir, and who's scure imploathing of dark, call'd you might, I'll guest\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d4a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
