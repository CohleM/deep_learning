{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece5c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d05d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "# from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, SequenceParallel, PrepareModuleInput\n",
    "from torch.distributed._tensor import Shard, Replicate, distribute_tensor, DTensor\n",
    "\n",
    "# from llama2_model import Transformer, ModelArgs\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "from torch.distributed._tensor import DeviceMesh, Shard, Replicate, distribute_tensor\n",
    "\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08bce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch\n",
    "from torch.distributed._tensor import DeviceMesh, Shard, Replicate, distribute_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cba9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed._composable.fsdp import fully_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d719e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device= 'cpu'\n",
    "    simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "\n",
    "    model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "    model.tok_embeddings.weight = nn.Parameter(torch.arange(1.,33.).reshape(8,4))\n",
    "\n",
    "\n",
    "    mesh = init_device_mesh('cpu', (2,1, 2), mesh_dim_names=[\"DDP\",\"FSDP\", \"TP\"])\n",
    "#     print(mesh)\n",
    "\n",
    "#     print(f' RANK {dist.get_rank()} {mesh['DDP', 'FSDP']} ')\n",
    "#     print(f' RANK {dist.get_rank()} {mesh['FSDP']} ')\n",
    "    \n",
    "    \n",
    "    layer_tp_plan = {\n",
    "        # Now the input and output of SequenceParallel has Shard(1) layouts,\n",
    "        # to represent the input/output tensors sharded on the sequence dimension\n",
    "        \"attention_norm\": SequenceParallel(),\n",
    "        # \"attention\": PrepareModuleInput(\n",
    "        #     input_layouts=(Shard(1), Replicate()),\n",
    "        #     desired_input_layouts=(Replicate(), Replicate()),\n",
    "        # ),\n",
    "        \"attention.wq\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wk\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wv\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"ffn_norm\": SequenceParallel(),\n",
    "        \"feed_forward\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1),),\n",
    "            desired_input_layouts=(Replicate(),),\n",
    "        ),\n",
    "        \"feed_forward.w1\": ColwiseParallel(),\n",
    "        \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"feed_forward.w3\": ColwiseParallel(),\n",
    "    }\n",
    "    # Apply TP\n",
    "    for layer_id, transformer_block in enumerate(model.layers):\n",
    "        # layer_tp_plan = {...}  # i.e. the plan we just generated\n",
    "\n",
    "        parallelize_module(\n",
    "            module=transformer_block,\n",
    "            device_mesh=mesh['TP'],\n",
    "            parallelize_plan=layer_tp_plan,\n",
    "        )\n",
    "\n",
    "    model = parallelize_module(\n",
    "        model,\n",
    "        mesh['TP'],\n",
    "        {\n",
    "            \"tok_embeddings\": RowwiseParallel(\n",
    "                input_layouts=Replicate(),\n",
    "                output_layouts=Shard(1),\n",
    "            ),\n",
    "            \"norm\": SequenceParallel(),\n",
    "            \"output\": ColwiseParallel(\n",
    "                input_layouts=Shard(1),\n",
    "                output_layouts=Replicate()\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "#     print(f'rank {dist.get_rank()} mesh {mesh['TP']} DDP mesh {mesh['DDP','FSDP']}')\n",
    "\n",
    "    for layer in model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP','FSDP'])\n",
    "\n",
    "    fully_shard(model, mesh=mesh['DDP','FSDP'])\n",
    "    \n",
    "\n",
    "    \n",
    "#     print(f' RANK {dist.get_rank()} its mesh {mesh['DDP', 'FSDP'] } wq.weight {model.layers[0].attention.wq.weight.to_local().shape}')\n",
    "\n",
    "#     if dist.get_rank() == 0:\n",
    "    print(f' RANK {dist.get_rank()} its mesh {mesh['DDP','FSDP'] } wq.weight {model.layers[0].attention.wq.weight.to_local()}, ')\n",
    "    full = model.layers[0].attention.wq.weight.full_tensor()\n",
    "#     print('yolo', full.shape, full)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80de07b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RANK 1 its mesh DeviceMesh('cpu', [[1], [3]], mesh_dim_names=('DDP', 'FSDP')) wq.weight tensor([[ 0.0227, -0.0034, -0.0062,  0.0211],\n",
      "        [-0.0046, -0.0220, -0.0173,  0.0011]],\n",
      "       grad_fn=<_ToTorchTensorBackward>), \n",
      " RANK 2 its mesh DeviceMesh('cpu', [[0], [2]], mesh_dim_names=('DDP', 'FSDP')) wq.weight tensor([[ 0.0187,  0.0047, -0.0212,  0.0276],\n",
      "        [ 0.0298, -0.0108,  0.0122, -0.0339]],\n",
      "       grad_fn=<_ToTorchTensorBackward>), \n",
      " RANK 0 its mesh DeviceMesh('cpu', [[0], [2]], mesh_dim_names=('DDP', 'FSDP')) wq.weight tensor([[ 0.0093, -0.0151,  0.0237, -0.0201],\n",
      "        [ 0.0156,  0.0106,  0.0451,  0.0130]],\n",
      "       grad_fn=<_ToTorchTensorBackward>), \n",
      " RANK 3 its mesh DeviceMesh('cpu', [[1], [3]], mesh_dim_names=('DDP', 'FSDP')) wq.weight tensor([[-0.0069,  0.0010, -0.0055,  0.0024],\n",
      "        [-0.0241, -0.0198, -0.0218,  0.0225]],\n",
      "       grad_fn=<_ToTorchTensorBackward>), \n"
     ]
    }
   ],
   "source": [
    "world_size = 4\n",
    "shard_big_tensor(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0665c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73dc57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device= 'cpu'\n",
    "\n",
    "\n",
    "    simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "\n",
    "    model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "    model.tok_embeddings.weight = nn.Parameter(torch.arange(1.,33.).reshape(8,4))\n",
    "#     print(model)\n",
    "    # model.init_weights()\n",
    "\n",
    "    # \"\"\"\n",
    "    # mesh = init_device_mesh('cpu', (2, 4), mesh_dim_names=[\"FSDP\", \"TP\"])\n",
    "\n",
    "    # mesh = init_device_mesh('cpu', (2,), mesh_dim_names=[\"TP\"])\n",
    "\n",
    "    mesh = init_device_mesh('cpu', (2, 1, 2), mesh_dim_names=[\"DDP\", \"FSDP\", \"TP\"])\n",
    "    layer_tp_plan = {\n",
    "        # Now the input and output of SequenceParallel has Shard(1) layouts,\n",
    "        # to represent the input/output tensors sharded on the sequence dimension\n",
    "        \"attention_norm\": SequenceParallel(),\n",
    "        \"attention\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1), Replicate()),\n",
    "            desired_input_layouts=(Replicate(), Replicate()),\n",
    "        ),\n",
    "        \"attention.wq\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wk\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wv\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"ffn_norm\": SequenceParallel(),\n",
    "        \"feed_forward\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1),),\n",
    "            desired_input_layouts=(Replicate(),),\n",
    "        ),\n",
    "        \"feed_forward.w1\": ColwiseParallel(),\n",
    "        \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"feed_forward.w3\": ColwiseParallel(),\n",
    "    }\n",
    "    # Apply TP\n",
    "    for layer_id, transformer_block in enumerate(model.layers):\n",
    "        # layer_tp_plan = {...}  # i.e. the plan we just generated\n",
    "\n",
    "        parallelize_module(\n",
    "            module=transformer_block,\n",
    "            device_mesh=mesh['TP'],\n",
    "            parallelize_plan=layer_tp_plan,\n",
    "        )\n",
    "\n",
    "    model = parallelize_module(\n",
    "        model,\n",
    "        mesh['TP'],\n",
    "        {\n",
    "            \"tok_embeddings\": RowwiseParallel(\n",
    "                input_layouts=Replicate(),\n",
    "                # output_layouts=Shard(1),\n",
    "            ),\n",
    "            \"norm\": SequenceParallel(),\n",
    "            \"output\": ColwiseParallel(\n",
    "                input_layouts=Shard(1),\n",
    "                output_layouts=Replicate()\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP','FSDP'])\n",
    "\n",
    "    fully_shard(model, mesh=mesh['DDP','FSDP'])\n",
    "    \n",
    "#     for layer in model.layers:\n",
    "#         fully_shard(layer, mesh=mesh['FSDP'])\n",
    "\n",
    "#     fully_shard(model, mesh=mesh['FSDP'])\n",
    "    \n",
    "\n",
    "    \n",
    "    x = torch.arange(1,17).reshape(1,16)\n",
    "    # out = model.layers[0].feed_forward.w1(x)\n",
    "    # out = model.layers[0].attention.wq(x)\n",
    "\n",
    "    # import time\n",
    "    # time.sleep(5)\n",
    "    print(f'Global rank: {dist.get_rank()}, tok_embeddings {model.tok_embeddings.weight} \\n\\n OUTPUT:\\n ')\n",
    "    # print(f'Global rank: {dist.get_rank()}, \\n\\n OUTPUT:\\n {model.tok_embeddings(x).shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e48134ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1]], [[2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n",
      "Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20.],\n",
      "        [21., 22., 23., 24.],\n",
      "        [25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1]], [[2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n",
      "Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1]], [[2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n",
      "Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20.],\n",
      "        [21., 22., 23., 24.],\n",
      "        [25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1]], [[2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "world_size = 4\n",
    "shard_big_tensor(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61012ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n",
      "Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n",
      "Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20.],\n",
      "        [21., 22., 23., 24.]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n",
      "Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0))) \n",
      "\n",
      " OUTPUT:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "world_size = 4\n",
    "shard_big_tensor(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297a17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465b296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05067fab",
   "metadata": {},
   "source": [
    "# Let's fucking goooo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f211bb0",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n",
    " - load data from huggingface hub using hf datasets\n",
    " - make a dataloader (shuffle data, set batch sizes and so on)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34d7b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('CohleM/rlpr_test_small', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee07bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f923efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "840ce3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data = load_dataset(path, split='train')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data['prompt'][idx]\n",
    "        answer = self.data['reward_model'][idx]\n",
    "        return {'question' : question , 'answer' : answer }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13e0cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDataset('CohleM/rlpr_test_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "875432b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': [[{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'If the firm could reduce the average age of its inventory from 73 days, to 63 day, by how much would it reduce its dollar investment in working capital?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'A sump pump (used to drain water from the basement of houses built below the water table) is draining a flooded basement at the rate of 0.95 L/s, with an output pressure of 3.25 {eq}\\\\times\\n\\n{/eq} 10{eq}^5\\n\\n{/eq} N/m{eq}^2\\n\\n{/eq}. The water enters a hose with a 2.8 cm inside diameter and rises 2.9 m above the pump. What is its pressure at this point N/m{eq}^2\\n\\n{/eq}? You may neglect frictional losses.',\n",
       "    'role': 'user'}]],\n",
       " 'answer': [{'ground_truth': '2.74%', 'style': 'rule'},\n",
       "  {'ground_truth': '2.97e5 N/m^2', 'style': 'rule'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea3531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10eae48d",
   "metadata": {},
   "source": [
    "### training algorithm\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "mesh = init_device_mesh(3,2) # 3 DDP, 2 TP\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(the_dataset, shuffle=True, batch_size=64)\n",
    "for train_data in train_dataloader: # each train_data is of len batch_size=64\n",
    "    # split the data into different 3 DDP groups\n",
    "\n",
    "    complete_rollout = []\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    for each_data in train_data:\n",
    "        each_data = each_data.reapeat(rollout.n)\n",
    "        \n",
    "        # generate responses.\n",
    "        responses = llm.generate(each_data)\n",
    "        advantages = calculate_advantages(each_data, responses)\n",
    "        complete_rollout.append(each_data, responses)\n",
    "    \n",
    "    old_llm = make_copy_of_current_llm()    \n",
    "    for update in range(no_of_updates_per_ppo_step):\n",
    "        minibatches = DataLoader(complete_rollout, batch_size=ppo_mini_batch_size) # no shuffle here\n",
    "        for mini_batch in minibatches:\n",
    "            # generate the logprobs \n",
    "            old_logprobs = old_llm(mini_batchl)\n",
    "            logprobs = llm(mini_batch)\n",
    "            ppo_loss = calculate_ppo_loss(old_logprobs, logprobs, advantages) # i.e ratio * adv, where ratio = logprobs/ old_logprobs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            \n",
    "        \n",
    "        \n",
    "    # we're doing GRPO so for each question generate,  \n",
    "    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c5d4d",
   "metadata": {},
   "source": [
    "### training algorithm\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "mesh = init_device_mesh(3,2) # 3 DDP, 2 TP\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(the_dataset, shuffle=True, batch_size=64)\n",
    "for train_data in train_dataloader: # each train_data is of len batch_size=64\n",
    "    # split the data into different 3 DDP groups\n",
    "\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    # repeat the train_data X rollout.n = new_train_data\n",
    "    # split the new_train_data into different 3 DDP groups processes, each will get (len(new_train_data) / DDP_size) data\n",
    "    new_train_data = train_data.repeat(rollout.n)\n",
    "        \n",
    "    #split here, i.e new_train_data into 3 ddp groups,\n",
    "    # we also need an estimate of how many data each group can process at a time, lets say it's vllm_mini_batch\n",
    "    new_train_data = DataLoader(new_train_data, vllm_mini_batch)\n",
    "    complete_rollout = []\n",
    "    for vllm_batch in new_train_data:\n",
    "        # generate responses.\n",
    "        responses = llm.generate(vllm_batch)\n",
    "        complete_rollout.append(vllm_batch,responses)\n",
    "    \n",
    "     \n",
    "    \n",
    "    # all_gather data into one process then caculate the return/advantages,\n",
    "    # after gathering complete_rollout should be full.\n",
    "    \n",
    "    advantages = calculate_advantages(each_data, responses)\n",
    "    complete_rollout.append(each_data, responses)\n",
    "    \n",
    "    \n",
    "    old_llm = make_copy_of_current_llm()  # we only need model parameters for old_llm, and will have the same sharding strategy as the current llm i.e llm.\n",
    "    \n",
    "    for update in range(no_of_updates_per_ppo_step):\n",
    "        minibatches = DataLoader(complete_rollout, batch_size=ppo_mini_batch_size) # no shuffle here, or maybe split the mini_batches \n",
    "        for mini_batch in minibatches:\n",
    "            # generate the logprobs \n",
    "            old_logprobs = old_llm(mini_batch)\n",
    "            logprobs = llm(mini_batch)\n",
    "            ppo_loss = calculate_ppo_loss(old_logprobs, logprobs, advantages) # i.e ratio * adv, where ratio = logprobs/ old_logprobs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            \n",
    "     # update the vllm model with our llm's weight.\n",
    "    \n",
    "        \n",
    "    # we're doing GRPO so for each question generate,  \n",
    "    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ea5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e19187d",
   "metadata": {},
   "source": [
    "# PPO trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46e0f4",
   "metadata": {},
   "source": [
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11e9f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_batch_size: int = 64\n",
    "    rollout_n: int = 8\n",
    "    mini_batch_size: int = 8\n",
    "    model_name: str = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "    dp_size: int = 2\n",
    "    tp_size: int = 2\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9b31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28934025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85e627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd082c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81bb9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_batch_size: int = 64\n",
    "    rollout_n: int = 8\n",
    "    mini_batch_size: int = 8\n",
    "    model_name: str = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "    dp_size: int = 2\n",
    "    tp_size: int = 2\n",
    "        \n",
    "config = Config()\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, SequenceParallel, PrepareModuleInput\n",
    "from llama2_model import Transformer, ModelArgs\n",
    "\n",
    "class Worker:\n",
    "    \"\"\"\n",
    "    This is the policy that we will be updating with each gradient update, we rollout using this policy's\n",
    "    parameters, and we use the logprobs from this policy, we will also copy it's weights to make it old policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        device= 'cpu'\n",
    "        \n",
    "        # define model from huggingface later on\n",
    "#         simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "#         self.model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "\n",
    "        # first make a device mesh\n",
    "        self.mesh = init_device_mesh(device,(config.dp_size, config.tp_size), mesh_dim_names=[\"FSDP\", \"TP\"])\n",
    "\n",
    "#         # shard model's parameters on tp axis\n",
    "#         self.prepare_tp_model()\n",
    "\n",
    "#         # shard model's parameter on dp axis\n",
    "#         prepare_dp_model()\n",
    "\n",
    "    def prepare_optimizer(self):\n",
    "        if config.tp_size > 1:\n",
    "            self.model = prepare_tp_model(self.model, self.mesh)\n",
    "            \n",
    "#             if dist.get_rank() == 0:\n",
    "                \n",
    "#                 print(f' rank : {dist.get_rank()} attention wq {self.model.layers[0].attention.wq.weight}')\n",
    "        \n",
    "        self.model = prepare_dp_model(self.model, self.mesh)\n",
    "#         if dist.get_rank() == 0:\n",
    "#             print(f' after dp model rank: {dist.get_rank()} attention wq {self.model.layers[0].attention.wq.weight}')\n",
    "        \n",
    "        \n",
    "def prepare_tp_model(model, mesh):\n",
    "\n",
    "    layer_tp_plan = {\n",
    "        # Now the input and output of SequenceParallel has Shard(1) layouts,\n",
    "        # to represent the input/output tensors sharded on the sequence dimension\n",
    "        \"attention_norm\": SequenceParallel(),\n",
    "        \"attention\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1), Replicate()),\n",
    "            desired_input_layouts=(Replicate(), Replicate()),\n",
    "        ),\n",
    "        \"attention.wq\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wk\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wv\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"ffn_norm\": SequenceParallel(),\n",
    "        \"feed_forward\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1),),\n",
    "            desired_input_layouts=(Replicate(),),\n",
    "        ),\n",
    "        \"feed_forward.w1\": ColwiseParallel(),\n",
    "        \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"feed_forward.w3\": ColwiseParallel(),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Apply TP\n",
    "    for layer_id, transformer_block in enumerate(model.layers):\n",
    "\n",
    "        parallelize_module(\n",
    "            module=transformer_block,\n",
    "            device_mesh=mesh['TP'],\n",
    "            parallelize_plan=layer_tp_plan,\n",
    "        )\n",
    "\n",
    "    model = parallelize_module(\n",
    "        model,\n",
    "        mesh['TP'],\n",
    "        {\n",
    "            \"tok_embeddings\": RowwiseParallel(\n",
    "                input_layouts=Replicate(),\n",
    "                output_layouts=Shard(1),\n",
    "            ),\n",
    "            \"norm\": SequenceParallel(),\n",
    "            \"output\": ColwiseParallel(\n",
    "                input_layouts=Shard(1),\n",
    "                output_layouts=Replicate()\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def prepare_dp_model(model, mesh):\n",
    "    for layer in model.layers:\n",
    "        fully_shard(layer, mesh=mesh['FSDP'])\n",
    "\n",
    "    sharded_model = fully_shard(model, mesh=mesh['FSDP'])\n",
    "    return sharded_model\n",
    "\n",
    "\n",
    "class Actor(Worker):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        device= 'cpu'\n",
    "        \n",
    "#         define model from huggingface later on\n",
    "        simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "        self.model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "        \n",
    "        # actor will need optimizer\n",
    "        self.prepare_optimizer()\n",
    "\n",
    "\n",
    "class Rollout(Worker):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "            \n",
    "        # init model using sgland ()\n",
    "        self.prepare_env_var()\n",
    "\n",
    "        if self.mesh['TP'].get_local_rank() == 0:\n",
    "            # initialize the SGL engine\n",
    "            # change this env var to 0, otherwise, it will block other gpu !=0 i.e in 2x2 mesh it blocks local_rank = 2\n",
    "            os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "            self.llm = Engine(\n",
    "                model_path=config.model_name,\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=self.mesh[\"tp\"].size(),\n",
    "                mem_fraction_static=config.gpu_memory_utilization,\n",
    "                enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "                \n",
    "                \n",
    "    def prepare_env_var(self):\n",
    "        if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys(): # remove the use of common store for communication\n",
    "            del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "        monkey_patch_torch_reductions()\n",
    "        \n",
    "        \n",
    "        # THE reason for doing this is because, we'll store rollout worker's (sglang) weight in these TP group\n",
    "        # otherwise, SGL will use all the available cuda devices.\n",
    "        cuda_visible_devices = [None] * self.config.tp_size\n",
    "        dist.all_gather_object(cuda_visible_devices, os.environ['LOCAL_RANK'], self.mesh['TP'].get_group())\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(cuda_visible_devices)\n",
    "        \n",
    "  \n",
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.actor = Actor(config)\n",
    "#         self.rollout = Rollout(config)\n",
    "        \n",
    "    def train():\n",
    "        pass\n",
    "        \n",
    "        \n",
    "import os\n",
    "from torch import optim\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    # can't use os.environ['LOCAL_RANK'] in spawn_threads_and_init_comms so using dist.get_rank() which gives the\n",
    "    # local rank, but instead we must to os.environ['LOCAL_RANK'] otherwise.\n",
    "    \n",
    "    local_rank = dist.get_rank()\n",
    "#     local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # if gpu set this\n",
    "    # torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    ppo_trainer = Trainer(config)\n",
    "    \n",
    "    dummy_var = [None] * world_size\n",
    "    dist.all_gather_object(\n",
    "        dummy_var,\n",
    "        local_rank,\n",
    "        ppo_trainer.actor.mesh['TP'].get_group()\n",
    "    )\n",
    "    \n",
    "    mesh = ppo_trainer.actor.mesh\n",
    "    model = ppo_trainer.actor.model\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
    "    \n",
    "    options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "    state_dict = get_model_state_dict(\n",
    "        model, options=options\n",
    "    )\n",
    "\n",
    "    for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "\n",
    "#         print(name, tensor)\n",
    "        if name == 'layers.0.attention.wq.weight':\n",
    "            \n",
    "            serialized_tensor = tensor.full_tensor() if isinstance(tensor, DTensor) else tensor\n",
    "            serialized_tensors = [None] * mesh['TP'].size() if mesh['TP'].get_local_rank() == 0 else None\n",
    "            \n",
    "            dist.gather_object(serialized_tensor, serialized_tensors, group_dst=0, group=mesh['TP'].get_group())\n",
    "\n",
    "        \n",
    "            local_tensor = tensor._local_tensor if isinstance(tensor, DTensor) else tensor\n",
    "            print(f\"rank {dist.get_rank()} local wq shape: {local_tensor.shape}\")\n",
    "#             print(f' rank {dist.get_rank()}', name, serialized_tensor.shape)\n",
    "        \n",
    "    \n",
    "#         assert model._is_root, \"Only support root model offloading to CPU\"\n",
    "#         for handle in model._all_handles:\n",
    "#             if handle._offload_params:\n",
    "#                 continue\n",
    "#             flat_param = handle.flat_param\n",
    "#             print(flat_param , 'shape', flat_param.shape)\n",
    "            \n",
    "#     print( isinstance(ppo_trainer.actor.model, FSDPModule))\n",
    "#     print('dummy var')\n",
    "#     actor = Actor(config)\n",
    "#     print(f'Global RANK {dist.get_rank()} wq shape: {ppo_trainer.actor.model.layers[0].attention.wq.weight.to_local().shape} good')\n",
    "#     print(f'Global RANK {dist.get_rank()}  dummy_var : {state_dict} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1ed4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ed6b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, SequenceParallel, PrepareModuleInput\n",
    "from llama2_model import Transformer, ModelArgs\n",
    "\n",
    "class Worker:\n",
    "    \"\"\"\n",
    "    This is the policy that we will be updating with each gradient update, we rollout using this policy's\n",
    "    parameters, and we use the logprobs from this policy, we will also copy it's weights to make it old policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        device= 'cpu'\n",
    "        \n",
    "        # define model from huggingface later on\n",
    "#         simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "#         self.model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "\n",
    "        # first make a device mesh\n",
    "        self.mesh = init_device_mesh(device,(config.dp_size, config.tp_size), mesh_dim_names=[\"FSDP\", \"TP\"])\n",
    "\n",
    "#         # shard model's parameters on tp axis\n",
    "#         self.prepare_tp_model()\n",
    "\n",
    "#         # shard model's parameter on dp axis\n",
    "#         prepare_dp_model()\n",
    "\n",
    "    def prepare_optimizer(self):\n",
    "        if config.tp_size > 1:\n",
    "            self.model = prepare_tp_model(self.model, self.mesh)\n",
    "            \n",
    "#             if dist.get_rank() == 0:\n",
    "                \n",
    "#                 print(f' rank : {dist.get_rank()} attention wq {self.model.layers[0].attention.wq.weight}')\n",
    "        \n",
    "        self.model = prepare_dp_model(self.model, self.mesh)\n",
    "#         if dist.get_rank() == 0:\n",
    "#             print(f' after dp model rank: {dist.get_rank()} attention wq {self.model.layers[0].attention.wq.weight}')\n",
    "        \n",
    "        \n",
    "def prepare_tp_model(model, mesh):\n",
    "\n",
    "    layer_tp_plan = {\n",
    "        # Now the input and output of SequenceParallel has Shard(1) layouts,\n",
    "        # to represent the input/output tensors sharded on the sequence dimension\n",
    "        \"attention_norm\": SequenceParallel(),\n",
    "        \"attention\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1), Replicate()),\n",
    "            desired_input_layouts=(Replicate(), Replicate()),\n",
    "        ),\n",
    "        \"attention.wq\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wk\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wv\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"ffn_norm\": SequenceParallel(),\n",
    "        \"feed_forward\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1),),\n",
    "            desired_input_layouts=(Replicate(),),\n",
    "        ),\n",
    "        \"feed_forward.w1\": ColwiseParallel(),\n",
    "        \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"feed_forward.w3\": ColwiseParallel(),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Apply TP\n",
    "    for layer_id, transformer_block in enumerate(model.layers):\n",
    "\n",
    "        parallelize_module(\n",
    "            module=transformer_block,\n",
    "            device_mesh=mesh['TP'],\n",
    "            parallelize_plan=layer_tp_plan,\n",
    "        )\n",
    "\n",
    "    model = parallelize_module(\n",
    "        model,\n",
    "        mesh['TP'],\n",
    "        {\n",
    "            \"tok_embeddings\": RowwiseParallel(\n",
    "                input_layouts=Replicate(),\n",
    "                output_layouts=Shard(1),\n",
    "            ),\n",
    "            \"norm\": SequenceParallel(),\n",
    "            \"output\": ColwiseParallel(\n",
    "                input_layouts=Shard(1),\n",
    "                output_layouts=Replicate()\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def prepare_dp_model(model, mesh):\n",
    "    for layer in model.layers:\n",
    "        fully_shard(layer, mesh=mesh['FSDP'])\n",
    "\n",
    "    sharded_model = fully_shard(model, mesh=mesh['FSDP'])\n",
    "    return sharded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94aeca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Worker):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        device= 'cpu'\n",
    "        \n",
    "#         define model from huggingface later on\n",
    "        simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "        self.model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "        \n",
    "        # actor will need optimizer\n",
    "        self.prepare_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6074fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Rollout(Worker):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "            \n",
    "        # init model using sgland ()\n",
    "        self.prepare_env_var()\n",
    "\n",
    "        if self.mesh['TP'].get_local_rank() == 0:\n",
    "            # initialize the SGL engine\n",
    "            # change this env var to 0, otherwise, it will block other gpu !=0 i.e in 2x2 mesh it blocks local_rank = 2\n",
    "            os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "            self.llm = Engine(\n",
    "                model_path=config.model_name,\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=self.mesh[\"tp\"].size(),\n",
    "                mem_fraction_static=config.gpu_memory_utilization,\n",
    "                enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "                \n",
    "                \n",
    "    def prepare_env_var(self):\n",
    "        if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys(): # remove the use of common store for communication\n",
    "            del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "        monkey_patch_torch_reductions()\n",
    "        \n",
    "        \n",
    "        # THE reason for doing this is because, we'll store rollout worker's (sglang) weight in these TP group\n",
    "        # otherwise, SGL will use all the available cuda devices.\n",
    "        cuda_visible_devices = [None] * self.config.tp_size\n",
    "        dist.all_gather_object(cuda_visible_devices, os.environ['LOCAL_RANK'], self.mesh['TP'].get_group())\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(cuda_visible_devices)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c871cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953d0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a192110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.actor = Actor(config)\n",
    "#         self.rollout = Rollout(config)\n",
    "        \n",
    "    def train():\n",
    "        pass\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d92579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a00f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    # can't use os.environ['LOCAL_RANK'] in spawn_threads_and_init_comms so using dist.get_rank() which gives the\n",
    "    # local rank, but instead we must to os.environ['LOCAL_RANK'] otherwise.\n",
    "    \n",
    "    local_rank = dist.get_rank()\n",
    "#     local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # if gpu set this\n",
    "    # torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    ppo_trainer = Trainer(config)\n",
    "    \n",
    "    dummy_var = [None] * world_size\n",
    "    dist.all_gather_object(\n",
    "        dummy_var,\n",
    "        local_rank,\n",
    "        ppo_trainer.actor.mesh['TP'].get_group()\n",
    "    )\n",
    "    \n",
    "    mesh = ppo_trainer.actor.mesh\n",
    "    model = ppo_trainer.actor.model\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
    "    \n",
    "    options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "    state_dict = get_model_state_dict(\n",
    "        model, options=options\n",
    "    )\n",
    "\n",
    "    for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "\n",
    "#         print(name, tensor)\n",
    "        if name == 'layers.0.attention.wq.weight':\n",
    "            \n",
    "            serialized_tensor = tensor.full_tensor() if isinstance(tensor, DTensor) else tensor\n",
    "            serialized_tensors = [None] * mesh['TP'].size() if mesh['TP'].get_local_rank() == 0 else None\n",
    "            \n",
    "            dist.gather_object(serialized_tensor, serialized_tensors, group_dst=0, group=mesh['TP'].get_group())\n",
    "\n",
    "        \n",
    "#             local_tensor = tensor._local_tensor if isinstance(tensor, DTensor) else tensor\n",
    "            print(f\"rank {dist.get_rank()} local wq shape: {serialized_tensor.shape}, {tensor}\")\n",
    "        \n",
    "    \n",
    "#         assert model._is_root, \"Only support root model offloading to CPU\"\n",
    "#         for handle in model._all_handles:\n",
    "#             if handle._offload_params:\n",
    "#                 continue\n",
    "#             flat_param = handle.flat_param\n",
    "#             print(flat_param , 'shape', flat_param.shape)\n",
    "            \n",
    "#     print( isinstance(ppo_trainer.actor.model, FSDPModule))\n",
    "#     print('dummy var')\n",
    "#     actor = Actor(config)\n",
    "#     print(f'Global RANK {dist.get_rank()} wq shape: {ppo_trainer.actor.model.layers[0].attention.wq.weight.to_local().shape} good')\n",
    "#     print(f'Global RANK {dist.get_rank()}  dummy_var : {state_dict} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0782c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.fsdp import CPUOffloadPolicy, FSDPModule\n",
    "from torch.distributed.fsdp._runtime_utils import _lazy_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9e3c8fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 3 local wq shape: torch.Size([4, 4]), DTensor(local_tensor=tensor([[ 0.0078, -0.0135, -0.0184, -0.0121]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0)))rank 0 local wq shape: torch.Size([4, 4]), DTensor(local_tensor=tensor([[0.0231, 0.0122, 0.0189, 0.0002]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0)))\n",
      "\n",
      "rank 1 local wq shape: torch.Size([4, 4]), DTensor(local_tensor=tensor([[ 0.0142,  0.0016,  0.0072, -0.0262]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0)))\n",
      "rank 2 local wq shape: torch.Size([4, 4]), DTensor(local_tensor=tensor([[ 0.0020,  0.0222,  0.0368, -0.0408]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(_StridedShard(dim=0, sf=2), Shard(dim=0)))\n"
     ]
    }
   ],
   "source": [
    "world_size = 4\n",
    "shard_big_tensor(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5379c6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05941f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543be02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7466f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c6624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ec9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667fb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6219a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c224e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501edbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94093b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d055e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d7c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "# import sglang as sgl\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "\n",
    "def setup():\n",
    "        \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        \n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "def prepare_environment_variables(mesh):\n",
    "\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    print(cuda_visible_devices)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "\n",
    "def start():\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['FSDP', 'TP'])\n",
    "    print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} \\n\\n  \")\n",
    "    \n",
    "\n",
    "    engine = Engine(model_path='Qwen/Qwen2.5-0.5B-Instruct', mem_fraction_static=0.8, enable_memory_saver=True,port=30000 + dist.get_rank() )\n",
    "print(torch.cuda.device_count())\n",
    "    # if mesh['TP'].get_local_rank() == 0:\n",
    "    #     prepare_environment_variables(mesh)\n",
    "    #     os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "    #     engine = Engine(model_path='Qwen/Qwen2.5-0.5B-Instruct', tp_size=2, mem_fraction_static=0.8, enable_memory_saver=True,port=30000 + dist.get_rank() )\n",
    "    #     print('I believe done')\n",
    "    # dist.barrier()\n",
    "\n",
    "        \n",
    "\n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032b9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21733b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import sglang as sgl\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "\n",
    "\n",
    "def setup():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "def prepare_environment_variables(mesh):\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    # print(f' GLOBAL RNAK {dist.get_rank()} devices {cuda_visible_devices} ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "def start():\n",
    "    # nest_asyncio.apply()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    torch.cuda.synchronize()\n",
    "    mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['FSDP', 'TP'])\n",
    "    \n",
    "    prepare_environment_variables(mesh)\n",
    "    \n",
    "    print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} visible devices {os.environ[\"CUDA_VISIBLE_DEVICES\"]}\\n\\n  \")\n",
    "    # print(torch.cuda.device_count())\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "        llm = Engine(\n",
    "                model_path=\"qwen/qwen2.5-0.5b-instruct\",\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=mesh[\"TP\"].size(),\n",
    "                mem_fraction_static=0.5,\n",
    "                enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "    dist.barrier()\n",
    "\n",
    "    \n",
    "    # Create engine on all ranks - SGLang will handle internal coordination\n",
    "    # llm = Engine(\n",
    "    #                 model_path=\"qwen/qwen2.5-0.5b-instruct\",\n",
    "    #                 dtype=\"bfloat16\",\n",
    "    #                 tp_size=4,\n",
    "    #                 mem_fraction_static=0.5,\n",
    "    #                 enable_memory_saver=True,\n",
    "    #             )\n",
    "    # llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\", tp_size=1,  dtype=\"bfloat16\")\n",
    "    # dist.barrier()\n",
    "    print(f'Engine created on rank {dist.get_rank()}')\n",
    "\n",
    "    return\n",
    "\n",
    "        \n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c5699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40854967",
   "metadata": {},
   "outputs": [],
   "source": [
    "## actor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import sglang as sgl\n",
    "import nest_asyncio\n",
    "from sglang.test.test_utils import DEFAULT_SMALL_MODEL_NAME_FOR_TEST\n",
    "from transformers import AutoModel\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed._tensor import (DTensor, Replicate, Shard,\n",
    "                                       distribute_tensor)\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import (ColwiseParallel,\n",
    "                                               PrepareModuleInput,\n",
    "                                               RowwiseParallel,\n",
    "                                               SequenceParallel,\n",
    "                                               parallelize_module)\n",
    "\n",
    "def setup():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "def prepare_llama_tp_layer(layer, device_mesh):\n",
    "\n",
    "    parallelize_plan = {\n",
    "        \"input_layernorm\": SequenceParallel(),\n",
    "        \"self_attn.q_proj\": ColwiseParallel(),\n",
    "        \"self_attn.k_proj\": ColwiseParallel(),\n",
    "        \"self_attn.v_proj\": ColwiseParallel(),\n",
    "        \"self_attn.o_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"post_attention_layernorm\": SequenceParallel(),\n",
    "        \"mlp.gate_proj\": ColwiseParallel(),\n",
    "        \"mlp.up_proj\": ColwiseParallel(),\n",
    "        \"mlp.down_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        )\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=layer,\n",
    "        device_mesh=device_mesh,\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "\n",
    "def prepare_environment_variables(mesh):\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    # print(f' GLOBAL RNAK {dist.get_rank()} devices {cuda_visible_devices} ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "def start():\n",
    "    # nest_asyncio.apply()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = dist.get_rank()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    # torch.cuda.synchronize()\n",
    "    mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP', 'TP'])\n",
    "\n",
    "    print(mesh)\n",
    "    # -- rollout ---\n",
    "    # prepare_environment_variables(mesh)\n",
    "    # -- rollout ---\n",
    "    \n",
    "    # print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} visible devices {os.environ[\"CUDA_VISIBLE_DEVICES\"]}\\n\\n  \")\n",
    "    # print(torch.cuda.device_count())\n",
    "\n",
    "    model_name = \"ibm-granite/granite-3.3-2b-base\"\n",
    "\n",
    "    # ------- ROLLOUT ---------\n",
    "    # if mesh[\"TP\"].get_local_rank() == 0:\n",
    "    #     os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "    #     engine = Engine(\n",
    "    #             model_path=model_name,\n",
    "    #             dtype=\"bfloat16\",\n",
    "    #             tp_size=mesh[\"TP\"].size(),\n",
    "    #             mem_fraction_static=0.5,\n",
    "    #             # enable_memory_saver=True,\n",
    "    #             port=30000 + dist.get_rank()\n",
    "    #         )\n",
    "\n",
    "    #     # print(engine)\n",
    "    #     param_name = 'model.layers.0.self_attn.q_proj.weight'\n",
    "    #     # if dist.get_rank() == 0:\n",
    "    #     #     breakpoint()\n",
    "\n",
    "    #     reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "    #     total_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "    #     param = engine.get_weights_by_name(param_name)[0][:5]\n",
    "    #     print(f\"Global rank {dist.get_rank()}  param : {param} \\n\\n  \")\n",
    "    # ------- ROLLOUT ---------\n",
    "\n",
    "\n",
    "        \n",
    "    # dist.barrier()\n",
    "\n",
    "    \n",
    "\n",
    "    # ----- rollout -----\n",
    "    # Do the rollout\n",
    "    # if mesh[\"TP\"].get_local_rank() == 0:\n",
    "    #     # release memory temporarily.\n",
    "        # engine.release_memory_occupation()\n",
    "    # ----- rollout -----\n",
    "\n",
    "\n",
    "    # load the actor model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    # print(model)\n",
    "    for layer in model.model.layers:\n",
    "        prepare_llama_tp_layer(layer, mesh['TP'])\n",
    "\n",
    "    \n",
    "# ----- outer block --------\n",
    "    parallelize_plan = {\n",
    "        \"model.embed_tokens\": ColwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"model.norm\": SequenceParallel(),\n",
    "        \"lm_head\": ColwiseParallel()\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=mesh['TP'],\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "# ----- outer block --------\n",
    "\n",
    "# ----------FSDP -----------\n",
    "    for layer in model.model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP', 'FSDP'])\n",
    "    \n",
    "    fully_shard(model, mesh=mesh['DDP', 'FSDP'])\n",
    "\n",
    "    print(f' GLOBAL RANK {dist.get_rank()} {model.model.layers[0].self_attn.q_proj.weight.to_local().shape}')\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "        \n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e8ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d87882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import sglang as sgl\n",
    "import nest_asyncio\n",
    "from sglang.test.test_utils import DEFAULT_SMALL_MODEL_NAME_FOR_TEST\n",
    "from transformers import AutoModel\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed._tensor import (DTensor, Replicate, Shard,\n",
    "                                       distribute_tensor)\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import (ColwiseParallel,\n",
    "                                               PrepareModuleInput,\n",
    "                                               RowwiseParallel,\n",
    "                                               SequenceParallel,\n",
    "                                               parallelize_module)\n",
    "\n",
    "\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "\n",
    "def setup():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "def prepare_llama_tp_layer(layer, device_mesh):\n",
    "\n",
    "    parallelize_plan = {\n",
    "        \"input_layernorm\": SequenceParallel(),\n",
    "        \"self_attn.q_proj\": ColwiseParallel(),\n",
    "        \"self_attn.k_proj\": ColwiseParallel(),\n",
    "        \"self_attn.v_proj\": ColwiseParallel(),\n",
    "        \"self_attn.o_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"post_attention_layernorm\": SequenceParallel(),\n",
    "        \"mlp.gate_proj\": ColwiseParallel(),\n",
    "        \"mlp.up_proj\": ColwiseParallel(),\n",
    "        \"mlp.down_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        )\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=layer,\n",
    "        device_mesh=device_mesh,\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "\n",
    "def prepare_environment_variables(mesh):\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    # print(f' GLOBAL RNAK {dist.get_rank()} devices {cuda_visible_devices} ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "def start():\n",
    "    # nest_asyncio.apply()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = dist.get_rank()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    # torch.cuda.synchronize()\n",
    "    mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP', 'TP'])\n",
    "\n",
    "    print(mesh)\n",
    "    # -- rollout ---\n",
    "    # prepare_environment_variables(mesh)\n",
    "    # -- rollout ---\n",
    "    \n",
    "    # print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} visible devices {os.environ[\"CUDA_VISIBLE_DEVICES\"]}\\n\\n  \")\n",
    "    # print(torch.cuda.device_count())\n",
    "\n",
    "    model_name = \"ibm-granite/granite-3.3-2b-base\"\n",
    "\n",
    "    # ------- ROLLOUT ---------\n",
    "    # if mesh[\"TP\"].get_local_rank() == 0:\n",
    "    #     os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "    #     engine = Engine(\n",
    "    #             model_path=model_name,\n",
    "    #             dtype=\"bfloat16\",\n",
    "    #             tp_size=mesh[\"TP\"].size(),\n",
    "    #             mem_fraction_static=0.5,\n",
    "    #             # enable_memory_saver=True,\n",
    "    #             port=30000 + dist.get_rank()\n",
    "    #         )\n",
    "\n",
    "    #     # print(engine)\n",
    "    #     param_name = 'model.layers.0.self_attn.q_proj.weight'\n",
    "    #     # if dist.get_rank() == 0:\n",
    "    #     #     breakpoint()\n",
    "\n",
    "    #     reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "    #     total_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "    #     param = engine.get_weights_by_name(param_name)[0][:5]\n",
    "    #     print(f\"Global rank {dist.get_rank()}  param : {param} \\n\\n  \")\n",
    "    # ------- ROLLOUT ---------\n",
    "\n",
    "\n",
    "        \n",
    "    # dist.barrier()\n",
    "\n",
    "    \n",
    "\n",
    "    # ----- rollout -----\n",
    "    # Do the rollout\n",
    "    # if mesh[\"TP\"].get_local_rank() == 0:\n",
    "    #     # release memory temporarily.\n",
    "        # engine.release_memory_occupation()\n",
    "    # ----- rollout -----\n",
    "\n",
    "\n",
    "    # load the actor model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    # print(model)\n",
    "    for layer in model.model.layers:\n",
    "        prepare_llama_tp_layer(layer, mesh['TP'])\n",
    "\n",
    "    \n",
    "# ----- outer block --------\n",
    "    parallelize_plan = {\n",
    "        \"model.embed_tokens\": ColwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"model.norm\": SequenceParallel(),\n",
    "        \"lm_head\": ColwiseParallel()\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=mesh['TP'],\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "# ----- outer block --------\n",
    "\n",
    "# ----------FSDP -----------\n",
    "    for layer in model.model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP', 'FSDP'])\n",
    "    \n",
    "    fully_shard(model, mesh=mesh['DDP', 'FSDP'])\n",
    "\n",
    "    # print(f' GLOBAL RANK {dist.get_rank()} {model.model.layers[0].self_attn.q_proj.weight.to_local().shape} memory allocated {torch.cuda.memory_allocated() / (1024 **3) }')\n",
    "\n",
    "    # offload to cpu\n",
    "    options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "    state_dict = get_model_state_dict(\n",
    "        model, options=options\n",
    "    )\n",
    "\n",
    "    for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "        # load to gpu again\n",
    "        tensor = tensor.to(torch.cuda.current_device())\n",
    "        # print(name)\n",
    "        if name == 'model.layers.0.self_attn.q_proj.weight':\n",
    "            \n",
    "            serialized_tensor = tensor.full_tensor() if isinstance(tensor, DTensor) else tensor\n",
    "            serialized_tensors = [None] * mesh['TP'].size() if mesh['TP'].get_local_rank() == 0 else None\n",
    "            \n",
    "            dist.gather_object(serialized_tensor, serialized_tensors, group_dst=0, group=mesh['TP'].get_group())\n",
    "\n",
    "    \n",
    "            print(f\"rank {dist.get_rank()} seriliazed_tensor {serialized_tensor.shape} len_ST: {len(serialized_tensors) if isinstance(serialized_tensors,list) else serialized_tensors} \")\n",
    "            \n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "        \n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbfd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e63ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4758cd2b",
   "metadata": {},
   "source": [
    "Sglang weight update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import sglang as sgl\n",
    "import nest_asyncio\n",
    "from sglang.test.test_utils import DEFAULT_SMALL_MODEL_NAME_FOR_TEST\n",
    "from transformers import AutoModel\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed._tensor import (DTensor, Replicate, Shard,\n",
    "                                       distribute_tensor)\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import (ColwiseParallel,\n",
    "                                               PrepareModuleInput,\n",
    "                                               RowwiseParallel,\n",
    "                                               SequenceParallel,\n",
    "                                               parallelize_module)\n",
    "\n",
    "\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "\n",
    "def setup():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "def prepare_llama_tp_layer(layer, device_mesh):\n",
    "\n",
    "    parallelize_plan = {\n",
    "        \"input_layernorm\": SequenceParallel(),\n",
    "        \"self_attn.q_proj\": ColwiseParallel(),\n",
    "        \"self_attn.k_proj\": ColwiseParallel(),\n",
    "        \"self_attn.v_proj\": ColwiseParallel(),\n",
    "        \"self_attn.o_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"post_attention_layernorm\": SequenceParallel(),\n",
    "        \"mlp.gate_proj\": ColwiseParallel(),\n",
    "        \"mlp.up_proj\": ColwiseParallel(),\n",
    "        \"mlp.down_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        )\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=layer,\n",
    "        device_mesh=device_mesh,\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "\n",
    "def prepare_environment_variables(mesh):\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    # print(f' GLOBAL RNAK {dist.get_rank()} devices {cuda_visible_devices} ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "def start():\n",
    "    # nest_asyncio.apply()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = dist.get_rank()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    # torch.cuda.synchronize()\n",
    "    mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP', 'TP'])\n",
    "\n",
    "    # print(mesh)\n",
    "    # -- rollout ---\n",
    "    prepare_environment_variables(mesh)\n",
    "    # -- rollout ---\n",
    "    \n",
    "    # print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} visible devices {os.environ[\"CUDA_VISIBLE_DEVICES\"]}\\n\\n  \")\n",
    "    # print(torch.cuda.device_count())\n",
    "\n",
    "    model_name = \"ibm-granite/granite-3.3-2b-base\"\n",
    "\n",
    "    # ------- ROLLOUT ---------\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "        engine = Engine(\n",
    "                model_path=model_name,\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=mesh[\"TP\"].size(),\n",
    "                mem_fraction_static=0.5,\n",
    "                # enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "\n",
    "        # print(engine)\n",
    "        param_name = 'model.layers.0.self_attn.q_proj.weight'\n",
    "        # if dist.get_rank() == 0:\n",
    "        #     breakpoint()\n",
    "\n",
    "        # reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        # total_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "        # param = engine.get_weights_by_name(param_name)[0][:5]\n",
    "        # print(f\"Global rank {dist.get_rank()}  param : {param} \\n\\n  \")\n",
    "    # ------- ROLLOUT ---------\n",
    "\n",
    "    print(f'gloabal rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "        \n",
    "    # dist.barrier()\n",
    "\n",
    "    \n",
    "\n",
    "    # ----- rollout -----\n",
    "    # Do the rollout\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        # release memory temporarily.\n",
    "        engine.release_memory_occupation()\n",
    "        torch.cuda.empty_cache()\n",
    "    # ----- rollout -----\n",
    "\n",
    "    print(f'after release memory oocupation rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "    \n",
    "    # load the actor model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    # print(model)\n",
    "    print(f'After automodel rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "    \n",
    "    for layer in model.model.layers:\n",
    "        prepare_llama_tp_layer(layer, mesh['TP'])\n",
    "\n",
    "    \n",
    "# ----- outer block --------\n",
    "    parallelize_plan = {\n",
    "        \"model.embed_tokens\": ColwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"model.norm\": SequenceParallel(),\n",
    "        \"lm_head\": ColwiseParallel()\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=mesh['TP'],\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "# ----- outer block --------\n",
    "\n",
    "# ----------FSDP -----------\n",
    "    for layer in model.model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP', 'FSDP'])\n",
    "    \n",
    "    fully_shard(model, mesh=mesh['DDP', 'FSDP'])\n",
    "\n",
    "    # print(f' GLOBAL RANK {dist.get_rank()} {model.model.layers[0].self_attn.q_proj.weight.to_local().shape} memory allocated {torch.cuda.memory_allocated() / (1024 **3) }')\n",
    "\n",
    "    # offload to cpu\n",
    "    options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "    state_dict = get_model_state_dict(\n",
    "        model, options=options\n",
    "    )\n",
    "\n",
    "    # -- rollout --\n",
    "    # resume memory occupation\n",
    "    torch.cuda.empty_cache()\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        engine.resume_memory_occupation()\n",
    "\n",
    "\n",
    "    # -- actor changes the model---\n",
    "    # let's make some changes to the first and last row\n",
    "    new_var = torch.tensor([1.5] * 5).to(torch.cuda.current_device())\n",
    "    model.model.layers[0].self_attn.q_proj.weight[0][:5] = new_var\n",
    "    model.model.layers[0].self_attn.q_proj.weight[-1][:5] = new_var\n",
    "    \n",
    "    print(model.model.layers[0].self_attn.q_proj.weight[0][:5])\n",
    "        \n",
    "    for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "        # load to gpu again\n",
    "        tensor = tensor.to(torch.cuda.current_device())\n",
    "        # print(name)\n",
    "        # if name == 'model.layers.0.self_attn.q_proj.weight':\n",
    "            \n",
    "        serialized_tensor = tensor.full_tensor() if isinstance(tensor, DTensor) else tensor\n",
    "        serialized_tensors = [None] * mesh['TP'].size() if mesh['TP'].get_local_rank() == 0 else None\n",
    "        \n",
    "        dist.gather_object(serialized_tensor, serialized_tensors, group_dst=0, group=mesh['TP'].get_group())\n",
    "        \n",
    "        if mesh[\"TP\"].get_local_rank() == 0:\n",
    "            engine.update_weights_from_tensor(named_tensors=[(name, serialized_tensor)])\n",
    "\n",
    "        \n",
    "        print(f\"rank {dist.get_rank()} seriliazed_tensor {serialized_tensor.shape} len_ST: {len(serialized_tensors) if isinstance(serialized_tensors,list) else serialized_tensors} \")\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        param_start = engine.get_weights_by_name(param_name)[0][:5]\n",
    "        param_end = engine.get_weights_by_name(param_name)[-1][:5]\n",
    "        print(f\"Global rank {dist.get_rank()}  param after weight update : {param_start} and param end {param_end} \\n\\n  \")\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "        \n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6657f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7563c2b",
   "metadata": {},
   "source": [
    "actor changes the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # -- actor changes the model---\n",
    "    # let's make some changes to the first and last row\n",
    "    new_var = torch.tensor([1.5] * 5).to(torch.cuda.current_device())\n",
    "    # print(model.model.layers[0].self_attn.q_proj.weight)\n",
    "    # print(model.model.layers[0].self_attn.q_proj.weight[0][:5\n",
    "\n",
    "    old_w = model.model.layers[0].self_attn.q_proj.weight\n",
    "    local_weight = old_w.to_local().detach().clone()  \n",
    "    local_weight[0][:5] = new_var\n",
    "\n",
    "    new_mesh = old_w.device_mesh            # reuse original mesh\n",
    "    placements = old_w.placements         # usually (Shard(0),)\n",
    "\n",
    "    new_dt = DTensor.from_local(\n",
    "        local_weight,\n",
    "        device_mesh=new_mesh,\n",
    "        placements=placements,\n",
    "        run_check=False                    # safe because we kept the shape\n",
    "    )\n",
    "\n",
    "    model.model.layers[0].self_attn.q_proj.weight = torch.nn.Parameter(new_dt)\n",
    "\n",
    "    print('after replacement', model.model.layers[0].self_attn.q_proj.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8ae54",
   "metadata": {},
   "source": [
    "# rollout and update works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import sglang as sgl\n",
    "import nest_asyncio\n",
    "from sglang.test.test_utils import DEFAULT_SMALL_MODEL_NAME_FOR_TEST\n",
    "from transformers import AutoModel\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed._tensor import (DTensor, Replicate, Shard,\n",
    "                                       distribute_tensor)\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import (ColwiseParallel,\n",
    "                                               PrepareModuleInput,\n",
    "                                               RowwiseParallel,\n",
    "                                               SequenceParallel,\n",
    "                                               parallelize_module)\n",
    "\n",
    "\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "from sglang.srt.utils import MultiprocessingSerializer\n",
    "from sglang.srt.model_executor.model_runner import LocalSerializedTensor\n",
    "\n",
    "\n",
    "def setup():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "def prepare_llama_tp_layer(layer, device_mesh):\n",
    "\n",
    "    parallelize_plan = {\n",
    "        \"input_layernorm\": SequenceParallel(),\n",
    "        \"self_attn.q_proj\": ColwiseParallel(),\n",
    "        \"self_attn.k_proj\": ColwiseParallel(),\n",
    "        \"self_attn.v_proj\": ColwiseParallel(),\n",
    "        \"self_attn.o_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"post_attention_layernorm\": SequenceParallel(),\n",
    "        \"mlp.gate_proj\": ColwiseParallel(),\n",
    "        \"mlp.up_proj\": ColwiseParallel(),\n",
    "        \"mlp.down_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        )\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=layer,\n",
    "        device_mesh=device_mesh,\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "\n",
    "def prepare_environment_variables(mesh):\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    # print(f' GLOBAL RNAK {dist.get_rank()} devices {cuda_visible_devices} ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "def start():\n",
    "    # nest_asyncio.apply()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = dist.get_rank()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    # torch.cuda.synchronize()\n",
    "    mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP', 'TP'])\n",
    "\n",
    "    # print(mesh)\n",
    "    # -- rollout ---\n",
    "    prepare_environment_variables(mesh)\n",
    "    # -- rollout ---\n",
    "    \n",
    "    # print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} visible devices {os.environ[\"CUDA_VISIBLE_DEVICES\"]}\\n\\n  \")\n",
    "    # print(torch.cuda.device_count())\n",
    "\n",
    "    model_name = \"ibm-granite/granite-3.3-2b-base\"\n",
    "\n",
    "    # ------- ROLLOUT ---------\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "        engine = Engine(\n",
    "                model_path=model_name,\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=mesh[\"TP\"].size(),\n",
    "                mem_fraction_static=0.5,\n",
    "                # enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "\n",
    "        # print(engine)\n",
    "        param_name = 'model.layers.0.self_attn.q_proj.weight'\n",
    "        # if dist.get_rank() == 0:\n",
    "        #     breakpoint()\n",
    "\n",
    "        # reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        # total_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "        # param = engine.get_weights_by_name(param_name)[0][:5]\n",
    "        # print(f\"Global rank {dist.get_rank()}  param : {param} \\n\\n  \")\n",
    "    # ------- ROLLOUT ---------\n",
    "\n",
    "    print(f'gloabal rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "        \n",
    "    # dist.barrier()\n",
    "\n",
    "    \n",
    "\n",
    "    # ----- rollout -----\n",
    "    # Do the rollout\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        # release memory temporarily.\n",
    "        engine.release_memory_occupation()\n",
    "        torch.cuda.empty_cache()\n",
    "    # ----- rollout -----\n",
    "\n",
    "    print(f'after release memory oocupation rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "    \n",
    "    # load the actor model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    # print(model)\n",
    "    print(f'After automodel rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "    \n",
    "    for layer in model.model.layers:\n",
    "        prepare_llama_tp_layer(layer, mesh['TP'])\n",
    "\n",
    "    \n",
    "# ----- outer block --------\n",
    "    parallelize_plan = {\n",
    "        \"model.embed_tokens\": ColwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"model.norm\": SequenceParallel(),\n",
    "        \"lm_head\": ColwiseParallel()\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=mesh['TP'],\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "# ----- outer block --------\n",
    "\n",
    "# ----------FSDP -----------\n",
    "    for layer in model.model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP', 'FSDP'])\n",
    "    \n",
    "    fully_shard(model, mesh=mesh['DDP', 'FSDP'])\n",
    "\n",
    "    # print(f' GLOBAL RANK {dist.get_rank()} {model.model.layers[0].self_attn.q_proj.weight.to_local().shape} memory allocated {torch.cuda.memory_allocated() / (1024 **3) }')\n",
    "\n",
    "\n",
    "\n",
    "    # -- rollout --\n",
    "    # resume memory occupation\n",
    "    torch.cuda.empty_cache()\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        engine.resume_memory_occupation()\n",
    "\n",
    "\n",
    "    # -- actor changes the model---\n",
    "    # let's make some changes to the first and last row\n",
    "    # -- actor changes the model---\n",
    "    # let's make some changes to the first and last row\n",
    "    new_var = torch.tensor([1.5] * 5).to(torch.cuda.current_device())\n",
    "    # print(model.model.layers[0].self_attn.q_proj.weight)\n",
    "    # print(model.model.layers[0].self_attn.q_proj.weight[0][:5\n",
    "\n",
    "    old_w = model.model.layers[0].self_attn.q_proj.weight\n",
    "    local_weight = old_w.to_local().detach().clone()  \n",
    "    local_weight[0][:5] = new_var\n",
    "\n",
    "    new_mesh = old_w.device_mesh            # reuse original mesh\n",
    "    placements = old_w.placements         # usually (Shard(0),)\n",
    "\n",
    "    new_dt = DTensor.from_local(\n",
    "        local_weight,\n",
    "        device_mesh=new_mesh,\n",
    "        placements=placements,\n",
    "        run_check=False                    # safe because we kept the shape\n",
    "    )\n",
    "\n",
    "    model.model.layers[0].self_attn.q_proj.weight = torch.nn.Parameter(new_dt)\n",
    "\n",
    "    print('after replacement', model.model.layers[0].self_attn.q_proj.weight)\n",
    "\n",
    "\n",
    "    # offload to cpu\n",
    "    options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "    state_dict = get_model_state_dict(\n",
    "        model, options=options\n",
    "    )\n",
    "    \n",
    "    for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "        # load to gpu again\n",
    "        tensor = tensor.to(torch.cuda.current_device())\n",
    "        # print(name)\n",
    "        # if name == 'model.layers.0.self_attn.q_proj.weight':\n",
    "            \n",
    "        serialized_tensor = MultiprocessingSerializer.serialize(tensor.full_tensor() if isinstance(tensor, DTensor) else tensor)\n",
    "        serialized_tensors = [None] * mesh['TP'].size() if mesh['TP'].get_local_rank() == 0 else None\n",
    "        \n",
    "        dist.gather_object(serialized_tensor, serialized_tensors, group_dst=0, group=mesh['TP'].get_group())\n",
    "        \n",
    "        if mesh[\"TP\"].get_local_rank() == 0:\n",
    "            # print(serialized_tensors)\n",
    "            engine.update_weights_from_tensor(named_tensors=[(name, LocalSerializedTensor(values=serialized_tensors))])\n",
    "\n",
    "        \n",
    "        # print(f\"rank {dist.get_rank()} seriliazed_tensor {serialized_tensor.shape} len_ST: {len(serialized_tensors) if isinstance(serialized_tensors,list) else serialized_tensors} \")\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        param_start = engine.get_weights_by_name(param_name)[0][:5]\n",
    "        # param_end = engine.get_weights_by_name(param_name)[-1][:5]\n",
    "        print(f\"Global rank {dist.get_rank()}  param after weight update : {param_start} \\n\\n  \")\n",
    "            \n",
    "            \n",
    "\n",
    "    return\n",
    "\n",
    "        \n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
